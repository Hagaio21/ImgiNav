================================================================================
PROJECT CODE DUMP - 2025-10-15 17:23:17
================================================================================

TABLE OF CONTENTS:
--------------------------------------------------------------------------------
data_preperation\build_graphs.py (255 lines)
data_preperation\collect_dataset.py (138 lines)
data_preperation\collect_graphs.py (83 lines)
data_preperation\generate_palette.py (26 lines)
data_preperation\layout_collection.py (67 lines)
data_preperation\semantic_scene_visualization.py (214 lines)
data_preperation\stage1_build_scenes.py (386 lines)
data_preperation\stage2_split2rooms.py (291 lines)
data_preperation\stage3_create_room_scenes_layouts.py (366 lines)
data_preperation\stage4_create_room_povs.py (483 lines)
data_preperation\stage5_1_build_room_graphs.py (368 lines)
data_preperation\stage5_2_build_scene_graphs.py (318 lines)
data_preperation\stage6_create_layout_embeddings.py (90 lines)
data_preperation\stage7_create_pov_embeddings.py (141 lines)
data_preperation\stage8_create_graph_embeddings.py (214 lines)
data_preperation\manifests\manifist_merge.py (80 lines)
data_preperation\utils\geometry_utils.py (68 lines)
data_preperation\utils\semantic_utils.py (570 lines)
data_preperation\utils\utils.py (398 lines)
data_preperation\utils\validate_scenes.py (51 lines)
data_preperation\utils\visualize_palette.py (374 lines)
modules\attention.py (0 lines)
modules\autoencoder.py (184 lines)
modules\condition_mixer.py (80 lines)
modules\datasets.py (182 lines)
modules\diffusion.py (148 lines)
modules\scheduler.py (76 lines)
modules\unet.py (174 lines)
modules\unified_dataset.py (136 lines)
pipeline\pipeline.py (0 lines)
scripts\analyze_reconstruction.py (999 lines)
scripts\evaluate_ae_sweep.py (501 lines)
scripts\test_autoencoder_identity.py (193 lines)
training\sampling_utils.py (112 lines)
training\training_utils.py (345 lines)
training\train_autoencoder.py (567 lines)
training\train_base_diffusion.py (252 lines)
training\train_conditioned_diffusion.py (244 lines)

================================================================================

================================================================================
FILE: data_preperation\build_graphs.py
================================================================================

#!/usr/bin/env python3
"""
build_graphs.py

Constructs scene-level and room-level graphs from parquet point cloud data.

Inputs:
  - semantic_maps.json (must be located in dataset root or parent folder)
  - <scene_id>/<scene_id>_scene_info.json
  - <scene_id>/rooms/<room_id>/<scene_id>_<room_id>.parquet
  - <scene_id>/rooms/<room_id>/<scene_id>_<room_id>_meta.json

Outputs:
  - <scene_id>/<scene_id>_graph.json
  - <scene_id>/rooms/<room_id>/<scene_id>_<room_id>_graph.json
"""

import argparse, sys, json, re, csv
from pathlib import Path
from typing import Optional, Dict, List
import numpy as np
import pandas as pd

# ----------------- helpers -----------------

def find_semantic_maps_json(start: Path) -> Optional[Path]:
    for p in [start, *start.parents]:
        cand = p / "semantic_maps.json"
        if cand.exists():
            return cand
    return None

def load_semantic_maps(maps_path: Path) -> Dict:
    return json.loads(maps_path.read_text(encoding="utf-8"))

def center_and_bbox(df: pd.DataFrame):
    coords = df[["x","y","z"]].to_numpy(dtype=np.float64)
    if coords.shape[0] == 0:
        return [0,0,0], [[0,0,0],[0,0,0]]
    center = coords.mean(axis=0).tolist()
    mins = coords.min(axis=0).tolist()
    maxs = coords.max(axis=0).tolist()
    return center, [mins, maxs]

def bbox_overlap(b1, b2, tol=0.0):
    """Check if 2 axis-aligned bounding boxes overlap with tolerance."""
    for i in range(3):
        if b1[1][i] < b2[0][i] - tol: return False
        if b2[1][i] < b1[0][i] - tol: return False
    return True

def xy_overlap(b1, b2, tol=0.0):
    """Check XY overlap only (ignore Z)."""
    for i in [0,1]:
        if b1[1][i] < b2[0][i] - tol: return False
        if b2[1][i] < b1[0][i] - tol: return False
    return True

def objects_from_parquet(parquet_path: Path, maps: Dict) -> List[Dict]:
    df = pd.read_parquet(parquet_path)
    objs = []
    for oid, g in df.groupby("label_id"):
        center, bbox = center_and_bbox(g)
        label = None
        for k,v in maps.get("label2id", {}).items():
            if v == oid:
                label = k
                break
        objs.append({
            "object_id": f"{parquet_path.stem}_{oid}",
            "label_id": int(oid),
            "label": label or "unknown",
            "center": center,
            "bbox": bbox
        })
    return objs

def build_room_graph(scene_id: str, room_id: str, room_dir: Path, maps: Dict,
                     near_thresh: float, overlap_tol: float, above_gap: float) -> Dict:
    parquet_files = list(room_dir.glob(f"{scene_id}_{room_id}.parquet"))
    if not parquet_files:
        return {}
    parquet_path = parquet_files[0]
    objects = objects_from_parquet(parquet_path, maps)

    edges = []
    for i, a in enumerate(objects):
        for j, b in enumerate(objects):
            if j <= i: continue
            ca, cb = np.array(a["center"]), np.array(b["center"])
            ba, bb = a["bbox"], b["bbox"]

            # near
            d = np.linalg.norm(ca - cb)
            if d < near_thresh:
                edges.append({
                    "obj_a": a["object_id"],
                    "obj_b": b["object_id"],
                    "relation": "near",
                    "distance": float(d)
                })

            # overlap
            if bbox_overlap(ba, bb, tol=overlap_tol):
                edges.append({
                    "obj_a": a["object_id"],
                    "obj_b": b["object_id"],
                    "relation": "overlap"
                })

            # vertical relations
            if xy_overlap(ba, bb, tol=overlap_tol):
                if ba[0][2] >= bb[1][2] - overlap_tol:
                    dz = ba[0][2] - bb[1][2]
                    if abs(dz) <= above_gap:
                        edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "on_top_of"})
                    elif dz > above_gap:
                        edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "above"})
                if bb[0][2] >= ba[1][2] - overlap_tol:
                    dz = bb[0][2] - ba[1][2]
                    if abs(dz) <= above_gap:
                        edges.append({"obj_a": b["object_id"], "obj_b": a["object_id"], "relation": "on_top_of"})
                    elif dz > above_gap:
                        edges.append({"obj_a": b["object_id"], "obj_b": a["object_id"], "relation": "above"})

            # directional (simple global x/y comparison)
            if abs(ca[0]-cb[0]) > abs(ca[1]-cb[1]):
                if ca[0] < cb[0]:
                    edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "left_of"})
                else:
                    edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "right_of"})
            else:
                if ca[1] < cb[1]:
                    edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "front_of"})
                else:
                    edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "behind"})

    graph = {
        "scene_id": scene_id,
        "room_id": room_id,
        "objects": objects,
        "edges": edges
    }
    outp = room_dir / f"{scene_id}_{room_id}_graph.json"
    outp.write_text(json.dumps(graph, indent=2), encoding="utf-8")
    print(f"  â†³ wrote room graph: {outp}")
    return graph

def build_scene_graph(scene_id: str, scene_dir: Path, maps: Dict,
                      adjacent_thresh: float, overlap_tol: float) -> Dict:
    room_graphs = []
    rooms_root = scene_dir / "rooms"
    if not rooms_root.exists():
        return {}

    for room_id_dir in sorted(rooms_root.iterdir()):
        if not room_id_dir.is_dir():
            continue
        room_id = room_id_dir.name
        rg_path = room_id_dir / f"{scene_id}_{room_id}_graph.json"
        if not rg_path.exists():
            continue
        try:
            if rg_path.stat().st_size == 0:
                print(f"[warn] empty room graph file skipped: {rg_path}")
                continue
            rg = json.loads(rg_path.read_text(encoding="utf-8"))
        except Exception as e:
            print(f"[warn] failed to load room graph {rg_path}: {e}")
            continue

        if rg and rg.get("objects"):
            all_centers = np.array([o["center"] for o in rg["objects"]])
            center = all_centers.mean(axis=0).tolist()
            # compute room bbox
            mins = np.min([o["bbox"][0] for o in rg["objects"]], axis=0).tolist()
            maxs = np.max([o["bbox"][1] for o in rg["objects"]], axis=0).tolist()
            room_graphs.append({
                "room_id": room_id,
                "center": center,
                "bbox": [mins, maxs]
            })

    edges = []
    for i, a in enumerate(room_graphs):
        for j, b in enumerate(room_graphs):
            if j <= i: continue
            ca, cb = np.array(a["center"]), np.array(b["center"])
            d = np.linalg.norm(ca - cb)

            # adjacent
            if d < adjacent_thresh:
                edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "adjacent", "distance": float(d)})

            # overlap / connected
            if bbox_overlap(a["bbox"], b["bbox"], tol=overlap_tol):
                edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "overlap"})
            else:
                # check if touching (faces within tol)
                touching = any(abs(a["bbox"][1][k]-b["bbox"][0][k]) <= overlap_tol or
                               abs(b["bbox"][1][k]-a["bbox"][0][k]) <= overlap_tol for k in range(3))
                if touching:
                    edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "connected"})

            # directional
            if abs(ca[0]-cb[0]) > abs(ca[1]-cb[1]):
                if ca[0] < cb[0]:
                    edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "left_of"})
                else:
                    edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "right_of"})
            else:
                if ca[1] < cb[1]:
                    edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "front_of"})
                else:
                    edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "behind"})

    graph = {
        "scene_id": scene_id,
        "rooms": room_graphs,
        "edges": edges
    }
    outp = scene_dir / f"{scene_id}_graph.json"
    outp.write_text(json.dumps(graph, indent=2), encoding="utf-8")
    print(f"âœ” wrote scene graph: {outp}")
    return graph

# ----------------- main -----------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_dir", required=True, help="Root folder containing scenes/<scene_id>")
    ap.add_argument("--manifest", type=str,
        help="Optional manifest CSV listing files to process (overrides auto discovery)")

    # thresholds
    ap.add_argument("--near-thresh", type=float, default=2.0)
    ap.add_argument("--adjacent-thresh", type=float, default=5.0)
    ap.add_argument("--overlap-tol", type=float, default=0.1)
    ap.add_argument("--above-gap", type=float, default=0.5)

    args = ap.parse_args()

    in_dir = Path(args.in_dir)
    maps_path = find_semantic_maps_json(in_dir)
    if maps_path is None:
        print("semantic_maps.json not found.", file=sys.stderr)
        sys.exit(2)
    maps = load_semantic_maps(maps_path)

    scenes = []
    if args.manifest:
        with open(args.manifest, newline='', encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if "scene_id" in row and row["scene_id"]:
                    scenes.append(row["scene_id"])
    else:
        for p in in_dir.rglob("*_scene_info.json"):
            scenes.append(p.stem.replace("_scene_info",""))

    if not scenes:
        print("No scenes found.", file=sys.stderr)
        sys.exit(2)

    for sid in scenes:
        scene_dir = in_dir / sid
        if not scene_dir.exists():
            continue
        print(f"Processing scene {sid} ...")

        # build room graphs
        rooms_root = scene_dir / "rooms"
        if rooms_root.exists():
            for room_id_dir in sorted(rooms_root.iterdir()):
                if not room_id_dir.is_dir():
                    continue
                build_room_graph(
                    sid, room_id_dir.name, room_id_dir, maps,
                    near_thresh=args.near_thresh,
                    overlap_tol=args.overlap_tol,
                    above_gap=args.above_gap
                )

        # build scene graph
        build_scene_graph(
            sid, scene_dir, maps,
            adjacent_thresh=args.adjacent_thresh,
            overlap_tol=args.overlap_tol
        )

if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\collect_dataset.py
================================================================================

import os
import csv
import argparse

def load_empty_map(layouts_csv):
    """Load room emptiness info from layouts.csv into dict[(scene_id, room_id)] = is_empty"""
    empty_map = {}
    if not os.path.isfile(layouts_csv):
        return empty_map
    with open(layouts_csv, newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            sid = row.get("scene_id", "")
            rid = row.get("room_id", "")
            try:
                empty = int(row.get("is_empty", "0"))
            except ValueError:
                empty = 0
            empty_map[(sid, rid)] = empty
    return empty_map


def collect_povs(root, out_csv, empty_map):
    rows = []
    for scene_id in os.listdir(root):
        scene_dir = os.path.join(root, scene_id)
        if not os.path.isdir(scene_dir):
            continue
        rooms_dir = os.path.join(scene_dir, "rooms")
        if not os.path.isdir(rooms_dir):
            continue

        for room_id in os.listdir(rooms_dir):
            povs_dir = os.path.join(rooms_dir, room_id, "povs")
            if not os.path.isdir(povs_dir):
                continue
            for pov_type in ("seg", "tex"):
                tdir = os.path.join(povs_dir, pov_type)
                if not os.path.isdir(tdir):
                    continue
                for f in os.listdir(tdir):
                    if not f.endswith(".png"):
                        continue
                    path = os.path.join(tdir, f)
                    is_empty = empty_map.get((scene_id, room_id), 0)
                    rows.append([scene_id, room_id, pov_type, path, is_empty])

    with open(out_csv, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["scene_id", "room_id", "type", "pov_path", "is_empty"])
        writer.writerows(rows)


def collect_data(root, out_csv):
    rows = []

    for scene_id in os.listdir(root):
        scene_dir = os.path.join(root, scene_id)
        if not os.path.isdir(scene_dir):
            continue

        # --- scene-level files ---
        for f in os.listdir(scene_dir):
            path = os.path.join(scene_dir, f)
            if not os.path.isfile(path):
                continue

            if f.endswith("_scene_info.json"):
                cat = "scene_info"
            elif f.endswith("_sem_pointcloud.parquet"):
                cat = "scene_parquet"
            elif f.endswith("_scene_layout.png"):
                cat = "scene_layout"
            else:
                cat = "other"
            rows.append([scene_id, "", cat, path])

        # --- rooms ---
        rooms_dir = os.path.join(scene_dir, "rooms")
        if not os.path.isdir(rooms_dir):
            continue

        for room_id in os.listdir(rooms_dir):
            room_dir = os.path.join(rooms_dir, room_id)
            if not os.path.isdir(room_dir):
                continue

            for f in os.listdir(room_dir):
                path = os.path.join(room_dir, f)
                if not os.path.isfile(path):
                    continue
                if f.endswith(".parquet"):
                    cat = "room_parquet"
                elif f.endswith("_meta.json"):
                    cat = "room_meta"
                else:
                    cat = "other"
                rows.append([scene_id, room_id, cat, path])

            # --- room layouts ---
            layouts_dir = os.path.join(room_dir, "layouts")
            if os.path.isdir(layouts_dir):
                for f in os.listdir(layouts_dir):
                    path = os.path.join(layouts_dir, f)
                    if f.endswith("_room_seg_layout.png"):
                        cat = "room_layout_seg"
                    else:
                        cat = "other"
                    rows.append([scene_id, room_id, cat, path])

            # --- povs ---
            povs_dir = os.path.join(room_dir, "povs")
            if os.path.isdir(povs_dir):
                for f in os.listdir(povs_dir):
                    path = os.path.join(povs_dir, f)
                    if f.endswith("_pov_meta.json"):
                        cat = "pov_meta"
                    elif f.endswith("_minimap.png"):
                        cat = "pov_minimap"
                    else:
                        cat = "other"
                    if os.path.isfile(path):
                        rows.append([scene_id, room_id, cat, path])

                for pov_type in ("seg", "tex"):
                    tdir = os.path.join(povs_dir, pov_type)
                    if not os.path.isdir(tdir):
                        continue
                    for f in os.listdir(tdir):
                        path = os.path.join(tdir, f)
                        if not f.endswith(".png"):
                            cat = "other"
                        else:
                            cat = "pov_seg" if pov_type == "seg" else "pov_tex"
                        rows.append([scene_id, room_id, cat, path])

    # --- write CSV ---
    with open(out_csv, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["scene_id", "room_id", "category", "file_path"])
        writer.writerows(rows)


def main():
    parser = argparse.ArgumentParser(description="Collect POVs and full dataset manifest.")
    parser.add_argument("--root", required=True, help="Dataset root directory")
    parser.add_argument("--out", required=True, help="Output directory for CSVs")
    parser.add_argument("--layouts", required=True, help="Path to layouts.csv (for is_empty info)")
    args = parser.parse_args()

    os.makedirs(args.out, exist_ok=True)

    empty_map = load_empty_map(args.layouts)

    povs_csv = os.path.join(args.out, "povs.csv")
    data_csv = os.path.join(args.out, "data.csv")

    collect_povs(args.root, povs_csv, empty_map)
    collect_data(args.root, data_csv)


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\collect_graphs.py
================================================================================

#!/usr/bin/env python3
"""
collect_graph_manifest.py

Scans filesystem for existing graph JSON files and creates manifest.
"""

import argparse
import csv
from pathlib import Path


def collect_graphs(root_dir: Path):
    """Scan filesystem for graph JSON files that exist."""
    graphs = []
    
    print("Scanning for graph files...")
    
    for graph_path in root_dir.rglob("*_graph.json"):
        filename = graph_path.stem
        
        if filename.endswith("_scene_graph"):
            scene_id = filename.replace("_scene_graph", "")
            graph_type = "scene"
            room_id = "scene"
        else:
            parts = filename.replace("_graph", "").split("_")
            if len(parts) >= 2:
                room_id = parts[-1]
                scene_id = "_".join(parts[:-1])
                graph_type = "room"
            else:
                continue
        
        if graph_type == "scene":
            layout_filename = f"{scene_id}_scene_layout.png"
        else:
            layout_filename = f"{scene_id}_{room_id}_room_seg_layout.png"
        
        layout_path = graph_path.parent / layout_filename
        
        graphs.append({
            'scene_id': scene_id,
            'type': graph_type,
            'room_id': room_id,
            'layout_path': str(layout_path) if layout_path.exists() else '',
            'graph_path': str(graph_path),
            'is_empty': 'false'
        })
    
    return graphs


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--root", required=True)
    parser.add_argument("--output", required=True)
    args = parser.parse_args()
    
    root_dir = Path(args.root)
    output_path = Path(args.output)
    
    if not root_dir.exists():
        print(f"[error] Directory not found: {root_dir}")
        return
    
    graphs = collect_graphs(root_dir)
    
    if not graphs:
        print("[error] No graphs found")
        return
    
    scene_count = sum(1 for g in graphs if g['type'] == 'scene')
    room_count = sum(1 for g in graphs if g['type'] == 'room')
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['scene_id', 'type', 'room_id', 'layout_path', 'graph_path', 'is_empty']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(graphs)
    
    print(f"\nTotal graphs: {len(graphs)}")
    print(f"Scene graphs: {scene_count}")
    print(f"Room graphs:  {room_count}")
    print(f"\nManifest: {output_path}")


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\generate_palette.py
================================================================================

#!/usr/bin/env python3
import json
from pathlib import Path
import colorsys

def assign_colors(label2id: dict) -> dict:
    """Assign deterministic colors for each label id."""
    ids = sorted(int(v) for v in label2id.values())
    palette = {}
    phi = 0.61803398875  # golden ratio
    for i, lid in enumerate(ids):
        h = (lid * phi) % 1.0
        r, g, b = colorsys.hsv_to_rgb(h, 0.65, 0.95)
        palette[str(lid)] = [int(r*255), int(g*255), int(b*255)]
    return palette

def main(json_path: Path):
    data = json.loads(json_path.read_text(encoding="utf-8"))
    if "id2color" in data:
        print("id2color already exists, skipping.")
        return
    palette = assign_colors(data["label2id"])
    data["id2color"] = palette
    json_path.write_text(json.dumps(data, indent=2), encoding="utf-8")
    print(f"âœ” Added id2color to {json_path}")

if __name__ == "__main__":
    root = Path("/work3/s233249/ImgiNav/datasets/scenes/semantic_maps.json")  # adjust path
    main(root)


================================================================================
FILE: data_preperation\layout_collection.py
================================================================================

#!/usr/bin/env python3
import argparse
import csv
from pathlib import Path
from PIL import Image
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing


def analyze_layout(img_path: Path, white_vals, gray_vals) -> dict:
    """Analyze one layout file and return row dict."""
    stem = img_path.stem
    parts = stem.split("_")
    scene_id = parts[0]

    if "_scene_" in stem:
        layout_type = "scene"
        room_id = "scene"
    else:
        layout_type = "room"
        room_id = parts[1]

    im = Image.open(img_path).convert("RGB")
    colors = {tuple(rgb) for count, rgb in im.getcolors(maxcolors=1_000_000)}

    is_empty = colors.issubset(white_vals | gray_vals) and len(colors) <= 2

    return {
        "scene_id": scene_id,
        "type": layout_type,
        "room_id": room_id,
        "layout_path": str(img_path.resolve()),
        "is_empty": str(is_empty).lower(),
    }


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--root", required=True, help="Root folder containing layout PNGs")
    ap.add_argument("--out", default="layouts.csv", help="Output CSV path")
    ap.add_argument("--workers", type=int, default=multiprocessing.cpu_count(),
                    help="Number of parallel workers (default: all cores)")
    args = ap.parse_args()

    root = Path(args.root)

    # Adjust once you confirm actual RGB values
    white_vals = {(240, 240, 240), (255, 255, 255)}
    gray_vals = {(200, 200, 200), (211, 211, 211)}

    # Explicit search patterns
    print("[INFO] Scanning for scene layouts...", flush=True)
    scene_files = list(root.rglob("*/layouts/*_scene_layout.png"))
    print(f"[INFO] Found {len(scene_files)} scene layouts", flush=True)

    print("[INFO] Scanning for room layouts...", flush=True)
    room_files = list(root.rglob("*/rooms/*/layouts/*_room_*_layout.png"))
    print(f"[INFO] Found {len(room_files)} room layouts", flush=True)

    img_files = scene_files + room_files
    total = len(img_files)
    print(f"[INFO] Total files to process: {total}", flush=True)
    print(f"[INFO] Using {args.workers} workers", flush=True)

    rows = []
    completed = 0
    with ProcessPoolExecutor(max_workers=args.workers) as ex:
        future_to_file = {ex.submit(analyze_layout, p, white_vals, gray_vals): p for p in img_files}
        for future in as_completed(future_to_file):
            row = future.result()
            rows.append(row)
            completed += 1
            if completed % 500 == 0 or completed == total:
                print(f"[PROGRESS] {completed}/{total} files ({100*completed/total:.1f}%)", flush=True)

    with open(args.out, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["scene_id", "type", "room_id", "layout_path", "is_empty"])
        writer.writeheader()
        writer.writerows(rows)

    print(f"[INFO] Wrote {len(rows)} rows to {args.out}", flush=True)


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\semantic_scene_visualization.py
================================================================================

#!/usr/bin/env python3
"""
Visualize 3D-FRONT scenes with isometric plots: textured and semantic side-by-side.
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

try:
    from utils.semantic_utils import Taxonomy
except ImportError:
    # Try absolute import if relative doesn't work
    import importlib.util
    spec = importlib.util.spec_from_file_location("semantic_utils", Path(__file__).parent / "utils" / "semantic_utils.py")
    semantic_utils = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(semantic_utils)
    Taxonomy = semantic_utils.Taxonomy


def load_point_cloud(scene_dir: Path, scene_id: str, format: str = "parquet"):
    """Load point cloud from parquet or csv."""
    if format == "parquet":
        file_path = scene_dir / f"{scene_id}_sem_pointcloud.parquet"
        return pd.read_parquet(file_path)
    else:
        file_path = scene_dir / f"{scene_id}_sem_pointcloud.csv"
        return pd.read_csv(file_path)


def create_category_colormap(taxonomy: Taxonomy, categories: List[str]) -> Dict[str, np.ndarray]:
    """Create a consistent color mapping for categories."""
    unique_cats = sorted(set(categories))
    
    # Use a colorblind-friendly palette
    base_colors = plt.cm.tab20c(np.linspace(0, 1, 20))
    extended_colors = plt.cm.Set3(np.linspace(0, 1, 12))
    all_colors = np.vstack([base_colors, extended_colors])
    
    color_map = {}
    for i, cat in enumerate(unique_cats):
        color_map[cat] = all_colors[i % len(all_colors)][:3]  # RGB only
    
    # Special colors for architectural elements
    if 'floor' in color_map:
        color_map['floor'] = np.array([0.8, 0.8, 0.8])
    if 'wall' in color_map:
        color_map['wall'] = np.array([0.9, 0.9, 0.85])
    
    return color_map


def setup_isometric_view(ax: Axes3D, bounds: Dict):
    """Configure axis for isometric view."""
    # Set equal aspect ratio
    x_range = bounds['max'][0] - bounds['min'][0]
    y_range = bounds['max'][1] - bounds['min'][1]
    z_range = bounds['max'][2] - bounds['min'][2]
    
    max_range = max(x_range, y_range, z_range)
    mid_x = (bounds['max'][0] + bounds['min'][0]) / 2
    mid_y = (bounds['max'][1] + bounds['min'][1]) / 2
    mid_z = (bounds['max'][2] + bounds['min'][2]) / 2
    
    ax.set_xlim(mid_x - max_range/2, mid_x + max_range/2)
    ax.set_ylim(mid_y - max_range/2, mid_y + max_range/2)
    ax.set_zlim(mid_z - max_range/2, mid_z + max_range/2)
    
    # Isometric viewing angle (35.264Â° elevation, 45Â° azimuth)
    ax.view_init(elev=25, azim=45)
    
    # Clean up axes
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.grid(True, alpha=0.3)


def downsample_points(df: pd.DataFrame, max_points: int = 50000) -> pd.DataFrame:
    """Downsample point cloud for faster rendering."""
    if len(df) <= max_points:
        return df
    
    indices = np.random.choice(len(df), max_points, replace=False)
    return df.iloc[indices]


def plot_textured_view(ax: Axes3D, df: pd.DataFrame, bounds: Dict, point_size: float = 5.0):
    """Plot textured (RGB) view."""
    # Normalize RGB values
    colors = np.column_stack([df['r'], df['g'], df['b']]) / 255.0
    
    ax.scatter(df['x'], df['y'], df['z'], 
               c=colors, 
               s=point_size, 
               alpha=0.8,
               edgecolors='none')
    
    setup_isometric_view(ax, bounds)
    ax.set_title('Textured View', fontsize=14, fontweight='bold')


def plot_semantic_view(ax: Axes3D, df: pd.DataFrame, bounds: Dict, 
                       taxonomy: Taxonomy, point_size: float = 1.0):
    """Plot semantic (category-colored) view."""
    # Create color mapping
    color_map = create_category_colormap(taxonomy, df['category'].tolist())
    
    # Assign colors to each point
    colors = np.array([color_map[cat] for cat in df['category']])
    
    ax.scatter(df['x'], df['y'], df['z'], 
               c=colors, 
               s=point_size, 
               alpha=0.8,
               edgecolors='none')
    
    setup_isometric_view(ax, bounds)
    ax.set_title('Semantic View', fontsize=14, fontweight='bold')
    
    # Add legend for top categories
    unique_cats = df['category'].value_counts().head(10).index.tolist()
    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                                   markerfacecolor=color_map[cat], 
                                   markersize=8, label=cat)
                       for cat in unique_cats if cat in color_map]
    
    ax.legend(handles=legend_elements, loc='upper left', 
              bbox_to_anchor=(1.05, 1), fontsize=8)


def visualize_scene(scene_dir: Path, scene_id: str, taxonomy: Taxonomy,
                   output_path: Path = None, max_points: int = 50000,
                   point_size: float = 1.0, format: str = "parquet",
                   figsize: Tuple[int, int] = (16, 8), dpi: int = 150):
    """Create side-by-side isometric visualization."""
    
    # Load data
    print(f"Loading scene {scene_id}...")
    df = load_point_cloud(scene_dir, scene_id, format)
    
    # Load scene info for bounds
    scene_info_path = scene_dir / f"{scene_id}_scene_info.json"
    with open(scene_info_path, 'r') as f:
        scene_info = json.load(f)
    
    # Downsample if needed
    if len(df) > max_points:
        print(f"Downsampling from {len(df)} to {max_points} points...")
        df = downsample_points(df, max_points)
    
    # Create figure with two subplots
    fig = plt.figure(figsize=figsize, dpi=dpi)
    
    # Textured view (left)
    ax1 = fig.add_subplot(121, projection='3d')
    plot_textured_view(ax1, df, scene_info['bounds'], point_size)
    
    # Semantic view (right)
    ax2 = fig.add_subplot(122, projection='3d')
    plot_semantic_view(ax2, df, scene_info['bounds'], taxonomy, point_size)
    
    # Add main title
    fig.suptitle(f'Scene: {scene_id}', fontsize=16, fontweight='bold', y=0.98)
    
    plt.tight_layout()
    
    # Save or show
    if output_path:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, bbox_inches='tight', dpi=dpi)
        print(f"Saved visualization to {output_path}")
    else:
        plt.show()
    
    plt.close()


def main():
    parser = argparse.ArgumentParser(
        description="Generate isometric visualizations of 3D-FRONT scenes"
    )
    parser.add_argument("--scene_dir", type=str, required=True,
                       help="Directory containing scene point clouds and metadata")
    parser.add_argument("--scene_id", type=str, required=True,
                       help="Scene ID to visualize")
    parser.add_argument("--taxonomy", type=str, required=True,
                       help="Path to taxonomy file")
    parser.add_argument("--output", type=str, default=None,
                       help="Output image path (if not specified, displays interactively)")
    parser.add_argument("--format", type=str, default="parquet",
                       choices=["parquet", "csv"],
                       help="Point cloud format")
    parser.add_argument("--max_points", type=int, default=50000,
                       help="Maximum points to render (for performance)")
    parser.add_argument("--point_size", type=float, default=1.0,
                       help="Size of points in plot")
    parser.add_argument("--figsize", type=int, nargs=2, default=[16, 8],
                       help="Figure size (width height)")
    parser.add_argument("--dpi", type=int, default=150,
                       help="Output resolution")
    
    args = parser.parse_args()
    
    # Load taxonomy
    taxonomy = Taxonomy(Path(args.taxonomy))
    
    # Visualize scene
    output_path = Path(args.output) if args.output else None
    
    visualize_scene(
        scene_dir=Path(args.scene_dir),
        scene_id=args.scene_id,
        taxonomy=taxonomy,
        output_path=output_path,
        max_points=args.max_points,
        point_size=args.point_size,
        format=args.format,
        figsize=tuple(args.figsize),
        dpi=args.dpi
    )


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\stage1_build_scenes.py
================================================================================

#!/usr/bin/env python3
"""
Stage 1: Build 3D-FRONT scenes into point clouds + metadata.
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List

import numpy as np
import pandas as pd
import trimesh
from scipy.spatial.transform import Rotation

# --- imports ---
from utils.semantic_utils import Taxonomy
from utils.utils import (
    gather_paths_from_sources, infer_ids_from_path,
    load_config_with_profile, create_progress_tracker,
    safe_mkdir, write_json
)

# --- Global Taxonomy Object ---
TAXONOMY: Taxonomy = None
ARGS = None

# ---------------------------------------------------------------------
# Utility Functions
# ---------------------------------------------------------------------
def export_outputs(scene_id, output_dir, textured_scene, meshes_info, point_cloud, args, taxonomy):
    """Export all requested formats and scene metadata."""

    # ----- Save GLB -----
    if args.save_glb and textured_scene:
        textured_scene.export(output_dir / f"{scene_id}_textured.glb", file_type="glb")

    # ----- Save OBJ -----
    if args.save_obj and meshes_info:
        with open(output_dir / f"{scene_id}.obj", 'w') as f:
            f.write("# Scene OBJ\n")
            vertex_offset = 1
            for info in meshes_info:
                mesh = info['mesh']
                for v in mesh.vertices:
                    f.write(f"v {v[0]:.6f} {v[1]:.6f} {v[2]:.6f}\n")
                for face in mesh.faces:
                    f.write(f"f {face[0]+vertex_offset} {face[1]+vertex_offset} {face[2]+vertex_offset}\n")
                vertex_offset += len(mesh.vertices)

    # ----- Scene Metadata -----
    if meshes_info:
        all_vertices = np.vstack([info['mesh'].vertices for info in meshes_info])
        scene_info = {
            "bounds": {
                "min": all_vertices.min(axis=0).tolist(),
                "max": all_vertices.max(axis=0).tolist()
            },
            "size": (all_vertices.max(axis=0) - all_vertices.min(axis=0)).tolist(),
            "up_normal": [0.0, 0.0, 1.0]
        }
        write_json(scene_info, output_dir / f"{scene_id}_scene_info.json")

    # ----- Point Cloud Exports -----
    if point_cloud is not None and point_cloud.size > 0:
        xyz = np.column_stack([point_cloud["x"], point_cloud["y"], point_cloud["z"]])
        rgb = np.column_stack([point_cloud["r"], point_cloud["g"], point_cloud["b"]])

        titles = point_cloud["title"].tolist()
        labels = point_cloud["label"].tolist()
        categories = point_cloud["category"].tolist()
        supers = point_cloud["super"].tolist()
        rooms = point_cloud["room_type"].tolist()

        df_data = {
            "x": xyz[:, 0], "y": xyz[:, 1], "z": xyz[:, 2],
            "r": rgb[:, 0], "g": rgb[:, 1], "b": rgb[:, 2],
            "title": titles,
            "label": labels,
            "category": categories,
            "super": supers,
            "room_type": rooms,
            "title_id": point_cloud["title_id"].tolist(),
            "label_id": point_cloud["label_id"].tolist(),
            "category_id": point_cloud["category_id"].tolist(),
            "super_id": point_cloud["super_id"].tolist(),
            "room_id": point_cloud["room_id"].tolist(),
        }


        if args.save_parquet:
            pd.DataFrame(df_data).to_parquet(
                output_dir / f"{scene_id}_sem_pointcloud.parquet", index=False
            )

        if args.save_csv:
            pd.DataFrame(df_data).to_csv(
                output_dir / f"{scene_id}_sem_pointcloud.csv", index=False
            )


def get_point_colors(mesh, points, face_indices):
    """Sample colors from mesh using UV/vertex colors."""
    N = len(points)
    if N == 0 or mesh is None or mesh.is_empty:
        return np.zeros((0, 3), dtype=np.uint8)

    vis = getattr(mesh, "visual", None)

    # Try UV texture sampling
    try:
        uv = getattr(vis, "uv", None) if vis else None
        mat = getattr(vis, "material", None) if vis else None
        img = getattr(mat, "image", None) if mat else None
        if uv is not None and img is not None:
            tris = mesh.triangles[face_indices]
            bary = trimesh.triangles.points_to_barycentric(tris, points)
            faces = mesh.faces[face_indices]
            tri_uv = uv[faces]
            uv_pts = (bary[:, :, None] * tri_uv).sum(axis=1)

            img_np = np.asarray(img.convert("RGB"))
            H, W = img_np.shape[:2]
            u = np.clip(uv_pts[:, 0], 0.0, 1.0) * (W - 1)
            v = (1.0 - np.clip(uv_pts[:, 1], 0.0, 1.0)) * (H - 1)
            ui = np.clip(np.round(u).astype(np.int64), 0, W - 1)
            vi = np.clip(np.round(v).astype(np.int64), 0, H - 1)
            return img_np[vi, ui, :].astype(np.uint8)
    except Exception:
        pass

    # Try vertex colors
    try:
        vcols = getattr(vis, "vertex_colors", None) if vis else None
        if vcols is not None and len(vcols) == len(mesh.vertices):
            tris = mesh.triangles[face_indices]
            bary = trimesh.triangles.points_to_barycentric(tris, points)
            faces = mesh.faces[face_indices]
            tri_vc = vcols[faces][:, :, :3]
            cols = (bary[:, :, None] * tri_vc.astype(np.float32)).sum(axis=1)
            return np.clip(np.round(cols), 0, 255).astype(np.uint8)
    except Exception:
        pass

    # Fallback: flat gray
    return np.full((N, 3), 128, dtype=np.uint8)

def load_mesh(model_dir: Path, jid: str):
    """Load mesh from OBJ or GLB."""
    obj_path = model_dir / jid / "raw_model.obj"
    glb_path = model_dir / jid / "raw_model.glb"

    if obj_path.exists():
        resolver = trimesh.visual.resolvers.FilePathResolver(obj_path.parent)
        return trimesh.load(str(obj_path), force="mesh", process=False,
                            maintain_order=True, resolver=resolver)
    elif glb_path.exists():
        return trimesh.load(str(glb_path), force="mesh", process=False,
                            maintain_order=True)
    else:
        raise FileNotFoundError(f"Model not found: {obj_path} or {glb_path}")

def create_arch_mesh(arch: Dict):
    """Create mesh from architectural JSON data."""
    vertices = np.array(arch["xyz"], dtype=np.float64).reshape(-1, 3)
    faces = np.array(arch["faces"], dtype=np.int64).reshape(-1, 3)
    mesh = trimesh.Trimesh(vertices=vertices, faces=faces, process=True)
    mesh.visual.vertex_colors = np.array([200, 200, 200, 255], dtype=np.uint8)
    return mesh

def build_transform(child: Dict):
    """Build transform matrix from child node."""
    pos = np.array(child.get("pos", [0, 0, 0]), dtype=np.float64)
    rot = np.array(child.get("rot", [0, 0, 0, 1]), dtype=np.float64)
    scl = np.array(child.get("scale", [1, 1, 1]), dtype=np.float64)

    T = np.eye(4, dtype=np.float64)
    T[:3, 3] = pos
    Rm = Rotation.from_quat(rot).as_matrix()
    Sm = np.diag(scl)
    T[:3, :3] = Rm @ Sm
    return T

def sample_points(scene_objects):
    """Sample points from scene objects and attach taxonomy info."""
    areas = [max(0.0, obj['mesh'].area) for obj in scene_objects]
    total_area = sum(a for a in areas if a > 0.0) or 1.0

    all_data = []
    for obj, area in zip(scene_objects, areas):
        if area <= 0.0:
            continue

        if ARGS.ppsm > 0.0:
            n_pts = int(round(area * ARGS.ppsm))
        else:
            n_pts = int(round((area / total_area) * ARGS.total_points))

        n_pts = max(ARGS.min_pts_per_mesh, n_pts)
        if ARGS.max_pts_per_mesh > 0:
            n_pts = min(n_pts, ARGS.max_pts_per_mesh)

        if n_pts <= 0:
            continue

        # ---- sample points ----
        pts_local, face_indices = trimesh.sample.sample_surface(obj['mesh'], n_pts)
        colors = get_point_colors(obj['mesh'], pts_local, face_indices)
        pts_world = trimesh.transform_points(pts_local, obj['transform'])

        title = obj['label']
        room_type = obj.get('room_type', 'UnknownRoom')

        # category
        category_id = TAXONOMY.translate(title, output="id")
        category = TAXONOMY.translate(category_id, output="name") if category_id else "UnknownCategory"

        # super (explicit name + id)
        super_cat = TAXONOMY.get_sup(title, output="name")
        super_id  = TAXONOMY.get_sup(title, output="id")


        # title id
        title_id = TAXONOMY.translate(title, output="id")

        # room
        room_id = TAXONOMY.translate(room_type, output="id")


        all_data.append({
            'xyz': pts_world.astype(np.float32),
            'rgb': colors.astype(np.uint8),
            'title': [title] * n_pts,
            'label': [title] * n_pts,
            'category': [category] * n_pts,
            'super': [super_cat] * n_pts,
            'room_type': [room_type] * n_pts,
            'title_id': [title_id] * n_pts,
            'label_id': [title_id] * n_pts,
            'category_id': [category_id or 0] * n_pts,
            'super_id': [super_id] * n_pts,
            'room_id': [room_id] * n_pts,
        })

    if not all_data:
        return np.array([])

    # ---- combine into structured array ----
    xyz = np.vstack([d['xyz'] for d in all_data])
    rgb = np.vstack([d['rgb'] for d in all_data])

    def flat(key): return sum([d[key] for d in all_data], [])

    dtype = [
        ('x','f4'), ('y','f4'), ('z','f4'),
        ('r','u1'), ('g','u1'), ('b','u1'),
        ('title','U100'), ('label','U100'),
        ('category','U80'), ('super','U80'),
        ('room_type','U50'),
        ('title_id','i4'), ('label_id','i4'),
        ('category_id','i4'), ('super_id','i4'),
        ('room_id','i4'),
    ]

    N = xyz.shape[0]
    structured = np.empty(N, dtype=dtype)
    structured['x'], structured['y'], structured['z'] = xyz.T
    structured['r'], structured['g'], structured['b'] = rgb.T
    structured['title'] = flat('title')
    structured['label'] = flat('label')
    structured['category'] = flat('category')
    structured['super'] = flat('super')
    structured['room_type'] = flat('room_type')
    structured['title_id'] = flat('title_id')
    structured['label_id'] = flat('label_id')
    structured['category_id'] = flat('category_id')
    structured['super_id'] = flat('super_id')
    structured['room_id'] = flat('room_id')

    return structured


# ---------------------------------------------------------------------
# Main Processing
# ---------------------------------------------------------------------
def process_scene(scene_data, model_dir, model_info_map):
    """Process scene into objects and point cloud."""
    failed_models = {}
    scene_objects = []
    furniture_map = {f['uid']: f for f in scene_data.get('furniture', [])}
    arch_map = {m['uid']: m for m in scene_data.get('mesh', [])}

    for room in scene_data.get("scene", {}).get("room", []):
        room_type = room.get("type", "UnknownRoom")  # <-- capture room type once

        for child_index, child in enumerate(room.get("children", [])):
            ref_id = child.get("ref")
            if not ref_id:
                continue

            try:
                mesh, label, model_item = None, "unknown", None

                if ref_id in furniture_map:
                    item_info = furniture_map[ref_id]
                    jid = item_info.get('jid')
                    if not jid:
                        raise ValueError("Missing 'jid'")

                    mesh = load_mesh(model_dir, jid)
                    label = (model_info_map.get(jid, {}).get('category')
                             or item_info.get('title') or "unknown")
                    model_item = item_info

                elif ref_id in arch_map:
                    arch = arch_map[ref_id]
                    if "Ceiling" in arch.get("type", ""):
                        continue
                    mesh = create_arch_mesh(arch)
                    label = 'floor' if 'Floor' in arch.get("type", "") else 'wall'
                else:
                    failed_models[ref_id] = "Reference not found"
                    continue

                if mesh is None or mesh.is_empty:
                    raise ValueError("Empty mesh")

                transform = build_transform(child)

                # ---- Save room type alongside the object ----
                scene_objects.append({
                    "mesh": mesh,
                    "transform": transform,
                    "label": label,
                    "room_type": room_type,            # << attach parent room type
                    "node_name": f"{ref_id}_{child_index}",
                    "model_item": model_item
                })

            except Exception as e:
                failed_models[ref_id] = str(e)

    if not scene_objects:
        return None, None, None, failed_models

    # Build scene and sample points
    textured_scene = trimesh.Scene()
    meshes_world = []
    for obj in scene_objects:
        textured_scene.add_geometry(obj['mesh'], node_name=obj['node_name'],
                                    transform=obj['transform'])
        mesh_copy = obj['mesh'].copy()
        mesh_copy.apply_transform(obj['transform'])
        meshes_world.append({'mesh': mesh_copy, 'label': obj['label']})

    point_cloud = sample_points(scene_objects)
    return textured_scene, meshes_world, point_cloud, failed_models

def process_one_scene(
    scene_path: Path, model_dir: Path, model_info_file: Path, out_root: Path,
    args: argparse.Namespace
) -> bool:
    """Process one scene into meshes, point cloud, and metadata."""
    scene_id = infer_ids_from_path(scene_path)
    if isinstance(scene_id, tuple):
        scene_id = scene_id[0]
    scene_id = str(scene_id)

    out_dir = out_root / scene_id if args.per_scene_subdir else out_root
    if args.per_scene_subdir:
        safe_mkdir(out_dir)

    # Load scene JSON
    with open(scene_path, "r", encoding="utf-8") as f:
        scene = json.load(f)

    # Load model_info.json
    with open(model_info_file, "r", encoding="utf-8") as f:
        model_info_map = {m["model_id"]: m for m in json.load(f)}

    # Build config dict
    config = {
        "ppsm": args.ppsm,
        "total_points": args.total_points,
        "min_pts": args.min_pts_per_mesh,
        "max_pts": args.max_pts_per_mesh,
    }

    # Process scene: returns (trimesh.Scene, meshes_world, structured_pointcloud, failed_models)
    textured_scene, meshes_info, point_cloud, failed = process_scene(
        scene, model_dir, model_info_map)

    if point_cloud is None or point_cloud.size == 0:
        print(f"[WARN] No points sampled for scene {scene_id}")
        return False

    # Delegate all saving to export_outputs
    export_outputs(scene_id, out_dir, textured_scene, meshes_info, point_cloud, args, TAXONOMY)

    return True


# ---------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------

def main():
    global TAXONOMY
    global ARGS
    ap = argparse.ArgumentParser()
    ap.add_argument("--scenes", nargs="+")
    ap.add_argument("--scene_list")
    ap.add_argument("--scene_file")
    ap.add_argument("--out_dir", required=True)
    ap.add_argument("--model_dir", required=True)
    ap.add_argument("--model_info", required=True)
    ap.add_argument("--taxonomy", required=True)
    ap.add_argument("--num_points", type=int, default=2048)
    ap.add_argument("--total_points", type=int, default=500000)
    ap.add_argument("--ppsm", type=float, default=0.0)
    ap.add_argument("--min_pts_per_mesh", type=int, default=100)
    ap.add_argument("--max_pts_per_mesh", type=int, default=0)
    ap.add_argument("--save_glb", action="store_true")
    ap.add_argument("--save_obj", action="store_true")
    ap.add_argument("--save_parquet", action="store_true")
    ap.add_argument("--save_csv", action="store_true")
    ap.add_argument("--per_scene_subdir", action="store_true", default=True)
    ap.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Maximum number of scenes to process (default: all)"
    )
    args = ap.parse_args()
    ARGS = args
    TAXONOMY = Taxonomy(Path(args.taxonomy))

    scene_paths = gather_paths_from_sources(args.scene_file, args.scenes, args.scene_list)
    if not scene_paths:
        print("No scenes found")
        return

    if args.limit is not None:
        scene_paths = scene_paths[:args.limit]

    out_root = Path(args.out_dir)
    safe_mkdir(out_root)

    progress = create_progress_tracker(len(scene_paths), "scenes")
    success_count = 0
    for i, scene_path in enumerate(scene_paths, 1):
        try:
            success = process_one_scene(scene_path, Path(args.model_dir),
                                        Path(args.model_info), out_root, args)
            if success:
                success_count += 1
            progress(i, scene_path.name, success)
        except Exception as e:
            progress(i, f"failed {scene_path.name}: {e}", False)

    print(f"\nSuccessfully processed {success_count}/{len(scene_paths)} scenes")



if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\stage2_split2rooms.py
================================================================================

#!/usr/bin/env python3
"""
stage2_split2rooms.py (refactored)

Create partitioned Parquet dataset by room from scene-level semantic point clouds,
and optionally compute per-room floor-aligned frames.
"""

import argparse
import sys
import numpy as np
import pandas as pd
import json
from pathlib import Path
from typing import Optional, Tuple, List
from utils.utils import (
    discover_files, infer_scene_id, find_semantic_maps_json, 
    get_floor_label_ids, safe_mkdir, write_json, 
    create_progress_tracker
)
from utils.semantic_utils import Taxonomy
TAXONOMY: Taxonomy = None


# ---------- Frame Computation ----------

def pca_plane(points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Return (origin, unit normal) using PCA on candidate floor points."""
    origin = points.mean(axis=0)
    X = points - origin
    if X.shape[0] < 3:
        n = np.array([0, 0, 1.0], dtype=np.float64)
        return origin.astype(np.float64), n
    
    C = np.cov(X.T)
    w, V = np.linalg.eigh(C)  # ascending eigenvalues
    n = V[:, 0]  # smallest eigenvalue = normal to plane
    n = n / (np.linalg.norm(n) + 1e-12)
    return origin.astype(np.float64), n

def orient_normal_upward(normal: np.ndarray, all_xyz: np.ndarray, origin: np.ndarray) -> np.ndarray:
    """Choose normal sign so most points have positive height along +normal."""
    heights = (all_xyz - origin) @ normal
    if (heights > 0).sum() < (heights < 0).sum():
        normal = -normal
    return normal / (np.linalg.norm(normal) + 1e-12)

def build_orthonormal_frame(origin: np.ndarray, normal: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Build coherent UVN frame: v=proj(+Y), u=nÃ—v, re-orthonormalize."""
    Y = np.array([0, 1, 0], dtype=np.float64)
    X = np.array([1, 0, 0], dtype=np.float64)
    
    # Project Y onto plane perpendicular to normal
    v = Y - (Y @ normal) * normal
    if np.linalg.norm(v) < 1e-9:
        # Y is parallel to normal, use X instead
        v = X - (X @ normal) * normal
    v = v / (np.linalg.norm(v) + 1e-12)
    
    # Complete right-handed frame
    u = np.cross(normal, v)
    u = u / (np.linalg.norm(u) + 1e-12)
    v = np.cross(u, normal)
    v = v / (np.linalg.norm(v) + 1e-12)
    
    return u, v, normal

def world_to_local(xyz: np.ndarray, origin: np.ndarray, u: np.ndarray, v: np.ndarray, n: np.ndarray) -> np.ndarray:
    """Transform world coordinates to local UVH frame."""
    R = np.stack([u, v, n], axis=1)  # world -> local transformation
    return (xyz - origin) @ R

def compute_room_frame(parquet_path: Path, floor_label_ids=None, height_band=(0.05, 0.50)) -> dict:
    """Compute floor-aligned coordinate frame for room."""
    try:
        import pandas as pd
    except ImportError:
        raise RuntimeError("pandas required for room frame computation")
    
    # Read point cloud
    df = pd.read_parquet(parquet_path)
    required_cols = {"x", "y", "z"}
    if not required_cols.issubset(df.columns):
        raise RuntimeError(f"Missing columns {required_cols} in {parquet_path}")
    
    xyz = df[["x", "y", "z"]].to_numpy(dtype=np.float64, copy=False)
    
    # Extract floor points if label info available
    floor_pts = np.empty((0, 3), dtype=np.float64)
    if ("label_id" in df.columns) and (floor_label_ids is not None):
        mask = np.isin(df["label_id"].to_numpy(), np.array(floor_label_ids, dtype=df["label_id"].dtype))
        floor_pts = xyz[mask]
    
    # Fallback to low-Z points if insufficient floor labels
    if floor_pts.shape[0] < 50:
        z = xyz[:, 2]
        z_cutoff = np.quantile(z, 0.02)
        candidates = xyz[z <= z_cutoff + 1e-6]
        
        if floor_pts.shape[0] == 0 and "label_id" in df.columns and floor_label_ids is not None:
            pass  # Keep empty to make error obvious
        else:
            floor_pts = candidates
    
    if floor_pts.shape[0] < 3:
        raise RuntimeError(f"Too few floor points to compute plane (check floor_label_ids) point num = {floor_pts.shape[0]}")
    
    # Compute floor plane
    origin, normal = pca_plane(floor_pts)
    normal = orient_normal_upward(normal, xyz, origin)
    u, v, n = build_orthonormal_frame(origin, normal)
    
    # Transform all points to local coordinates
    uvh = world_to_local(xyz, origin, u, v, n)
    umin, umax = float(uvh[:, 0].min()), float(uvh[:, 0].max())
    vmin, vmax = float(uvh[:, 1].min()), float(uvh[:, 1].max())
    
    # Auto-orient: longer axis becomes forward (+v)
    yaw_auto = 0.0 if (vmax - vmin) >= (umax - umin) else 90.0
    
    return {
        "origin_world": origin.tolist(),
        "u_world": u.tolist(),
        "v_world": v.tolist(),
        "n_world": n.tolist(),
        "uv_bounds": [umin, umax, vmin, vmax],
        "yaw_auto": float(yaw_auto),
        "map_band_m": [float(height_band[0]), float(height_band[1])]
    }

# ---------- Meta Writing ----------

def write_room_meta_files(root: Path, layout: str, floor_label_ids=None, height_band=(0.05, 0.50)):
    """Write meta.json files for all rooms based on layout type."""
    if layout == "inplace":
        pattern = "*/rooms/*/*.parquet"
    else:
        pattern = "part-*.parquet"
    
    parquet_files = list(root.rglob(pattern))
    progress = create_progress_tracker(len(parquet_files), "room frames")
    
    for i, parquet_path in enumerate(parquet_files, 1):
        try:
            meta = compute_room_frame(parquet_path, floor_label_ids, height_band)
            
            if layout == "inplace":
                # New layout: <scene>_<room>_meta.json
                stem = parquet_path.stem
                meta_path = parquet_path.parent / f"{stem}_meta.json"
            else:
                # Old layout: meta.json
                meta_path = parquet_path.parent / "meta.json"
            
            write_json(meta, meta_path)
            progress(i, f"frame: {meta_path}", True)
            
        except Exception as e:
            progress(i, f"[warn] frame failed for {parquet_path}: {e}", False)

# ---------- Main Processing ----------

def split_scene_to_rooms(input_files: List[Path], output_config: dict, columns: List[str] = None) -> int:
    """Split scene-level parquets into room-level parquets."""
    processed_count = 0
    progress = create_progress_tracker(len(input_files), "scene splits")
    
    # Determine processing engine
    engine = output_config.get("engine", "auto")
    if engine in ("auto", "dataset"):
        try:
            import pyarrow as pa
            import pyarrow.parquet as pq
            engine = "dataset"
        except ImportError:
            if output_config.get("engine") == "dataset":
                raise RuntimeError("pyarrow not available; use --engine pandas or install pyarrow")
            engine = "pandas"

    for i, input_path in enumerate(input_files, 1):
        try:
            scene_id = infer_scene_id(input_path)
            scene_dir = input_path.parent
            
            if engine == "dataset":
                import pyarrow.parquet as pq
                table = pq.read_table(input_path)
                
                if "room_id" not in table.column_names:
                    progress(i, f"skip {input_path.name}: missing room_id", False)
                    continue
                
                # Apply column filtering
                if columns:
                    keep_cols = [c for c in columns if c in table.column_names]
                    for required in ("scene_id", "room_id"):
                        if required not in keep_cols and required in table.column_names:
                            keep_cols.append(required)
                    table = table.select(keep_cols)
                
                df = table.to_pandas()
            else:
                # Pandas fallback
                df = pd.read_parquet(input_path)
                if "room_id" not in df.columns:
                    progress(i, f"skip {input_path.name}: missing room_id", False)
                    continue
                
                # Apply column filtering
                if columns:
                    keep_cols = [c for c in columns if c in df.columns]
                    for required in ("scene_id", "room_id"):
                        if required not in keep_cols and required in df.columns:
                            keep_cols.append(required)
                    df = df[keep_cols]
            
            # Ensure scene_id column exists
            if "scene_id" not in df.columns:
                df["scene_id"] = scene_id
            
            # Split by room and save
            for (sc_id, room_id), group in df.groupby(["scene_id", "room_id"]):
                if output_config["inplace"]:
                    # New layout: scenes/<scene>/rooms/<rid>/<scene>_<rid>.parquet
                    room_dir = scene_dir / "rooms" / str(int(room_id))
                    safe_mkdir(room_dir)
                    output_path = room_dir / f"{scene_id}_{room_id}.parquet"
                else:
                    # Old layout: dataset/scene_id=<>/rooms/room_id=<>/part-*.parquet
                    room_dir = (output_config["output_dir"] / f"scene_id={sc_id}" / 
                               "rooms" / f"room_id={int(room_id)}")
                    safe_mkdir(room_dir)
                    output_path = room_dir / "part-00000.parquet"
                
                group.to_parquet(output_path, index=False)
            
            processed_count += 1
            progress(i, f"split {scene_id}", True)
            
        except Exception as e:
            progress(i, f"failed {input_path.name}: {e}", False)
    
    return processed_count



def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_dir", required=True, help="Root folder with scene parquet files")
    ap.add_argument("--glob", default="*_sem_pointcloud.parquet", help="Glob for input files")
    ap.add_argument("--out_root", help="(Default mode) Root folder for partitioned dataset")
    ap.add_argument("--dataset_name", default="room_dataset", help="(Default mode) Dataset folder name")
    ap.add_argument("--columns", nargs="*", default=[], help="Optional column restriction")
    ap.add_argument("--engine", choices=["auto", "dataset", "pandas"], default="auto")
    ap.add_argument("--inplace", action="store_true", help="Write inside each scene directory instead of separate dataset")

    # Frame computation
    ap.add_argument("--compute-frames", action="store_true", help="Compute per-room floor frames")
    ap.add_argument("--floor-label-ids", type=int, nargs="*", help="Override floor label IDs manually")
    ap.add_argument("--taxonomy", required=True, help="Path to taxonomy.json")
    ap.add_argument("--map-band", type=float, nargs=2, default=[0.05, 0.50],
                    help="Height band [min max] above floor")
    ap.add_argument("--manifest", help="Optional manifest CSV listing files to process")

    args = ap.parse_args()
    global TAXONOMY
    TAXONOMY = Taxonomy(Path(args.taxonomy))

    in_dir = Path(args.in_dir)

    # Validate output configuration
    if not args.inplace and not args.out_root:
        print("ERROR: --out_root required unless using --inplace", file=sys.stderr)
        sys.exit(2)

    # Set up output configuration
    output_config = {
        "inplace": args.inplace,
        "engine": args.engine
    }

    if not args.inplace:
        output_config["output_dir"] = Path(args.out_root) / args.dataset_name
        safe_mkdir(output_config["output_dir"])

    # Discover input files
    manifest_path = Path(args.manifest) if args.manifest else None
    input_files = discover_files(in_dir, args.glob, manifest_path, "scene_parquet")

    if not input_files:
        print("No input files found", file=sys.stderr)
        sys.exit(2)

    print(f"Found {len(input_files)} input files")

    # Process splits
    processed = split_scene_to_rooms(input_files, output_config, args.columns)
    print(f"Processed {processed} scene files")

    # Compute frames if requested
    if args.compute_frames:
        if args.floor_label_ids:
            floor_ids = tuple(args.floor_label_ids)
            print(f"Using provided floor label IDs: {list(floor_ids)}")
        else:
            floor_ids = TAXONOMY.get_floor_ids()
            print(f"Using floor label IDs from taxonomy {args.taxonomy}: {list(floor_ids)}")

        # --- Write per-room frames ---
        print("Computing per-room floor frames...")
        layout = "inplace" if args.inplace else "default"
        search_root = in_dir if args.inplace else output_config["output_dir"]

        write_room_meta_files(
            search_root, layout,
            floor_label_ids=floor_ids,
            height_band=tuple(args.map_band)
        )
        print("Done computing frames")


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\stage3_create_room_scenes_layouts.py
================================================================================

#!/usr/bin/env python3
"""
stage3_create_room_scenes_layouts.py (refactored)

Generate segmented top-down layout images per room and scene.
"""

import argparse
import csv
import time
from pathlib import Path
from typing import List, Tuple

import numpy as np
import pandas as pd
from PIL import Image
import json

from utils.utils import (
    discover_files, load_room_meta, extract_frame_from_meta, 
    find_semantic_maps_json, load_global_palette, create_progress_tracker, safe_mkdir
)
from utils.semantic_utils import Taxonomy
from utils.geometry_utils import (
    world_to_local_coords

)

TAXONOMY = None
# ---------- Rendering Helpers ----------

def draw_point(canvas: np.ndarray, x: int, y: int, color: np.ndarray, size: int = 1):
    """Draw a square point on canvas with given size."""
    half = size // 2
    x0, x1 = max(x - half, 0), min(x + half, canvas.shape[1] - 1)
    y0, y1 = max(y - half, 0), min(y + half, canvas.shape[0] - 1)
    canvas[y0:y1 + 1, x0:x1 + 1] = color

def points_to_image_coords(u_vals: np.ndarray, v_vals: np.ndarray, 
                          uv_bounds: Tuple[float, float, float, float],
                          resolution: int, margin: int = 10) -> Tuple[np.ndarray, np.ndarray]:
    """Convert UV coordinates to image pixel coordinates."""
    umin, umax, vmin, vmax = uv_bounds
    span = max(umax - umin, vmax - vmin, 1e-6)
    scale = (resolution - 2 * margin) / span
    
    u_pix = (u_vals - umin) * scale + margin
    v_pix = (v_vals - vmin) * scale + margin
    
    # Flip V for image coordinates (origin at top-left)
    x_img = np.clip(np.round(u_pix).astype(np.int32), 0, resolution - 1)
    y_img = np.clip(np.round((resolution - 1) - v_pix).astype(np.int32), 0, resolution - 1)
    
    return x_img, y_img

def load_taxonomy(taxonomy_path):
    """Load taxonomy JSON once and prepare lookups."""
    with open(taxonomy_path, "r", encoding="utf-8") as f:
        taxonomy = json.load(f)

    category2color = {}
    super2color = {}

    # Structural categories (direct mapping)
    if "structural" in taxonomy:
        for cat, info in taxonomy["structural"].items():
            if "color" in info:
                category2color[cat.lower()] = tuple(info["color"])

    # Furniture categories (nested under super)
    if "furniture" in taxonomy:
        for super_name, super_info in taxonomy["furniture"].items():
            if "color" in super_info:
                super2color[super_name.lower()] = tuple(super_info["color"])
            if "categories" in super_info:
                for cat, info in super_info["categories"].items():
                    if "color" in info:
                        category2color[cat.lower()] = tuple(info["color"])

    return category2color, super2color


# ---------- Room Layout Generation ----------


def create_room_layout(
    parquet_path: Path,
    output_path: Path,
    color_mode: str = "category",
    resolution: int = 512,
    margin: int = 10,
    height_min: float = None,
    height_max: float = None,
    point_size: int = 1,
):
    """Generate segmented layout image for a single room."""
    # Load room metadata
    meta = load_room_meta(parquet_path.parent)
    if meta is None:
        raise RuntimeError(f"No metadata found for {parquet_path}")

    origin, u, v, n, uv_bounds, _, map_band = extract_frame_from_meta(meta)

    # Resolve height filtering bounds
    if height_min is None or height_max is None:
        if map_band and len(map_band) == 2:
            if height_min is None:
                height_min = float(map_band[0])
            if height_max is None:
                height_max = float(map_band[1])

    if height_min is None:
        height_min = 0.0
    if height_max is None:
        height_max = 2.5
    if height_max <= height_min:
        raise ValueError(f"height_max ({height_max}) must be > height_min ({height_min})")

    # Load and process point cloud
    df = pd.read_parquet(parquet_path)
    required_cols = {"x", "y", "z", "label_id"}
    if not required_cols.issubset(df.columns):
        raise RuntimeError(f"Missing required columns {required_cols} in {parquet_path}")

    xyz = df[["x", "y", "z"]].to_numpy(dtype=np.float32)
    labels = df["label_id"].to_numpy(dtype=np.int32)

    # Transform to local coordinates
    uvh = world_to_local_coords(xyz, origin, u, v, n)

    # Apply height filtering
    #print("DEBUG layout:", height_min, height_max)
    height_mask = (uvh[:, 2] >= height_min) & (uvh[:, 2] <= height_max)
    if height_mask.sum() == 0:
        print(f"[warn] no points in height band [{height_min},{height_max}] m in {parquet_path}",flush=True)
        return

    u_vals, v_vals = uvh[height_mask, 0], uvh[height_mask, 1]
    filtered_labels = labels[height_mask]

    # Convert to image coordinates
    x_img, y_img = points_to_image_coords(u_vals, v_vals, uv_bounds, resolution, margin)

    # Render to canvas
    canvas = np.full((resolution, resolution, 3), 240, dtype=np.uint8)
    for lbl, x, y in zip(filtered_labels, x_img, y_img):
        if lbl is None:
            raise ValueError(f"Unexpected None label in {parquet_path}")
        color = TAXONOMY.get_color(lbl, mode=color_mode)
        draw_point(canvas, x, y, np.array(color, dtype=np.uint8), size=point_size)

    # Save image
    safe_mkdir(output_path.parent)
    Image.fromarray(canvas).save(output_path)

# ---------- Scene Layout Generation ----------

# def create_scene_layout(
#     scene_dir: Path,
#     output_path: Path,
#     color_mode: str = "category",
#     resolution: int = 512,
#     margin: int = 10,
#     height_min: float = None,
#     height_max: float = None,
#     point_size: int = 1,
# ):
#     """Generate combined layout image for entire scene using taxonomy palette."""
#     scene_id = scene_dir.name
#     room_parquets = sorted(scene_dir.rglob("rooms/*/*.parquet"))
#     if not room_parquets:
#         print(f"[warn] no room parquets found in {scene_dir}",flush=True)
#         return

#     # Get reference frame from first room
#     first_meta = load_room_meta(room_parquets[0].parent)
#     if first_meta is None:
#         print(f"[warn] missing metadata for {room_parquets[0]}",flush=True)
#         return
#     origin, u, v, n, _, _, _ = extract_frame_from_meta(first_meta)

#     # Collect global bounds
#     all_u_bounds, all_v_bounds = [], []
#     for parquet_path in room_parquets:
#         try:
#             df = pd.read_parquet(parquet_path, columns=["x", "y", "z"])
#             xyz = df.to_numpy(dtype=np.float32)
#             uvh = world_to_local_coords(xyz, origin, u, v, n)
#             all_u_bounds.extend([uvh[:, 0].min(), uvh[:, 0].max()])
#             all_v_bounds.extend([uvh[:, 1].min(), uvh[:, 1].max()])
#         except Exception as e:
#             print(f"[warn] failed bounds for {parquet_path}: {e}",flush=True)

#     if not all_u_bounds:
#         print(f"[warn] no usable points in {scene_dir}",flush=True)
#         return
#     global_uv_bounds = (min(all_u_bounds), max(all_u_bounds), min(all_v_bounds), max(all_v_bounds))

#     # Render all rooms to single canvas
#     canvas = np.full((resolution, resolution, 3), 240, dtype=np.uint8)
#     for parquet_path in room_parquets:
#         try:
#             df = pd.read_parquet(parquet_path)
#             required_cols = {"x", "y", "z", "label_id"}
#             if not required_cols.issubset(df.columns):
#                 continue

#             xyz = df[["x", "y", "z"]].to_numpy(dtype=np.float32)
#             labels = df["label_id"].to_numpy(dtype=np.int32)

#             uvh = world_to_local_coords(xyz, origin, u, v, n)
#             mask = np.ones(len(xyz), dtype=bool)
#             if height_min is not None:
#                 mask &= uvh[:, 2] >= height_min
#             if height_max is not None:
#                 mask &= uvh[:, 2] <= height_max

#             u_vals, v_vals = uvh[mask, 0], uvh[mask, 1]
#             f_labels = labels[mask]

#             x_img, y_img = points_to_image_coords(u_vals, v_vals, global_uv_bounds, resolution, margin)
#             for lbl, x, y in zip(f_labels, x_img, y_img):
#                 color = TAXONOMY.get_color(lbl, mode=color_mode)
#                 draw_point(canvas, x, y, np.array(color, dtype=np.uint8), size=point_size)
#         except Exception as e:
#             print(f"[warn] skipping {parquet_path}: {e}",flush=True)

#     safe_mkdir(output_path.parent)
#     Image.fromarray(canvas).save(output_path)



def create_scene_layout(
    scene_dir: Path,
    output_path: Path,
    color_mode: str = "category",
    resolution: int = 512,
    margin: int = 10,
    height_min: float = None,
    height_max: float = None,
    point_size: int = 1,
):
    """Generate combined layout image for entire scene using taxonomy palette."""
    scene_id = scene_dir.name
    room_parquets = sorted(scene_dir.rglob("rooms/*/*.parquet"))
    if not room_parquets:
        print(f"[warn] no room parquets found in {scene_dir}",flush=True)
        return

    # Get reference frame from first room
    first_meta = load_room_meta(room_parquets[0].parent)
    if first_meta is None:
        print(f"[warn] missing metadata for {room_parquets[0]}",flush=True)
        return
    origin, u, v, n, _, _, _ = extract_frame_from_meta(first_meta)

    # SAVE THE SCENE COORDINATE FRAME TO scene_info.json
    scene_info_path = scene_dir / f"{scene_id}_scene_info.json"
    if scene_info_path.exists():
        # Load existing scene_info
        scene_info = json.loads(scene_info_path.read_text(encoding="utf-8"))
    else:
        scene_info = {}
    
    # Add coordinate frame (same frame used to generate layout)
    scene_info["origin_world"] = origin.tolist()
    scene_info["u_world"] = u.tolist()
    scene_info["v_world"] = v.tolist()
    scene_info["n_world"] = n.tolist()
    
    # Save updated scene_info
    scene_info_path.write_text(json.dumps(scene_info, indent=2), encoding="utf-8")

    # Collect global bounds
    all_u_bounds, all_v_bounds = [], []
    for parquet_path in room_parquets:
        try:
            df = pd.read_parquet(parquet_path, columns=["x", "y", "z"])
            xyz = df.to_numpy(dtype=np.float32)
            uvh = world_to_local_coords(xyz, origin, u, v, n)
            all_u_bounds.extend([uvh[:, 0].min(), uvh[:, 0].max()])
            all_v_bounds.extend([uvh[:, 1].min(), uvh[:, 1].max()])
        except Exception as e:
            print(f"[warn] failed bounds for {parquet_path}: {e}",flush=True)

    if not all_u_bounds:
        print(f"[warn] no usable points in {scene_dir}",flush=True)
        return
    global_uv_bounds = (min(all_u_bounds), max(all_u_bounds), min(all_v_bounds), max(all_v_bounds))

    # Render all rooms to single canvas
    canvas = np.full((resolution, resolution, 3), 240, dtype=np.uint8)
    for parquet_path in room_parquets:
        try:
            df = pd.read_parquet(parquet_path)
            required_cols = {"x", "y", "z", "label_id"}
            if not required_cols.issubset(df.columns):
                continue

            xyz = df[["x", "y", "z"]].to_numpy(dtype=np.float32)
            labels = df["label_id"].to_numpy(dtype=np.int32)

            uvh = world_to_local_coords(xyz, origin, u, v, n)
            mask = np.ones(len(xyz), dtype=bool)
            if height_min is not None:
                mask &= uvh[:, 2] >= height_min
            if height_max is not None:
                mask &= uvh[:, 2] <= height_max

            u_vals, v_vals = uvh[mask, 0], uvh[mask, 1]
            f_labels = labels[mask]

            x_img, y_img = points_to_image_coords(u_vals, v_vals, global_uv_bounds, resolution, margin)
            for lbl, x, y in zip(f_labels, x_img, y_img):
                color = TAXONOMY.get_color(lbl, mode=color_mode)
                draw_point(canvas, x, y, np.array(color, dtype=np.uint8), size=point_size)
        except Exception as e:
            print(f"[warn] skipping {parquet_path}: {e}",flush=True)

    safe_mkdir(output_path.parent)
    Image.fromarray(canvas).save(output_path)
# ---------- Discovery Helpers ----------

def discover_rooms(root: Path, pattern: str = None, manifest: Path = None) -> List[Path]:
    """Discover room parquet files."""
    if manifest:
        # Match your manifest column header
        return discover_files(root, pattern, manifest, "room_parquet_file_path")
    
    # Default discovery
    files = list(root.rglob("part-*.parquet"))        # old format
    files.extend(root.rglob("*_*[0-9].parquet"))      # new format
    if not files:
        files = list(root.rglob("rooms/*/*.parquet"))
    
    return sorted(files)


def discover_scenes_from_rooms(room_files: List[Path]) -> List[str]:
    """Extract unique scene IDs from room files."""
    scene_ids = set()
    for room_file in room_files:
        parts = room_file.stem.split("_")
        if len(parts) >= 2:
            scene_ids.add(parts[0])  # <scene_id>_<room_id>
        else:
            for parent in room_file.parents:
                if parent.name.startswith("scene_id="):
                    scene_ids.add(parent.name.replace("scene_id=", ""))
                    break
    return sorted(scene_ids)


def discover_scenes_from_manifest(manifest: Path) -> List[str]:
    """Extract scene IDs from manifest CSV."""
    scene_ids = set()
    with open(manifest, newline='', encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            if "scene_id" in row and row["scene_id"]:
                scene_ids.add(row["scene_id"])
    return sorted(scene_ids)

# ---------- Main Processing ----------


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_root", required=True, help="Root folder with scenes or room dataset")
    ap.add_argument("--taxonomy", required=True, help="Path to taxonomy JSON file")
    ap.add_argument("--pattern", help="Glob pattern for parquet files")
    ap.add_argument("--res", type=int, default=512, help="Output image resolution")
    ap.add_argument("--hmin", type=float, default=0.1, help="Minimum height filter")
    ap.add_argument("--hmax", type=float, default=1.8, help="Maximum height filter")
    ap.add_argument("--point-size", type=int, default=5, help="Point rendering size")
    ap.add_argument("--manifest", help="Optional manifest CSV")
    ap.add_argument("--mode", choices=["room", "scene", "both"], default="both")
    ap.add_argument("--color-mode", choices=["category", "super"], default="category",
                    help="Color mode for rendering")
    args = ap.parse_args()

    in_root = Path(args.in_root)
    taxonomy_path = Path(args.taxonomy)
    manifest_path = Path(args.manifest) if args.manifest else None
    
    # Load taxonomy once
    global TAXONOMY
    TAXONOMY = Taxonomy(taxonomy_path)


    # Rooms
    if args.mode in ("room", "both"):
        room_files = discover_rooms(in_root, args.pattern, manifest_path)
        progress = create_progress_tracker(len(room_files), "room layouts")
        for i, parquet_path in enumerate(room_files, 1):
            scene_id, room_id = parquet_path.stem.split("_")[:2]
            output_path = parquet_path.parent / "layouts" / f"{scene_id}_{room_id}_room_seg_layout.png"
            try:
                create_room_layout(parquet_path, output_path,
                                   args.color_mode,
                                   resolution=args.res, height_min=args.hmin,
                                   height_max=args.hmax, point_size=args.point_size)
                progress(i, f"{parquet_path.name} -> {output_path}", True)
            except Exception as e:
                progress(i, f"failed {parquet_path.name}: {e}", False)

    # Scenes
    if args.mode in ("scene", "both"):
        if manifest_path:
            # Use scene IDs from manifest
            scene_ids = discover_scenes_from_manifest(manifest_path)
            scene_info_files = [in_root / sid / f"{sid}_scene_info.json" for sid in scene_ids]
        else:
            # Default discovery
            scene_info_files = list(in_root.rglob("*_scene_info.json"))
            scene_ids = [p.stem.replace("_scene_info", "") for p in scene_info_files]

        progress = create_progress_tracker(len(scene_ids), "scene layouts")
        for i, scene_id in enumerate(scene_ids, 1):
            scene_dir = in_root / scene_id
            output_path = scene_dir / "layouts" / f"{scene_id}_scene_layout.png"
            try:
                create_scene_layout(scene_dir, output_path,
                                    args.color_mode,
                                    resolution=args.res, height_min=args.hmin,
                                    height_max=args.hmax, point_size=args.point_size)
                progress(i, f"{scene_id} -> {output_path}", True)
            except Exception as e:
                progress(i, f"failed {scene_id}: {e}", False)




if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\stage4_create_room_povs.py
================================================================================

#!/usr/bin/env python3

import argparse, json, math, re, sys, hashlib
from pathlib import Path
from typing import Optional, Tuple, List
import numpy as np
import open3d as o3d
import pandas as pd
import csv
import time
from pathlib import Path
from typing import Optional, List
from sklearn.cluster import KMeans
from utils.utils import create_progress_tracker
from shapely.geometry import MultiPoint
import alphashape
from utils.semantic_utils import Taxonomy  # Import our taxonomy class

# only for HPC
# from xvfbwrapper import Xvfb
# from xvfbwrapper import Xvfb

# ----------- constants -----------
TILT_DEG = 10.0  # look slightly downward for better floor visibility
SEED = 1

# ----------- IO helpers -----------
def infer_scene_id(p: Path) -> str:
    # new filename format: <scene_id>_<room_id>.parquet
    m = re.match(r"([0-9a-fA-F-]+)_\d+\.parquet$", p.name)
    if m: return m.group(1)
    # old path format: .../scene_id=<ID>/room_id=...
    m = re.search(r"scene_id=([^/\\]+)", str(p))
    if m: return m.group(1)
    # fallback from data if present later
    return p.stem

def infer_room_id(p: Path) -> int:
    # new filename format: ..._<room_id>.parquet
    m = re.match(r".+_(\d+)\.parquet$", p.name)
    if m: return int(m.group(1))
    # old path format
    m = re.search(r"room_id=(\d+)", str(p))
    if m: return int(m.group(1))
    return -1

def find_room_files(root: Path, manifest: Optional[Path] = None) -> List[Path]:
    """
    Discover room parquet files either from a manifest CSV or by scanning the dataset root.
    """
    if manifest is not None:
        rows = []
        with open(manifest, newline='', encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if "room_parquet" in row:
                    rows.append(Path(row["room_parquet"]))
                elif "room_parquet_file_path" in row:
                    rows.append(Path(row["room_parquet_file_path"]))

        return rows

    files = sorted(root.rglob("scene_id=*/room_id=*/*.parquet"))
    return files or sorted(root.rglob("*.parquet"))

def load_meta(parquet_path: Path):
    """Load meta from room folder. Tries new '<scene>_<rid>_meta.json', then legacy 'meta.json'."""
    room_dir = parquet_path.parent
    cand = list(room_dir.glob("*_meta.json"))
    if cand:
        mpath = cand[0]
    else:
        mpath = room_dir / "meta.json"
        if not mpath.exists():
            return None
    j = json.loads(mpath.read_text(encoding="utf-8"))
    to_arr = lambda k: np.array(j[k], dtype=np.float32)
    origin = to_arr("origin_world")
    u = to_arr("u_world")
    v = to_arr("v_world")
    n = to_arr("n_world")
    uv_bounds = tuple(j["uv_bounds"])         # (umin, umax, vmin, vmax)
    yaw_auto = float(j.get("yaw_auto", 0.0))
    map_band = tuple(j.get("map_band_m", [0.05, 0.50]))
    return origin, u, v, n, uv_bounds, yaw_auto, map_band

def get_pov_locations(
    u: np.ndarray,
    v: np.ndarray,
    n: np.ndarray,
    origin: np.ndarray,
    uvh: np.ndarray,
    is_floor: np.ndarray,
    num_views: int,
    yaw_auto: float,
    center_uv: Tuple[float, float],
    seed: int
):
    """
    Extract POVs from floor corners instead of clustering.
    Works for convex and concave rooms (e.g. L-shaped).
    """

    # --- Step 1: collect floor points ---
    floor_pts = uvh[is_floor, :2] if is_floor.any() else uvh[:, :2]
    if floor_pts.shape[0] < 4:
        return []  # not enough data for polygon

    # --- Step 2: concave hull polygon (Î±-shape) ---
    try:
        alpha = 0.05  # controls detail, may tune per dataset
        poly = alphashape.alphashape(floor_pts, alpha)
    except Exception:
        return []

    if poly.is_empty or not poly.is_valid:
        return []

    # --- Step 3: polygon vertices ---
    coords = list(poly.exterior.coords)
    corners = []

    # --- Step 4: corner detection via angle test ---
    def angle(p_prev, p, p_next):
        a = np.array(p_prev) - np.array(p)
        b = np.array(p_next) - np.array(p)
        cosang = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9)
        return np.degrees(np.arccos(np.clip(cosang, -1.0, 1.0)))

    angle_thresh = 150.0  # anything sharper = real corner
    for i in range(1, len(coords) - 1):
        ang = angle(coords[i - 1], coords[i], coords[i + 1])
        if ang < angle_thresh:
            corners.append(coords[i])

    # --- Step 5: select up to num_views corners ---
    if len(corners) > num_views:
        rng = np.random.RandomState(seed if seed is not None else 0)
        if len(corners) > num_views:
            rng = np.random.RandomState(seed if seed is not None else 0)
            idxs = rng.choice(len(corners), num_views, replace=False)
            corners = [corners[i] for i in idxs]

    povs = []
    for idx, (cu, cv) in enumerate(corners, start=1):
        # direction: look at polygon centroid
        center = np.array(poly.centroid.coords[0])
        d = center - np.array([cu, cv])
        if np.linalg.norm(d) < 1e-9:
            f_world = math.cos(math.radians(yaw_auto)) * v + math.sin(math.radians(yaw_auto)) * u
        else:
            d = d / (np.linalg.norm(d) + 1e-12)
            f_world = float(d[0]) * u + float(d[1]) * v

        povs.append({
            "name": f"v{idx:02d}",
            "uv": (float(cu), float(cv)),
            "forward": f_world,
            "center_uv": (float(center[0]), float(center[1]))
        })

    return povs

def render_offscreen(pcd, width, height, eye, center, up, fov_deg, bg_rgb, point_size, out_path) -> bool:
    import open3d as o3d
    import time as _t, math as _m
    vis = o3d.visualization.Visualizer()
    vis.create_window(visible=False, width=width, height=height)
    vis.add_geometry(pcd)
    opt = vis.get_render_option()
    opt.point_size = float(point_size)
    opt.background_color = np.array(bg_rgb, dtype=np.float32) / 255.0

    fx = (0.5 * width) / _m.tan(_m.radians(fov_deg) / 2.0)
    fy = (0.5 * height) / _m.tan(_m.radians(fov_deg) / 2.0)
    cx, cy = width / 2.0, height / 2.0
    pin = o3d.camera.PinholeCameraParameters()
    pin.intrinsic = o3d.camera.PinholeCameraIntrinsic(width, height, fx, fy, cx, cy)

    def look_at(eye_, center_, up_):
        f = center_ - eye_
        f = f / (np.linalg.norm(f) + 1e-12)
        upn = up_ / (np.linalg.norm(up_) + 1e-12)
        l = np.cross(upn, f)
        l = l / (np.linalg.norm(l) + 1e-12)
        u2 = np.cross(f, l)
        M = np.eye(4, dtype=np.float64)
        M[0, :3] = l
        M[1, :3] = u2
        M[2, :3] = f
        T = np.eye(4, dtype=np.float64)
        T[:3, 3] = -eye_
        return M @ T

    pin.extrinsic = look_at(
        eye.astype(np.float64), center.astype(np.float64), up.astype(np.float64)
    )

    ctr = vis.get_view_control()
    ctr.convert_from_pinhole_camera_parameters(pin, allow_arbitrary=True)

    vis.poll_events()
    vis.update_renderer()
    _t.sleep(0.12)  # give OpenGL time
    vis.capture_screen_image(str(out_path), do_render=True)
    vis.destroy_window()
    return True

def render_legacy_capture(pcd, width, height, eye, center, up, fov_deg, bg_rgb, point_size, out_path) -> bool:
    import open3d as o3d, time as _t, math as _m
    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)
    vis = o3d.visualization.Visualizer()
    vis.create_window(visible=False, width=width, height=height)
    vis.add_geometry(pcd)
    opt = vis.get_render_option()
    opt.point_size = float(point_size)
    opt.background_color = np.array(bg_rgb, dtype=np.float32)/255.0
    fx = (0.5*width) / _m.tan(_m.radians(fov_deg)/2.0)
    fy = (0.5*height) / _m.tan(_m.radians(fov_deg)/2.0)
    cx, cy = width/2.0, height/2.0
    pin = o3d.camera.PinholeCameraParameters()
    pin.intrinsic = o3d.camera.PinholeCameraIntrinsic(width, height, fx, fy, cx, cy)
    def look_at(eye_, center_, up_):
        f = center_ - eye_
        f = f / (np.linalg.norm(f) + 1e-12)
        upn = up_ / (np.linalg.norm(up_) + 1e-12)
        l = np.cross(upn, f); l = l / (np.linalg.norm(l) + 1e-12)
        u2 = np.cross(f, l)
        M = np.eye(4, dtype=np.float64)
        M[0, :3] = l; M[1, :3] = u2; M[2, :3] = f
        T = np.eye(4, dtype=np.float64); T[:3, 3] = -eye_
        return M @ T
    pin.extrinsic = look_at(eye.astype(np.float64), center.astype(np.float64), up.astype(np.float64))
    ctr = vis.get_view_control()
    ctr.convert_from_pinhole_camera_parameters(pin, allow_arbitrary=True)
    vis.poll_events(); vis.update_renderer(); _t.sleep(0.12)
    vis.capture_screen_image(str(out_path), do_render=True)
    vis.destroy_window()
    return True

# ----------- minimap -----------
def minimap_floor_black(uv: np.ndarray, is_floor: np.ndarray, res=768, margin=10,
                        floor_rgb=(255,0,0), other_rgb=(0,0,0), bg=(240,240,240)):
    from PIL import Image, ImageDraw
    if uv.shape[0]==0:
        return Image.new("RGB",(res,res),bg), (0,1,0,1)
    umin,vmin = uv.min(axis=0); umax,vmax = uv.max(axis=0)
    L = max(umax-umin, vmax-vmin, 1e-9); scale = (res-2*margin)/L
    upix = (uv[:,0]-umin)*scale + margin
    vpix = (uv[:,1]-vmin)*scale + margin
    xi = np.clip(np.round(upix).astype(np.int32), 0, res-1)
    yi = np.clip(np.round((res-1)-vpix).astype(np.int32), 0, res-1)
    floor_count = np.zeros((res,res), dtype=np.int32)
    other_count = np.zeros((res,res), dtype=np.int32)
    np.add.at(floor_count, (yi[is_floor], xi[is_floor]), 1)
    np.add.at(other_count, (yi[~is_floor], xi[~is_floor]), 1)
    canvas = np.full((res,res,3), np.array(bg,dtype=np.uint8), dtype=np.uint8)
    canvas[other_count>0] = np.array(other_rgb, dtype=np.uint8)
    canvas[floor_count>0] = np.array(floor_rgb, dtype=np.uint8)
    img = Image.fromarray(canvas)
    # axes legend uâ†’, vâ†‘
    draw = ImageDraw.Draw(img); ax=max(24,res//7); ox=margin+6; oy=res-margin-6
    draw.line([ox,oy,ox+ax,oy], fill=(0,0,0), width=2)
    draw.polygon([(ox+ax,oy),(ox+ax-8,oy-4),(ox+ax-8,oy+4)], fill=(0,0,0))
    draw.line([ox,oy,ox,oy-ax], fill=(0,0,0), width=2)
    draw.polygon([(ox,oy-ax),(ox-4,oy-ax+8),(ox+4,oy-ax+8)], fill=(0,0,0))
    return img, (umin,umax,vmin,vmax)

def draw_cam_arrows_on_minimap_uv(img, uv_bounds, cams_uv: np.ndarray, angles_deg: List[float], res: int):
    from PIL import ImageDraw
    umin,umax,vmin,vmax = uv_bounds; margin=10
    L = max(umax-umin, vmax-vmin, 1e-6); scale = (res-2*margin)/L
    draw = ImageDraw.Draw(img)
    Lp = max(16, res//10); head = max(6, res//40); r = max(3, res//90)
    for (cu,cv), ang in zip(cams_uv, angles_deg):
        cx = (cu-umin)*scale + margin
        cy = (cv-vmin)*scale + margin; cy = (res-1)-cy
        draw.ellipse([cx-r,cy-r,cx+r,cy+r], fill=(220,30,30), outline=(20,20,20), width=1)
        th = math.radians(float(ang))
        ex,ey = cx + Lp*math.sin(th), cy - Lp*math.cos(th)  # 0Â° = +v
        draw.line([cx,cy,ex,ey], fill=(220,30,30), width=2)
        left = th+math.radians(150); right = th-math.radians(150)
        p2=(ex+head*math.sin(left),  ey-head*math.cos(left))
        p3=(ex+head*math.sin(right), ey-head*math.cos(right))
        draw.polygon([(ex,ey),p2,p3], fill=(220,30,30))

def process_room(parquet_path: Path, root_out_unused: Path, taxonomy: Taxonomy,
                 width=1280, height=800, fov_deg=70.0, eye_height=1.6,
                 point_size=2.0, bg_rgb=(0,0,0),
                 num_views: int = 6, seed: int = SEED, verbose=False) -> bool:
    """
    Process a single room and generate POV renders.
    Now uses the Taxonomy class instead of hardcoded semantic_maps.json lookups.
    """
    meta = load_meta(parquet_path)
    if meta is None:
        print(f"[skip] no meta.json â†’ {parquet_path.parent}")
        return 0
    origin, u, v, n, uv_bounds_all, yaw_auto, _band = meta

    # Get floor IDs from taxonomy instead of semantic_maps.json
    floor_ids = taxonomy.get_floor_ids()
    if not floor_ids:
        print(f"[warning] No floor IDs found in taxonomy for {parquet_path.parent}")
        floor_ids = []

    df = pd.read_parquet(parquet_path)
    xyz = df[["x","y","z"]].to_numpy(np.float32)
    raw = df[["r","g","b"]].to_numpy()
    rgb = (raw.astype(np.float32)/255.0) if not np.issubdtype(raw.dtype, np.floating) else raw.astype(np.float32)
    labels = df["label_id"].to_numpy() if "label_id" in df.columns else None
    scene_id = df["scene_id"].iloc[0] if "scene_id" in df.columns else infer_scene_id(parquet_path)
    room_id  = int(df["room_id"].iloc[0]) if "room_id" in df.columns else infer_room_id(parquet_path)

    if labels is None:
        raise RuntimeError(f"'label_id' column missing in {parquet_path}")

    # --- output dirs inside room folder ---
    out_dir = parquet_path.parent / "povs"
    tex_dir = out_dir / "tex"
    seg_dir = out_dir / "seg"
    tex_dir.mkdir(parents=True, exist_ok=True)
    seg_dir.mkdir(parents=True, exist_ok=True)

    # build Open3D clouds
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(xyz.astype(np.float64, copy=False))
    pcd.colors = o3d.utility.Vector3dVector(rgb.astype(np.float64, copy=False))

    # Use taxonomy for colors instead of loading global palette
    seg_cols = np.zeros((labels.shape[0], 3), dtype=np.float32)
    for uid in np.unique(labels):
        color = taxonomy.get_color(int(uid))
        seg_cols[labels == uid] = np.array(color, dtype=np.float32) / 255.0
    
    seg = o3d.geometry.PointCloud()
    seg.points = pcd.points
    seg.colors = o3d.utility.Vector3dVector(seg_cols.astype(np.float64, copy=False))

    # local coords strictly from meta
    R = np.stack([u, v, n], axis=1)           # world <- local
    uvh = (xyz - origin) @ R
    is_floor = np.isin(labels, np.array(floor_ids, dtype=labels.dtype))
    center_uv = uvh[:, :2].mean(axis=0)
    
    # --- determine valid POV locations ---
    pov_locs = get_pov_locations(
            u=u,
            v=v,
            n=n,
            origin=origin,
            uvh=uvh,
            is_floor=is_floor,
            num_views=num_views,
            yaw_auto=yaw_auto,
            center_uv=center_uv,
            seed=seed
        )

    pov_meta = {"views": []}
    aims_used = []

    tilt = math.tan(math.radians(TILT_DEG))

    for idx, pov in enumerate(pov_locs, 1):
        cu, cv = pov["uv"]
        f_world = pov["forward"]

        aim = f_world - tilt * n
        eye = origin + cu*u + cv*v + eye_height*n
        center = eye + aim
        up = -n

        base_name = f"{scene_id}_{room_id}_v{idx:02d}"
        tex_name = f"{base_name}_pov_tex.png"
        seg_name = f"{base_name}_pov_seg.png"

        tex_path = tex_dir / tex_name
        seg_path = seg_dir / seg_name

        try:
            render_offscreen(pcd, width, height, eye, center, up,
                             fov_deg, bg_rgb, point_size, tex_path)
            render_offscreen(seg, width, height, eye, center, up,
                             fov_deg, bg_rgb, point_size, seg_path)
            if verbose:
                print(f"  âœ” {tex_path}", flush=True)
            aims_used.append(aim)

            pov_meta["views"].append({
                "name": base_name,
                "tex": str(tex_path),
                "seg": str(seg_path),
                "eye": eye.tolist(),
                "center": center.tolist(),
                "up": up.tolist(),
                "uv": [float(cu), float(cv)],
                "forward": f_world.tolist()
            })
        except Exception as e:
            print(f"  âœ— Failed POV {base_name}: {e}", flush=True)

    # --- minimap ---
    uv = uvh[:, :2]
    mm_img, uvb = minimap_floor_black(uv, is_floor, res=768, margin=10)

    angles = []
    for aim in aims_used:
        horiz = aim - np.dot(aim, n) * n
        au, av = float(np.dot(horiz, u)), float(np.dot(horiz, v))
        angles.append(math.degrees(math.atan2(au, av)))

    mm_uv = np.array([p["uv"] for p in pov_locs], dtype=np.float32)
    draw_cam_arrows_on_minimap_uv(mm_img, uvb, mm_uv, angles, 768)
    mm_name = f"{scene_id}_{room_id}_minimap.png"
    mm_path = out_dir / mm_name
    mm_img.save(str(mm_path))
    print(f"  âœ” {mm_path}", flush=True)

    pov_meta["minimap"] = str(mm_path)

    # --- save pov meta ---
    pov_meta_path = out_dir / f"{scene_id}_{room_id}_pov_meta.json"
    with open(pov_meta_path, "w", encoding="utf-8") as f:
        json.dump(pov_meta, f, indent=2)
    print(f"  âœ” {pov_meta_path}", flush=True)

    return len(pov_locs)

def render_pair(
    pcd, seg, eye, center, up,
    width, height, fov_deg, bg_rgb, point_size,
    tex_path: Path, seg_path: Path
):
    render_offscreen(pcd, width, height, eye, center, up,
                     fov_deg, bg_rgb, point_size, tex_path)
    render_offscreen(seg, width, height, eye, center, up,
                     fov_deg, bg_rgb, point_size, seg_path)
    return center - eye

def main_entry():
    ap = argparse.ArgumentParser()
    g = ap.add_mutually_exclusive_group(required=True)
    g.add_argument("--path", type=str, help="Single room parquet (new or old layout)")
    g.add_argument("--dataset-root", type=str, help="Root with scenes or room_dataset")
    ap.add_argument("--out-dir", type=str, default="./pov_out")
    ap.add_argument("--width", type=int, default=512)
    ap.add_argument("--height", type=int, default=512)
    ap.add_argument("--fov-deg", type=float, default=70.0)
    ap.add_argument("--eye-height", type=float, default=1.6)
    ap.add_argument("--point-size", type=float, default=3.0)
    ap.add_argument("--bg", type=int, nargs=3, default=[0, 0, 0])
    ap.add_argument("--num-views", type=int, default=6)
    ap.add_argument("--seed", type=int, default=SEED)
    ap.add_argument("--manifest", type=str)
    ap.add_argument("--taxonomy", type=str, required=True, 
                    help="Path to taxonomy.json file")
    ap.add_argument("--hpc", action="store_true", default=False,
                    help="Run inside Xvfb for headless HPC rendering")

    args = ap.parse_args()

    if args.hpc:
        try:
            from xvfbwrapper import Xvfb
            with Xvfb(width=args.width, height=args.height, colordepth=24):
                return run_main(args)
        except ImportError:
            print("[error] --hpc flag set but xvfbwrapper not installed", flush=True)
            sys.exit(1)
    else:
        return run_main(args)

def run_main(args):
    bg_rgb = tuple(args.bg)

    # Load taxonomy once at the beginning
    print(f"Loading taxonomy from {args.taxonomy}...")
    taxonomy = Taxonomy(args.taxonomy)
    print(f"Taxonomy loaded successfully.")

    # --- Single room mode ---
    if args.path:
        t0 = time.time()
        ok = process_room(
            Path(args.path), Path(args.out_dir), taxonomy,
            args.width, args.height, args.fov_deg,
            args.eye_height, args.point_size, bg_rgb,
            num_views=args.num_views, seed=args.seed
        )
        dt = time.time() - t0
        print(f"Time for room {args.path}: {dt:.2f}s", flush=True)
        sys.exit(0 if ok else 1)

    # --- Multi-room mode ---
    root = Path(args.dataset_root)
    files = find_room_files(root, Path(args.manifest) if args.manifest else None)
    if not files:
        print(f"No room parquets found (manifest or scan).", flush=True)
        sys.exit(2)

    total_images = (args.num_views * 2 + 1) * len(files)  # 2 images per view (tex+seg) + minimap
    print(f"Found {len(files)} room files.")
    print(f"Will generate {total_images} images total ({args.num_views * 2 + 1} per room)", flush=True)
    
    ok_count = 0
    fail_count = 0
    skip_count = 0
    total_views = 0
    skipped_rooms = []
    failed_rooms = []

    for i, f in enumerate(files, 1):
        try:
            print(f"[{i}/{len(files)}] Processing {f}", flush=True)
            t0 = time.time()
            views = process_room(
                f, Path(args.out_dir), taxonomy,
                args.width, args.height, args.fov_deg,
                args.eye_height, args.point_size, bg_rgb,
                num_views=args.num_views, seed=args.seed
            )
            dt = time.time() - t0

            if views > 0:
                ok_count += 1
                total_views += views
                print(f"  âœ“ Completed in {dt:.2f}s with {views} POVs", flush=True)
            else:
                skip_count += 1
                skipped_rooms.append(str(f))
                print(f"  [skip] No valid POVs â†’ {f}", flush=True)

        except Exception as e:
            fail_count += 1
            failed_rooms.append(str(f))
            print(f"  [error] {e}", flush=True)

    # --- final summary ---
    print("\nSummary:")
    print(f"  Completed: {ok_count}/{len(files)} rooms")
    print(f"  Skipped:   {skip_count} {skipped_rooms}")
    print(f"  Failed:    {fail_count} {failed_rooms}")
    print(f"  Generated: {total_views * 2 + ok_count} images total "
          f"({total_views} POVs x 2 + {ok_count} minimaps)")

if __name__ == "__main__":
    main_entry()


================================================================================
FILE: data_preperation\stage5_1_build_room_graphs.py
================================================================================

#!/usr/bin/env python3
"""
build_room_graphs_from_layouts.py

Creates room-level graphs from color-segmented layouts.
Each color blob (clustered by minimal linkage) becomes a node.
"""

import argparse, json, csv, gc
from pathlib import Path
import cv2
import numpy as np
from scipy.cluster.hierarchy import fclusterdata
from collections import defaultdict

# ------------------------------------------------------------
def load_taxonomy(tax_path: Path):
    tax = json.loads(tax_path.read_text(encoding="utf-8"))
    color_to_label = {}
    
    # Load super-category colors (1000-1999) and wall category (2053)
    for sid, rgb in tax.get("id2color", {}).items():
        sid_int = int(sid)
        
        if 1000 <= sid_int <= 1999:
            # Super categories
            super_label = tax["id2super"].get(sid, f"Unknown_{sid}")
            color_to_label[tuple(rgb)] = {
                "label_id": sid_int,
                "label": super_label
            }
        elif sid_int == 2053:
            # Wall - get category name first, then map to super
            category_name = tax["id2category"].get(sid, "wall")
            super_label = tax.get("category2super", {}).get(category_name, "Structure")
            color_to_label[tuple(rgb)] = {
                "label_id": sid_int,
                "label": super_label
            }
    
    print(f"Loaded {len(color_to_label)} taxonomy colors (super categories + wall)", flush=True)
    return color_to_label

def find_layouts(root: Path | None, manifest: Path | None):
    layouts = []
    if manifest and manifest.exists():
        with open(manifest, newline='', encoding='utf-8') as f:
            for row in csv.DictReader(f):
                # Skip scene-level layouts - only process individual rooms
                if row["room_id"] == "scene":
                    continue
                    
                # Use the layout_path from CSV directly
                layout_path = Path(row["layout_path"])
                if layout_path.exists():
                    sid = row["scene_id"]
                    rid = row["room_id"]
                    layouts.append((sid, rid, layout_path))
    elif root:
        for p in root.rglob("*_room_seg_layout.png"):
            parts = p.stem.split("_")
            # Skip scene layouts based on filename pattern
            if len(parts) >= 3 and parts[1] == "scene":
                continue
            if len(parts) >= 3:
                sid, rid = parts[0], parts[1]
                layouts.append((sid, rid, p))
    return layouts

# ------------------------------------------------------------
def compute_room_center(img):
    mask_room = ~(img == 255).all(axis=2)
    ys, xs = np.nonzero(mask_room)
    if len(xs) == 0:
        return np.array([img.shape[1] / 2, img.shape[0] / 2])
    return np.array([xs.mean(), ys.mean()])

def angle_from_center(center, point):
    v = point - center
    return np.arctan2(v[1], v[0])

# ------------------------------------------------------------
# No longer needed - replaced by vectorized approach

def extract_color_regions(img, color_to_label):
    """
    Extract regions by exact color match to taxonomy colors.
    Returns dict mapping taxonomy_color -> list of pixel coordinates
    """
    h, w = img.shape[:2]
    color_masks = defaultdict(list)
    
    # Get unique colors in the image
    unique_colors = np.unique(img.reshape(-1, 3), axis=0)
    
    print(f"  Found {len(unique_colors)} unique colors in image", flush=True)
    
    # For each unique color, check if it matches a taxonomy color
    for color in unique_colors:
        color_t = tuple(int(c) for c in color)
        
        # Skip white background
        if color_t == (255, 255, 255):
            continue
        
        # Check if this exact color is in taxonomy
        if color_t in color_to_label:
            # Find all pixels with this color
            mask = np.all(img == color, axis=-1)
            ys, xs = np.nonzero(mask)
            points = list(zip(xs, ys))
            color_masks[color_t] = points
            print(f"  Matched color {color_t} ({color_to_label[color_t]['label']}): {len(points)} pixels", flush=True)
        else:
            print(f"  Skipping unknown color {color_t}", flush=True)
    
    return color_masks

def find_color_clusters(points, linkage_thresh=15):
    """Cluster points using DBSCAN (handles sparse clouds well)."""
    if len(points) == 0:
        return []
    pts = np.array(points)
    if len(pts) == 1:
        return [pts]
    
    from sklearn.cluster import DBSCAN
    
    # DBSCAN parameters:
    # eps = maximum distance between points in same cluster
    # min_samples = minimum points to form a cluster
    clustering = DBSCAN(eps=linkage_thresh, min_samples=5).fit(pts)
    
    labels = clustering.labels_
    
    # Group points by cluster (-1 is noise, which we'll ignore or treat separately)
    clusters = []
    for label in set(labels):
        if label == -1:
            # Noise points - you can choose to ignore or keep as individual clusters
            continue
        cluster_mask = labels == label
        clusters.append(pts[cluster_mask])
    
    return clusters

# ------------------------------------------------------------
def visualize(img, room_center, nodes, edges, out_path):
    """Create a clear visualization with colored nodes and black edges."""
    h, w = img.shape[:2]
    
    # Create visualization on white background
    vis = np.ones((h, w, 3), dtype=np.uint8) * 255
    
    # Draw the original image with transparency
    alpha = 0.3
    vis = cv2.addWeighted(img, alpha, vis, 1 - alpha, 0)
    
    # Define color scheme
    EDGE_COLOR = (0, 0, 0)           # Black for edges
    NODE_OUTLINE = (0, 0, 0)         # Black outline for nodes
    ROOM_CENTER_COLOR = (0, 200, 0)  # Green for room center
    TEXT_COLOR = (0, 0, 0)           # Black text
    TEXT_BG_COLOR = (255, 255, 255)  # White background for text
    
    # Draw edges first (behind nodes)
    # Only visualize edges that have a distance relation (by/near)
    for e in edges:
        # Skip if no distance relation
        if e["distance_relation"] is None:
            continue
            
        a = next((n for n in nodes if n["id"] == e["obj_a"]), None)
        b = next((n for n in nodes if n["id"] == e["obj_b"]), None)
        if not a or not b:
            continue
        
        ca, cb = np.array(a["center"], int), np.array(b["center"], int)
        cv2.line(vis, tuple(ca), tuple(cb), EDGE_COLOR, 1, cv2.LINE_AA)
        
        # Label format: (distance_relation, direction_relation)
        mid = ((ca + cb) / 2).astype(int)
        text = f"({e['distance_relation']}, {e['direction_relation']})"
        (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.3, 1)
        cv2.rectangle(vis, (mid[0] - 2, mid[1] - th - 2), 
                     (mid[0] + tw + 2, mid[1] + 2), TEXT_BG_COLOR, -1)
        cv2.putText(vis, text, tuple(mid), cv2.FONT_HERSHEY_SIMPLEX, 
                   0.3, TEXT_COLOR, 1, cv2.LINE_AA)
    
    # Draw room center as a larger black circle
    rcx, rcy = map(int, room_center)
    # Draw filled black circle (larger than nodes)
    cv2.circle(vis, (rcx, rcy), 12, (0, 0, 0), -1, cv2.LINE_AA)
    # Optional: draw white outline for visibility
    cv2.circle(vis, (rcx, rcy), 12, (255, 255, 255), 1, cv2.LINE_AA)
    
    # Draw nodes with cluster colors
    for n in nodes:
        cx, cy = map(int, n["center"])
        
        # Use the stored taxonomy color instead of sampling from image
        node_color = n.get("color", (128, 128, 128))  # Default gray if color missing
        
        # Draw filled circle with cluster color
        cv2.circle(vis, (cx, cy), 8, node_color, -1, cv2.LINE_AA)
        # Draw black outline
        cv2.circle(vis, (cx, cy), 8, NODE_OUTLINE, 2, cv2.LINE_AA)
        
        # Add label BELOW the node
        label = n["label"]
        (tw, th), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)
        
        # Position text below node (centered)
        text_x = cx - tw // 2
        text_y = cy + 18  # Below the node
        
        # Draw semi-transparent background for text
        overlay = vis.copy()
        cv2.rectangle(overlay, (text_x - 2, text_y - th - 2), 
                     (text_x + tw + 2, text_y + baseline), TEXT_BG_COLOR, -1)
        cv2.addWeighted(overlay, 0.8, vis, 0.2, 0, vis)
        
        # Draw text
        cv2.putText(vis, label, (text_x, text_y), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, TEXT_COLOR, 1, cv2.LINE_AA)
    
    # Convert back to BGR for saving with OpenCV
    vis_bgr = cv2.cvtColor(vis, cv2.COLOR_RGB2BGR)
    cv2.imwrite(str(out_path), vis_bgr)
    print(f"  â†³ saved visualization {out_path}", flush=True)

# ------------------------------------------------------------
def build_room_graph(scene_id, room_id, layout_path, color_to_label):
    img = cv2.imread(str(layout_path))
    if img is None:
        print(f"[warn] cannot read {layout_path}", flush=True)
        return None
    
    # Convert BGR to RGB (OpenCV loads as BGR, taxonomy is RGB)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w = img.shape[:2]

    room_center = compute_room_center(img)
    near_thresh = 0.05 * w
    by_thresh = 0.02 * w

    # Extract color regions with exact matching
    color_regions = extract_color_regions(img, color_to_label)
    print(f"  Found {len(color_regions)} distinct taxonomy colors", flush=True)

    nodes = []
    node_id = 0

    # Store cluster points with nodes for distance calculations
    node_clusters = {}

    for tax_color, points in color_regions.items():
        label_info = color_to_label[tax_color]
        clusters = find_color_clusters(points, linkage_thresh=15)
        print(f"  Color {tax_color} ({label_info['label']}): {len(clusters)} clusters from {len(points)} pixels", flush=True)
        
        for ci, cluster in enumerate(clusters):
            centroid = cluster.mean(axis=0)
            node_key = f"{label_info['label']}_{node_id}"
            nodes.append({
                "id": node_key,
                "label": label_info["label"],
                "label_id": label_info["label_id"],
                "center": centroid.tolist(),
                "color": tax_color
            })
            # Store cluster points for distance calculation
            node_clusters[node_key] = cluster
            node_id += 1

    print(f"  Total nodes created: {len(nodes)}", flush=True)

    from scipy.spatial.distance import cdist
    
    edges = []
    
    for i, a in enumerate(nodes):
        ca = np.array(a["center"])
        la = a["label"].lower()
        cluster_a = node_clusters[a["id"]]
        
        for j, b in enumerate(nodes):
            if j <= i:
                continue
            cb = np.array(b["center"])
            lb = b["label"].lower()
            cluster_b = node_clusters[b["id"]]
            
            # Compute true minimum distance between clusters using cdist
            distances = cdist(cluster_a, cluster_b, metric='euclidean')
            min_cluster_distance = distances.min()
            
            # Delete distance matrix immediately
            del distances

            # Determine proximity relation based on minimum cluster distance
            prox_a_to_b = None
            prox_b_to_a = None
            if "structure" in (la, lb):
                if min_cluster_distance < by_thresh:
                    prox_a_to_b = prox_b_to_a = "by"
            else:
                if min_cluster_distance < near_thresh:
                    prox_a_to_b = prox_b_to_a = "near"

            # Directional relations based on centroids
            ang_a = angle_from_center(room_center, ca)
            ang_b = angle_from_center(room_center, cb)
            d_ang = np.rad2deg((ang_b - ang_a + np.pi*2) % (np.pi*2))
            
            # Direction from A to B
            if d_ang < 45 or d_ang > 315:
                dir_a_to_b = "front_of"
                dir_b_to_a = "behind"
            elif 45 <= d_ang < 135:
                dir_a_to_b = "right_of"
                dir_b_to_a = "left_of"
            elif 135 <= d_ang < 225:
                dir_a_to_b = "behind"
                dir_b_to_a = "front_of"
            else:
                dir_a_to_b = "left_of"
                dir_b_to_a = "right_of"

            # Create edges
            edges.append({
                "obj_a": a["id"], 
                "obj_b": b["id"], 
                "distance_relation": prox_a_to_b,
                "direction_relation": dir_a_to_b
            })
            edges.append({
                "obj_a": b["id"], 
                "obj_b": a["id"], 
                "distance_relation": prox_b_to_a,
                "direction_relation": dir_b_to_a
            })

    graph = {
        "scene_id": scene_id,
        "room_id": room_id,
        "room_center": room_center.tolist(),
        "nodes": nodes,
        "edges": edges
    }

    out_json = layout_path.with_name(f"{scene_id}_{room_id}_graph.json")
    out_vis = layout_path.with_name(f"{scene_id}_{room_id}_graph_vis.png")
    out_json.write_text(json.dumps(graph, indent=2), encoding="utf-8")
    visualize(img, room_center, nodes, edges, out_vis)
    print(f"âœ” wrote {out_json}", flush=True)
    
    # Clean up memory before returning
    del img, color_regions, node_clusters, nodes, edges, graph
    gc.collect()
    
    return None  # Changed from returning graph since we delete it

# ------------------------------------------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_dir", help="Dataset root containing scenes/<scene_id>/rooms/<room_id>")
    ap.add_argument("--taxonomy", required=True, help="Path to taxonomy.json")
    ap.add_argument("--manifest", help="Optional manifest CSV listing scene_id,room_id,layout_path")
    ap.add_argument("--layout", help="Optional single layout path for testing")
    args = ap.parse_args()

    color_to_label = load_taxonomy(Path(args.taxonomy))

    if args.layout:
        layout_path = Path(args.layout)
        parts = layout_path.stem.split("_")
        if len(parts) >= 3:
            sid, rid = parts[0], parts[1]
        else:
            sid, rid = "unknown_scene", "unknown_room"
        build_room_graph(sid, rid, layout_path, color_to_label)
        return

    # Find layouts from manifest or directory
    layouts = find_layouts(Path(args.in_dir) if args.in_dir else None, 
                          Path(args.manifest) if args.manifest else None)
    if not layouts:
        print("No layouts found.", flush=True)
        return

    for sid, rid, layout_path in layouts:
        build_room_graph(sid, rid, layout_path, color_to_label)
        gc.collect()  # Force garbage collection after each graph

if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\stage5_2_build_scene_graphs.py
================================================================================

#!/usr/bin/env python3
"""
stage5_build_and_visualize_scene_graphs.py

Build scene graphs from point clouds and visualize them on scene layouts.
"""

import argparse
import json
import csv
from pathlib import Path
import cv2
import numpy as np
import pandas as pd
from scipy.spatial.distance import cdist


def load_taxonomy(tax_path: Path):
    """Load room type mappings from taxonomy."""
    tax = json.loads(tax_path.read_text(encoding="utf-8"))
    return tax.get("id2room", {}), tax.get("room2id", {})


def find_scene_pointclouds(root: Path, manifest: Path = None):
    """Find scene-level point cloud files."""
    scenes = []
    if manifest and manifest.exists():
        with open(manifest, newline='', encoding='utf-8') as f:
            for row in csv.DictReader(f):
                # Accept your existing scene_list.csv format
                scene_id = row["scene_id"]
                pc_path = Path(row["parquet_file_path"])
                if pc_path.exists():
                    scenes.append((scene_id, pc_path))

    else:
        for pc_path in root.rglob("*_sem_pointcloud.parquet"):
            scene_id = pc_path.stem.replace("_sem_pointcloud", "")
            scenes.append((scene_id, pc_path))
    return scenes


def extract_room_data(pc_path: Path, id2room: dict):
    """Extract room information from scene point cloud."""
    df = pd.read_parquet(pc_path)
    
    if "room_id" not in df.columns:
        print(f"[warn] No room_id column in {pc_path}", flush=True)
        return []
    
    rooms = []
    xyz = df[["x", "y", "z"]].to_numpy()
    floor_label_ids = [4002, 2052]
    has_labels = "label_id" in df.columns
    
    for room_id, group in df.groupby("room_id"):
        room_id_int = int(room_id)
        room_type = id2room.get(str(room_id_int), f"Room_{room_id_int}")
        
        indices = group.index.to_numpy()
        room_xyz = xyz[indices]
        
        if len(room_xyz) < 10:
            continue
        
        centroid_xyz = room_xyz.mean(axis=0)
        centroid_xy = centroid_xyz[:2].tolist()
        floor_centroid_xy = centroid_xy
        
        if has_labels:
            room_labels = df.iloc[indices]["label_id"].to_numpy()
            floor_mask = np.isin(room_labels, floor_label_ids)
            if floor_mask.sum() > 0:
                floor_xyz = room_xyz[floor_mask]
                floor_centroid_xy = floor_xyz.mean(axis=0)[:2].tolist()
        
        rooms.append({
            "room_id": room_id_int,
            "room_type": room_type,
            "centroid_xy": centroid_xy,
            "floor_centroid_xy": floor_centroid_xy,
            "centroid_xyz": centroid_xyz.tolist(),
            "points": room_xyz,
            "bbox": {
                "min": room_xyz.min(axis=0).tolist(),
                "max": room_xyz.max(axis=0).tolist()
            }
        })
    
    return rooms


def check_adjacency_3d(points_a, points_b, threshold=0.3):
    """Check if two rooms are adjacent based on 3D proximity."""
    max_sample = 1000
    if len(points_a) > max_sample:
        points_a = points_a[np.random.choice(len(points_a), max_sample, replace=False)]
    if len(points_b) > max_sample:
        points_b = points_b[np.random.choice(len(points_b), max_sample, replace=False)]
    
    distances = cdist(points_a, points_b, metric='euclidean')
    return distances.min() < threshold


def compute_scene_center(room_centers):
    """Compute center of all room centers."""
    if len(room_centers) == 0:
        return np.array([0.0, 0.0])
    return np.array(room_centers).mean(axis=0)


def angle_from_center(center, point):
    """Calculate angle from center to point."""
    v = point - center
    return np.arctan2(v[1], v[0])


def build_scene_graph(scene_id, pc_path, id2room, dataset_root):
    """Build scene graph from point cloud."""
    print(f"\nProcessing: {scene_id}", flush=True)
    
    rooms = extract_room_data(pc_path, id2room)
    if len(rooms) == 0:
        print(f"[warn] No rooms found", flush=True)
        return None
    
    print(f"  Found {len(rooms)} rooms", flush=True)
    
    scene_center = compute_scene_center([r["floor_centroid_xy"] for r in rooms])
    
    # Build nodes
    nodes = []
    for r in rooms:
        nodes.append({
            "id": f"room_{r['room_id']}",
            "room_id": r["room_id"],
            "room_type": r["room_type"],
            "centroid_xy": r["centroid_xy"],
            "floor_centroid_xy": r["floor_centroid_xy"],
            "centroid_xyz": r["centroid_xyz"],
            "bbox": r["bbox"]
        })
    
    # Build edges
    edges = []
    for i, a in enumerate(rooms):
        for j, b in enumerate(rooms):
            if j <= i:
                continue
            
            is_adjacent = check_adjacency_3d(a["points"], b["points"])
            dist_rel = "adjacent" if is_adjacent else None
            
            ang_a = angle_from_center(scene_center, a["centroid_xy"])
            ang_b = angle_from_center(scene_center, b["centroid_xy"])
            d_ang = np.rad2deg((ang_b - ang_a + np.pi*2) % (np.pi*2))
            
            if d_ang < 45 or d_ang > 315:
                dir_a_to_b, dir_b_to_a = "front_of", "behind"
            elif 45 <= d_ang < 135:
                dir_a_to_b, dir_b_to_a = "right_of", "left_of"
            elif 135 <= d_ang < 225:
                dir_a_to_b, dir_b_to_a = "behind", "front_of"
            else:
                dir_a_to_b, dir_b_to_a = "left_of", "right_of"
            
            edges.append({
                "room_a": f"room_{a['room_id']}",
                "room_b": f"room_{b['room_id']}",
                "distance_relation": dist_rel,
                "direction_relation": dir_a_to_b
            })
            edges.append({
                "room_a": f"room_{b['room_id']}",
                "room_b": f"room_{a['room_id']}",
                "distance_relation": dist_rel,
                "direction_relation": dir_b_to_a
            })
    
    scene_graph = {
        "scene_id": scene_id,
        "scene_center": scene_center.tolist(),
        "nodes": nodes,
        "edges": edges
    }
    
    # Save
    output_json = pc_path.parent / f"{scene_id}_scene_graph.json"
    output_json.write_text(json.dumps(scene_graph, indent=2), encoding="utf-8")
    print(f"  âœ” Saved scene graph", flush=True)
    
    return scene_graph


def visualize_scene_graph(scene_id, dataset_root):
    """Create visualization of scene graph on scene layout."""
    scene_dir = dataset_root / scene_id
    
    # Load files
    scene_graph_path = scene_dir / f"{scene_id}_scene_graph.json"
    scene_info_path = scene_dir / f"{scene_id}_scene_info.json"
    layout_path = scene_dir / "layouts" / f"{scene_id}_scene_layout.png"
    
    if not all([scene_graph_path.exists(), scene_info_path.exists(), layout_path.exists()]):
        print(f"  [skip] Missing files for visualization", flush=True)
        return
    
    scene_graph = json.loads(scene_graph_path.read_text(encoding="utf-8"))
    scene_info = json.loads(scene_info_path.read_text(encoding="utf-8"))
    
    if "origin_world" not in scene_info:
        print(f"  [skip] No coordinate frame in scene_info", flush=True)
        return
    
    # Load coordinate frame
    origin = np.array(scene_info["origin_world"], dtype=np.float64)
    u = np.array(scene_info["u_world"], dtype=np.float64)
    v = np.array(scene_info["v_world"], dtype=np.float64)
    n = np.array(scene_info["n_world"], dtype=np.float64)
    
    # Transform function
    def world_to_uv(xyz):
        R = np.column_stack([u, v, n])
        local = (xyz - origin) @ R
        return local[:, :2]
    
    # Load image
    img = cv2.imread(str(layout_path))
    if img is None:
        print(f"  [skip] Cannot read layout", flush=True)
        return
    
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w = img.shape[:2]
    vis = cv2.addWeighted(img, 0.3, np.full_like(img, 255), 0.7, 0)
    
    nodes = scene_graph["nodes"]
    edges = scene_graph["edges"]
    scene_center = np.array(scene_graph["scene_center"], dtype=np.float64)
    
    if len(nodes) == 0:
        return
    
    # Transform node positions
    node_xyz = np.array([[n["floor_centroid_xy"][0], n["floor_centroid_xy"][1], n["centroid_xyz"][2]] 
                         for n in nodes], dtype=np.float64)
    node_uv = world_to_uv(node_xyz)
    
    u_min, u_max = node_uv[:, 0].min(), node_uv[:, 0].max()
    v_min, v_max = node_uv[:, 1].min(), node_uv[:, 1].max()
    
    span = max(u_max - u_min, v_max - v_min, 1e-6)
    margin = 10
    scale = (min(w, h) - 2 * margin) / span
    
    def uv_to_img(uv):
        u_pix = (uv[0] - u_min) * scale + margin
        v_pix = (uv[1] - v_min) * scale + margin
        return (int(np.clip(u_pix, 0, w - 1)), int(np.clip((h - 1) - v_pix, 0, h - 1)))
    
    node_positions = {nodes[i]["id"]: uv_to_img(node_uv[i]) for i in range(len(nodes))}
    
    scene_center_xyz = np.array([scene_center[0], scene_center[1], node_xyz[:, 2].mean()])
    scene_center_img = uv_to_img(world_to_uv(scene_center_xyz.reshape(1, 3))[0])
    
    # Draw edges
    drawn = set()
    for e in edges:
        if e.get("distance_relation") != "adjacent":
            continue
        key = tuple(sorted([e["room_a"], e["room_b"]]))
        if key in drawn:
            continue
        drawn.add(key)
        
        pa = node_positions.get(e["room_a"])
        pb = node_positions.get(e["room_b"])
        if pa and pb:
            cv2.line(vis, pa, pb, (0, 0, 0), 2, cv2.LINE_AA)
    
    # Draw center
    cv2.circle(vis, scene_center_img, 8, (0, 0, 0), -1, cv2.LINE_AA)
    cv2.circle(vis, scene_center_img, 8, (255, 255, 255), 1, cv2.LINE_AA)
    
    # Draw nodes
    for node in nodes:
        pos = node_positions[node["id"]]
        
        if 0 <= pos[0] < w and 0 <= pos[1] < h:
            color = tuple(int(c) for c in img[pos[1], pos[0]])
            if color == (255, 255, 255):
                np.random.seed(hash(node["room_type"]) % 2**32)
                color = tuple(np.random.randint(50, 255, 3).tolist())
        else:
            np.random.seed(hash(node["room_type"]) % 2**32)
            color = tuple(np.random.randint(50, 255, 3).tolist())
        
        cv2.circle(vis, pos, 12, color, -1, cv2.LINE_AA)
        cv2.circle(vis, pos, 12, (0, 0, 0), 2, cv2.LINE_AA)
        
        label = node["room_type"]
        (tw, th), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        tx, ty = pos[0] - tw // 2, pos[1] + 25
        
        overlay = vis.copy()
        cv2.rectangle(overlay, (tx - 3, ty - th - 3), (tx + tw + 3, ty + baseline + 1), (255, 255, 255), -1)
        cv2.addWeighted(overlay, 0.85, vis, 0.15, 0, vis)
        cv2.putText(vis, label, (tx, ty), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)
    
    # Save
    output_path = scene_dir / "layouts" / f"{scene_id}_scene_graph_vis.png"
    output_path.parent.mkdir(exist_ok=True)
    cv2.imwrite(str(output_path), cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))
    print(f"  âœ” Saved visualization", flush=True)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--in_dir", required=True)
    parser.add_argument("--taxonomy", required=True)
    parser.add_argument("--manifest", default=None)
    parser.add_argument("--adjacency_thresh", type=float, default=0.3)
    args = parser.parse_args()
    
    id2room, _ = load_taxonomy(Path(args.taxonomy))
    dataset_root = Path(args.in_dir)
    manifest_path = Path(args.manifest) if args.manifest else None
    
    scenes = find_scene_pointclouds(dataset_root, manifest_path)
    print(f"Found {len(scenes)} scenes\n", flush=True)
    
    for scene_id, pc_path in scenes:
        try:
            build_scene_graph(scene_id, pc_path, id2room, dataset_root)
            visualize_scene_graph(scene_id, dataset_root)
        except Exception as e:
            print(f"[error] {scene_id}: {e}", flush=True)


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\stage6_create_layout_embeddings.py
================================================================================

#!/usr/bin/env python3
import argparse
import os, sys
from pathlib import Path
import pandas as pd
from tqdm import tqdm


import torch
from torch.utils.data import DataLoader
import torchvision.transforms as T

sys.path.append(str(Path(__file__).parent.parent / "modules"))
from datasets import LayoutDataset, collate_skip_none
from autoencoder import AutoEncoder


def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True, help="Path to autoencoder YAML config")
    ap.add_argument("--ckpt", required=True, help="Path to autoencoder checkpoint .pt/.pth")
    ap.add_argument("--manifest", required=True, help="Input CSV manifest of layouts")
    ap.add_argument("--out_manifest", required=True, help="Output CSV manifest with embeddings column")
    ap.add_argument("--batch_size", type=int, default=128)
    ap.add_argument("--num_workers", type=int, default=4)
    ap.add_argument("--device", default="cuda")
    ap.add_argument("--format", choices=["pt", "npy"], default="pt")
    return ap.parse_args()


def main():
    args = parse_args()
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")

    # --- Load autoencoder ---
    ae = AutoEncoder.from_config(args.config)
    ckpt = torch.load(args.ckpt, map_location=device)
    ae.load_state_dict(ckpt["model"] if "model" in ckpt else ckpt)
    ae.to(device)
    ae.eval()

    # --- Transform (match training setup) ---
    transform = T.Compose([
        T.Resize((512, 512)),  # adjust if config differs
        T.ToTensor()
    ])

    # --- Dataset + Loader ---
    ds = LayoutDataset(args.manifest, transform=transform, mode="all", skip_empty=False, return_embeddings=False)
    dl = DataLoader(
                    ds,
                    batch_size=args.batch_size,
                    shuffle=False,
                    num_workers=args.num_workers,
                    collate_fn=collate_skip_none
                    )

    # --- Prepare manifest update ---
    df = pd.read_csv(args.manifest)
    df = pd.read_csv(args.manifest)
    if "embedding_path" not in df.columns:
        df["embedding_path"] = None
    if "embedding_dim" not in df.columns:
        df["embedding_dim"] = None


    # --- Encode and save with progress bar ---
    total = len(ds)
    with torch.no_grad():
        pbar = tqdm(total=total, desc="Embedding layouts", unit="layout")
        for batch in dl:
            imgs = batch["layout"].to(device)
            z = ae.encoder(imgs)

            for i in range(len(imgs)):
                scene_id = batch["scene_id"][i]
                room_id = batch["room_id"][i]
                typ = batch["type"][i]
                is_empty = batch["is_empty"][i]
                layout_path = Path(batch["path"][i])

                if is_empty:
                    pbar.update(1)
                    continue

                if typ == "room":
                    fname = f"{scene_id}_{room_id}_layout_emb.{args.format}"
                else:  # scene
                    fname = f"{scene_id}_layout_emb.{args.format}"

                out_path = layout_path.parent / fname
                emb = z[i].cpu()

                if args.format == "pt":
                    torch.save(emb, out_path)
                    emb_dim = emb.numel()
                else:
                    import numpy as np
                    np.save(out_path.with_suffix(".npy"), emb.numpy())

                mask = (df["scene_id"] == scene_id) & (df["room_id"] == room_id) & (df["type"] == typ)
                df.loc[mask, "embedding_path"] = str(out_path.resolve())
                df.loc[mask, "embedding_dim"] = emb_dim
                pbar.update(1)

        pbar.close()

    df.to_csv(args.out_manifest, index=False)
    print(f"[INFO] Wrote updated manifest to {args.out_manifest}")


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\stage7_create_pov_embeddings.py
================================================================================

#!/usr/bin/env python3
"""
stage8_create_pov_embeddings.py

Reads povs.csv manifest, generates ResNet embeddings for POV images,
and creates povs_with_embeddings.csv manifest.
"""

import argparse
import csv
import torch
from pathlib import Path
from torchvision import models, transforms
from PIL import Image
from tqdm import tqdm

from itertools import islice

def batched(iterable, n):
    it = iter(iterable)
    while True:
        batch = list(islice(it, n))
        if not batch:
            break
        yield batch

def load_resnet_model(device="cuda"):
    """Load ResNet18 with removed classifier head for feature extraction."""
    resnet = models.resnet18(weights="IMAGENET1K_V1").to(device)
    resnet.fc = torch.nn.Identity()  # remove classifier head
    resnet.eval()
    return resnet


def get_transform():
    """Get image preprocessing transform for ResNet."""
    return transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
    ])


def extract_embedding(image_path, model, transform, device):
    """Extract ResNet embedding from a single image."""
    img = Image.open(image_path).convert("RGB")
    x = transform(img).unsqueeze(0).to(device)
    with torch.no_grad(), torch.cuda.amp.autocast():
        embedding = model(x).squeeze(0).cpu()  # [512]
    return embedding



def process_povs(manifest_path: str, output_manifest: str,
                 save_format: str = "pt", batch_size: int = 1):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    print("Loading ResNet18 model...")
    model = load_resnet_model(device)
    transform = get_transform()

    manifest_path = Path(manifest_path)
    with open(manifest_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    print(f"Found {len(rows)} POV images to process")

    output_rows = []
    skipped = 0
    processed = 0

    for batch_rows in tqdm(batched(rows, batch_size),
                           total=len(rows)//batch_size + 1,
                           desc="Processing POV images"):
        batch_imgs = []
        valid_rows = []

        for row in batch_rows:
            pov_path = row['pov_path']
            if int(row['is_empty']) or not Path(pov_path).exists():
                out = row.copy()
                out['embedding_path'] = ''
                output_rows.append(out)
                skipped += 1
                continue
            try:
                img = Image.open(pov_path).convert("RGB")
                x = transform(img)
                batch_imgs.append(x)
                valid_rows.append(row)
            except Exception as e:
                print(f"Error reading {pov_path}: {e}")
                out = row.copy()
                out['embedding_path'] = ''
                output_rows.append(out)
                skipped += 1

        if not valid_rows:
            continue

        x = torch.stack(batch_imgs).to(device)
        with torch.no_grad(), torch.cuda.amp.autocast():
            emb = model(x).cpu()  # [B,512]

        for r, e in zip(valid_rows, emb):
            pov_path_obj = Path(r['pov_path'])
            embedding_path = pov_path_obj.with_suffix('.pt')
            torch.save(e, embedding_path)
            out = r.copy()
            out['embedding_path'] = str(embedding_path)
            output_rows.append(out)
            processed += 1

    # Write output manifest
    output_path = Path(output_manifest)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    fieldnames = list(rows[0].keys()) + ['embedding_path']
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(output_rows)

    print(f"\nâœ“ Processed {processed}/{len(rows)} POV images successfully")
    print(f"âœ“ Skipped {skipped} images (empty or errors)")
    print(f"âœ“ Output manifest: {output_path}")

def main():
    parser = argparse.ArgumentParser(
        description="Generate ResNet embeddings for POV images and create updated manifest"
    )
    parser.add_argument(
        "--manifest",
        required=True,
        help="Path to povs.csv manifest"
    )
    parser.add_argument(
    "--batch_size",
    type=int,
    default=32,
    help="Number of images processed per GPU batch"
    )
    parser.add_argument(
        "--output",
        required=True,
        help="Path for output povs_with_embeddings.csv"
    )
    parser.add_argument(
        "--format",
        choices=["pt", "npy"],
        default="pt",
        help="Embedding save format: pt (PyTorch) or npy (NumPy)"
    )
    
    args = parser.parse_args()
    
    process_povs(
        manifest_path=args.manifest,
        output_manifest=args.output,
        save_format=args.format,
        batch_size=args.batch_size
    )


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\stage8_create_graph_embeddings.py
================================================================================

#!/usr/bin/env python3
"""
stage8_create_graph_embeddings.py

Reads graphs.csv manifest, converts graphs to text, generates embeddings,
and creates graphs_with_embeddings.csv manifest.
"""

import argparse
import csv
import json
import torch
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
from tqdm import tqdm


def load_taxonomy(taxonomy_path: str):
    """Load taxonomy mapping from taxonomy.json."""
    t = json.loads(Path(taxonomy_path).read_text(encoding="utf-8"))
    return {int(k): v for k, v in t.get("id2room", {}).items()}


def articleize(label: str) -> str:
    """Add 'the', 'a', or 'an' before a label depending on plurality."""
    clean = label.strip().replace("_", " ")
    lower = clean.lower()

    # heuristic plural detection
    if lower.endswith(("s", "x", "z", "ch", "sh")) and not lower.endswith(("ss", "us")):
        article = "a"
    else:
        # singular
        vowels = "aeiou"
        article = "an" if lower[0] in vowels else "the"
    return f"{article} {clean}"


def graph2text(graph_path: str, taxonomy: dict, max_edges: int = 10_000):
    """
    Converts either a 3D-FRONT room graph or scene graph JSON to text.
    Uses taxonomy to decode room_id when available.
    Removes underscores and adds articles ('the', 'a', 'an').
    """
    path = Path(graph_path)
    
    if not path.exists():
        return ""
    
    g = json.loads(path.read_text(encoding="utf-8"))

    nodes = g.get("nodes", [])
    edges = g.get("edges", [])
    if not edges:
        return ""

    is_scene_graph = "room_a" in edges[0] or "room_b" in edges[0]

    # build node label map
    id_to_label = {}
    for n in nodes:
        if is_scene_graph:
            rid = n.get("room_id")
            raw_label = taxonomy.get(rid, n.get("room_type", str(rid)))
        else:
            raw_label = n.get("label", n.get("id"))
        id_to_label[n["id"]] = articleize(raw_label)

    sentences = []
    seen = set()

    for e in edges[:max_edges]:
        a = e.get("room_a") if is_scene_graph else e.get("obj_a")
        b = e.get("room_b") if is_scene_graph else e.get("obj_b")
        if not a or not b:
            continue

        label_a = id_to_label.get(a)
        label_b = id_to_label.get(b)
        if not label_a or not label_b:
            continue

        key = tuple(sorted([label_a, label_b]))
        if key in seen:
            continue
        seen.add(key)

        dist = e.get("distance_relation")
        direc = e.get("direction_relation")

        if dist and direc:
            sentence = f"{label_a} is {dist} and {direc} {label_b}."
        elif dist:
            sentence = f"{label_a} is {dist} {label_b}."
        elif direc:
            sentence = f"{label_a} is {direc} {label_b}."
        else:
            sentence = f"{label_a} relates to {label_b}."

        sentences.append(sentence)

    text = " ".join(sentences)
    return text.replace("_", " ")


def process_graphs(manifest_path: str, taxonomy_path: str, output_manifest: str, 
                   model_name: str = "all-MiniLM-L6-v2", save_format: str = "pt"):
    """
    Process all graphs in manifest: convert to text, generate embeddings, save.
    
    Args:
        manifest_path: Path to graphs.csv
        taxonomy_path: Path to taxonomy.json
        output_manifest: Path for graphs_with_embeddings.csv
        model_name: SentenceTransformer model to use
        save_format: 'pt' for PyTorch or 'npy' for NumPy
    """
    # Load taxonomy
    print(f"Loading taxonomy from {taxonomy_path}")
    taxonomy = load_taxonomy(taxonomy_path)
    
    # Load embedding model
    print(f"Loading SentenceTransformer model: {model_name}")
    embedder = SentenceTransformer(model_name)
    
    # Read manifest
    print(f"Reading manifest: {manifest_path}")
    manifest_path = Path(manifest_path)
    
    with open(manifest_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    print(f"Found {len(rows)} graphs to process")
    
    # Process each graph
    output_rows = []
    skipped = 0
    
    for row in tqdm(rows, desc="Processing graphs"):
        graph_path = row['graph_path']
        
        # Convert graph to text
        try:
            text = graph2text(graph_path, taxonomy)
            
            if not text:
                print(f"Warning: Empty text for {graph_path}")
                skipped += 1
                # Add row without embedding
                output_row = row.copy()
                output_row['embedding_path'] = ''
                output_rows.append(output_row)
                continue
            
            # Generate embedding
            embedding = embedder.encode(text, normalize_embeddings=True)
            
            # Determine save path
            graph_path_obj = Path(graph_path)
            if save_format == "pt":
                embedding_path = graph_path_obj.with_suffix('.pt')
                torch.save(torch.from_numpy(embedding), embedding_path)
            else:  # npy
                embedding_path = graph_path_obj.with_suffix('.npy')
                np.save(embedding_path, embedding)
            
            # Add to output manifest
            output_row = row.copy()
            output_row['embedding_path'] = str(embedding_path)
            output_rows.append(output_row)
            
        except Exception as e:
            print(f"Error processing {graph_path}: {e}")
            skipped += 1
            output_row = row.copy()
            output_row['embedding_path'] = ''
            output_rows.append(output_row)
    
    # Write output manifest
    output_path = Path(output_manifest)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    fieldnames = list(rows[0].keys()) + ['embedding_path']
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(output_rows)
    
    print(f"\nâœ“ Processed {len(rows) - skipped}/{len(rows)} graphs successfully")
    print(f"âœ“ Skipped {skipped} graphs")
    print(f"âœ“ Output manifest: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Generate embeddings for graph files and create updated manifest"
    )
    parser.add_argument(
        "--manifest",
        required=True,
        help="Path to graphs.csv manifest"
    )
    parser.add_argument(
        "--taxonomy",
        required=True,
        help="Path to taxonomy.json"
    )
    parser.add_argument(
        "--output",
        required=True,
        help="Path for output graphs_with_embeddings.csv"
    )
    parser.add_argument(
        "--model",
        default="all-MiniLM-L6-v2",
        help="SentenceTransformer model name (default: all-MiniLM-L6-v2)"
    )
    parser.add_argument(
        "--format",
        choices=["pt", "npy"],
        default="pt",
        help="Embedding save format: pt (PyTorch) or npy (NumPy)"
    )
    
    args = parser.parse_args()
    
    process_graphs(
        manifest_path=args.manifest,
        taxonomy_path=args.taxonomy,
        output_manifest=args.output,
        model_name=args.model,
        save_format=args.format
    )


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\manifests\manifist_merge.py
================================================================================

import argparse
import pandas as pd

def load_and_clean_csv(path):
    df = pd.read_csv(path)
    df = df.fillna("")
    for col in df.columns:
        df[col] = df[col].astype(str).str.strip()
    return df

def is_valid_path(x):
    invalid = {"", "false", "0", "none"}
    return str(x).strip().lower() not in invalid

def build_room_dataset(graphs_path, layouts_path, pov_path, out_path):
    graphs = load_and_clean_csv(graphs_path)
    layouts = load_and_clean_csv(layouts_path)
    pov = load_and_clean_csv(pov_path)

    graphs_room = graphs[graphs["type"] == "room"]
    layouts_room = layouts[layouts["type"] == "room"]

    merged = pov.merge(graphs_room, on=["scene_id", "room_id"], how="left", suffixes=("", "_graph"))
    merged = merged.merge(layouts_room, on=["scene_id", "room_id"], how="left", suffixes=("", "_layout"))
    merged = merged.dropna(subset=["graph_path", "embedding_path_graph", "layout_path_layout", "embedding_path_layout"])

    df = pd.DataFrame({
        "number": range(len(merged)),
        "SCENE_ID": merged["scene_id"],
        "POV_TYPE": merged["type"],
        "POV_PATH": merged["pov_path"],
        "POV_EMBEDDING_PATH": merged["embedding_path"],
        "ROOM_ID": merged["room_id"],
        "ROOM_GRAPH_PATH": merged["graph_path"],
        "ROOM_GRAPH_EMBEDDING_PATH": merged["embedding_path_graph"],
        "ROOM_LAYOUT_PATH": merged["layout_path"],
        "ROOM_LAYOUT_EMBEDDING_PATH": merged["embedding_path_layout"]
    })

    mask = (
        df["POV_PATH"].apply(is_valid_path)
        & df["POV_EMBEDDING_PATH"].apply(is_valid_path)
        & df["ROOM_GRAPH_PATH"].apply(is_valid_path)
        & df["ROOM_GRAPH_EMBEDDING_PATH"].apply(is_valid_path)
        & df["ROOM_LAYOUT_PATH"].apply(is_valid_path)
        & df["ROOM_LAYOUT_EMBEDDING_PATH"].apply(is_valid_path)
    )
    df = df[mask].reset_index(drop=True)
    df["number"] = range(len(df))
    df.to_csv(out_path, index=False)
    print(f"Room dataset saved: {out_path} ({len(df)} rows)")

def build_scene_dataset(graphs_path, layouts_path, out_path):
    graphs = load_and_clean_csv(graphs_path)
    layouts = load_and_clean_csv(layouts_path)

    graphs_scene = graphs[graphs["type"] == "scene"]
    layouts_scene = layouts[layouts["type"] == "scene"]

    merged = graphs_scene.merge(layouts_scene, on="scene_id", how="left", suffixes=("", "_layout"))

    df = pd.DataFrame({
    "number": range(len(merged)),
    "SCENE_ID": merged["scene_id"],
    "SCENE_GRAPH_PATH": merged["graph_path"],
    "SCENE_GRAPH_EMBEDDING_PATH": merged["embedding_path"],
    "SCENE_LAYOUT_PATH": merged["layout_path_layout"],
    "SCENE_LAYOUT_EMBEDDING_PATH": merged["embedding_path_layout"]
    })


    mask = (
        df["SCENE_GRAPH_PATH"].apply(is_valid_path)
        & df["SCENE_GRAPH_EMBEDDING_PATH"].apply(is_valid_path)
        & df["SCENE_LAYOUT_PATH"].apply(is_valid_path)
        & df["SCENE_LAYOUT_EMBEDDING_PATH"].apply(is_valid_path)
    )
    df = df[mask].reset_index(drop=True)
    df["number"] = range(len(df))
    df.to_csv(out_path, index=False)
    print(f"Scene dataset saved: {out_path} ({len(df)} rows)")

def main():
    parser = argparse.ArgumentParser(description="Unify graphs, layouts, and POV manifests into room and scene datasets.")
    parser.add_argument("--graphs", required=True, help="Path to graphs.csv")
    parser.add_argument("--layouts", required=True, help="Path to layouts.csv")
    parser.add_argument("--pov", required=True, help="Path to pov.csv")
    parser.add_argument("--room_out", required=True, help="Output CSV for room_dataset")
    parser.add_argument("--scene_out", required=True, help="Output CSV for scene_dataset")
    args = parser.parse_args()

    build_room_dataset(args.graphs, args.layouts, args.pov, args.room_out)
    build_scene_dataset(args.graphs, args.layouts, args.scene_out)

if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\utils\geometry_utils.py
================================================================================

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
geometry_utils.py - Pure geometric utility functions

Basic coordinate transformations and geometric calculations used across scripts.
"""

import math
from typing import Tuple
import numpy as np


def world_to_local_coords(points: np.ndarray, origin: np.ndarray, 
                         u: np.ndarray, v: np.ndarray, n: np.ndarray) -> np.ndarray:
    """Transform world coordinates to local UVH frame."""
    R = np.stack([u, v, n], axis=1)
    return (points - origin) @ R


def points_to_image_coords(u_vals: np.ndarray, v_vals: np.ndarray, 
                          uv_bounds: Tuple[float, float, float, float],
                          resolution: int, margin: int = 10) -> Tuple[np.ndarray, np.ndarray]:
    """Convert UV coordinates to image pixel coordinates."""
    umin, umax, vmin, vmax = uv_bounds
    span = max(umax - umin, vmax - vmin, 1e-6)
    scale = (resolution - 2 * margin) / span
    
    u_pix = (u_vals - umin) * scale + margin
    v_pix = (v_vals - vmin) * scale + margin
    
    x_img = np.clip(np.round(u_pix).astype(np.int32), 0, resolution - 1)
    y_img = np.clip(np.round((resolution - 1) - v_pix).astype(np.int32), 0, resolution - 1)
    
    return x_img, y_img


def pca_plane_fit(points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Return (origin, normal) of best-fit plane through points using PCA."""
    if points.shape[0] < 3:
        return points.mean(axis=0), np.array([0, 0, 1.0], dtype=np.float64)
    
    origin = points.mean(axis=0)
    X = points - origin
    C = np.cov(X.T)
    w, V = np.linalg.eigh(C)
    normal = V[:, 0]  # smallest eigenvalue
    return origin.astype(np.float64), normal / (np.linalg.norm(normal) + 1e-12)


def build_orthonormal_frame(normal: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Build orthonormal U,V vectors perpendicular to normal."""
    Y = np.array([0, 1, 0], dtype=np.float64)
    X = np.array([1, 0, 0], dtype=np.float64)
    
    v = Y - (Y @ normal) * normal
    if np.linalg.norm(v) < 1e-9:
        v = X - (X @ normal) * normal
    v = v / (np.linalg.norm(v) + 1e-12)
    
    u = np.cross(normal, v)
    u = u / (np.linalg.norm(u) + 1e-12)
    v = np.cross(u, normal)
    v = v / (np.linalg.norm(v) + 1e-12)
    
    return u, v


def get_2d_bounds(points_2d: np.ndarray) -> Tuple[float, float, float, float]:
    """Get 2D bounds (xmin, xmax, ymin, ymax) from 2D points."""
    if points_2d.shape[0] == 0:
        return (0.0, 1.0, 0.0, 1.0)
    
    mins = points_2d.min(axis=0)
    maxs = points_2d.max(axis=0)
    return float(mins[0]), float(maxs[0]), float(mins[1]), float(maxs[1])


def angle_between_vectors(v1: np.ndarray, v2: np.ndarray) -> float:
    """Angle in degrees between two vectors."""
    cos_angle = np.clip(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)), -1.0, 1.0)
    return math.degrees(math.acos(cos_angle))


================================================================================
FILE: data_preperation\utils\semantic_utils.py
================================================================================

#!/usr/bin/env python3
"""
taxonomy_builder.py

Build a unified taxonomy for 3D-FRONT scenes and 3D-FUTURE models.

- Scans scene JSON files for furniture, titles, and rooms
- Resolves labels/categories/super-categories using model_info.json (Stage 1 logic)
- Adds structural classes (floor/wall/ceiling)
- Assigns IDs to supers, categories, rooms, labels, and titles in fixed ranges
- Generates categoryâ†’super mapping and a color palette
"""

from __future__ import annotations
import argparse
import json
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import numpy as np
import colorsys


# ------------------------------
# Class
# ------------------------------
class Taxonomy:

    def __init__(self, taxonomy_path: str | Path):
        with open(Path(taxonomy_path), "r", encoding="utf-8") as f:
            self.data = json.load(f)
        self.ranges = self.data.get("ranges", {})
        self.structural_categories = {'wall', 'floor', 'ceiling'}



    # ----------------------------
    # Core name â†” id
    # ----------------------------
    def id_to_name(self, val: int) -> str:
        val_str = str(val)
        if self.ranges["super"][0] <= val <= self.ranges["super"][1]:
            return self.data["id2super"].get(val_str, "UnknownSuper")
        if self.ranges["category"][0] <= val <= self.ranges["category"][1]:
            return self.data["id2category"].get(val_str, "UnknownCategory")
        if self.ranges["room"][0] <= val <= self.ranges["room"][1]:
            return self.data["id2room"].get(val_str, "UnknownRoom")
        if self.ranges["label"][0] <= val <= self.ranges["label"][1]:
            return self.data["id2label"].get(val_str, "UnknownLabel")
        if self.ranges["title"][0] <= val <= self.ranges["title"][1]:
            return self.data["id2title"].get(val_str, "UnknownTitle")
        return "Unknown"

    def name_to_id(self, val: str) -> int:
        return (
            self.data.get("room2id", {}).get(val)
            or self.data.get("title2id", {}).get(val)
            or self.data.get("label2id", {}).get(val)
            or self.data.get("category2id", {}).get(val)
            or self.data.get("super2id", {}).get(val, 0)
            or 0
        )


    def translate(self, val, output: str = "id"):
        """
        Generic translator.
        Args:
            val: int or str
            output: 'id' or 'name'
        """
        if isinstance(val, int):
            return self.id_to_name(val) if output == "name" else val
        elif isinstance(val, str):
            return self.name_to_id(val) if output == "id" else val
        return None

    def get_floor_ids(self) -> list[int]:
        """
        Return taxonomy IDs that correspond to 'floor' surfaces.
        Includes both category and label definitions.
        """
        floor_ids = []
        for name, idx in self.data.get("category2id", {}).items():
            if name.lower() == "floor":
                floor_ids.append(int(idx))
        for name, idx in self.data.get("label2id", {}).items():
            if name.lower() == "floor":
                floor_ids.append(int(idx))
        return floor_ids

    # ----------------------------
    # Super-category
    # ----------------------------
    def get_sup(self, val, output: str = "name"):
        """
        Get super-category for a given category/title/etc.
        Args:
            val: str name or int id
            output: 'name' or 'id'
        """
        if isinstance(val, int):
            name = self.id_to_name(val)
        else:
            name = val

        super_name = (
            self.data.get("title2super", {}).get(name)
            or self.data.get("category2super", {}).get(name)
            or (name if name in self.data["super2id"] else "UnknownSuper")
        )

        if output == "name":
            return super_name
        return self.data["super2id"].get(super_name, 0)
    
    def get_category(self, title: str) -> str:
        return self.data.get("title2category", {}).get(title, "UnknownCategory")

    def get_room_id(self, room_type: str) -> int:
        return self.data.get("room2id", {}).get(room_type, 0)

    def get_color(self, val, mode: str = "none"):
        """
        Get color for any taxonomy ID or name.
        For structural elements (floor/wall/ceiling), always returns category color (2000 range).
        For other elements, returns super-category color.
        Returns RGB tuple.
        """
        default_color = (127, 127, 127)  # gray fallback

        if val is None:
            return default_color

        # Convert to int ID if it's a string
        if isinstance(val, str):
            if val.isdigit():
                val = int(val)
            else:
                # Convert name to ID
                val = self.name_to_id(val)
                if val == 0:  # name_to_id returns 0 for unknown
                    return default_color

        # Get the name to check if it's structural
        name = self.id_to_name(val)
        
        # Special handling for structural elements - ALWAYS use category color (2000 range)
        if name.lower() in self.structural_categories:
            # Find the corresponding category ID (2000 range) for this structural element
            category_id = self.data.get("category2id", {}).get(name.lower())
            if category_id is not None:
                color = self.data.get("id2color", {}).get(str(category_id), default_color)
                return tuple(color) if isinstance(color, list) else default_color
            else:
                print(f"DEBUG: No category found for structural element '{name}'")
                return default_color

        # For non-structural items, resolve to super-category and get color
        super_id = self.resolve_super(val)
        if super_id is None:
            print(f"DEBUG get_color: could not resolve super for {val} ({name})")
            return default_color

        # Look up color by super-category ID in id2color
        color = self.data.get("id2color", {}).get(str(super_id), default_color)
        return tuple(color) if isinstance(color, list) else default_color


# Additional helper method for debugging specific failures
    def debug_title_resolution(self, title_id):
        """
        Debug why a specific title ID can't resolve to super-category.
        """
        print(f"\n--- Debug title resolution for ID: {title_id} ---")
        
        title_name = self.id_to_name(title_id)
        print(f"Title ID {title_id} â†’ Name: '{title_name}'")
        
        # Check title2super mapping
        title2super_direct = self.data.get("title2super", {}).get(str(title_id))
        print(f"Direct title2super lookup: {title2super_direct}")
        
        # Check title2category mapping
        title2category = self.data.get("title2category", {}).get(str(title_id))
        print(f"title2category lookup: {title2category}")
        
        if title2category:
            # Check if category has super mapping
            category2super = self.data.get("category2super", {}).get(title2category)
            print(f"category2super for '{title2category}': {category2super}")
            
            if category2super:
                super_id = self.data.get("super2id", {}).get(category2super)
                print(f"super2id for '{category2super}': {super_id}")
        
        # Check available mappings for debugging
        print(f"\nAvailable title2super keys (first 10): {list(self.data.get('title2super', {}).keys())[:10]}")
        print(f"Available title2category keys (first 10): {list(self.data.get('title2category', {}).keys())[:10]}")
        
        print("--- End debug ---\n")


    # Fixed resolve_super method with better case handling and fallbacks
    def resolve_super(self, val: int) -> int | None:
        """
        Resolve any ID (title, label, category, super) to its super-category ID.
        Returns the super-category ID or None if not found.
        """
        if val is None or val == 0:
            return None
        
        val_str = str(val)
        
        # Already a super-category ID
        if self.ranges["super"][0] <= val <= self.ranges["super"][1]:
            return val

        # Title ID â†’ super-category ID
        if self.ranges["title"][0] <= val <= self.ranges["title"][1]:
            # Try direct title â†’ super mapping (using title ID as key)
            super_name = self.data.get("title2super", {}).get(val_str)
            if super_name:
                # Try exact match first
                super_id = self.data.get("super2id", {}).get(super_name)
                if super_id:
                    return super_id
                
                # Try case variations
                for key in self.data.get("super2id", {}):
                    if key.lower() == super_name.lower():
                        return self.data["super2id"][key]
                
                # If "others", try common variations
                if super_name.lower() in ["others", "other"]:
                    for variant in ["Others", "Other", "others", "other", "UnknownSuper"]:
                        super_id = self.data.get("super2id", {}).get(variant)
                        if super_id:
                            return super_id
            
            # Fallback: title â†’ category â†’ super (using title ID as key)
            category_name = self.data.get("title2category", {}).get(val_str)
            if category_name:
                super_name = self.data.get("category2super", {}).get(category_name)
                if super_name:
                    super_id = self.data.get("super2id", {}).get(super_name)
                    if super_id:
                        return super_id
                    
                    # Try case variations for categoryâ†’super lookup too
                    for key in self.data.get("super2id", {}):
                        if key.lower() == super_name.lower():
                            return self.data["super2id"][key]
            
            # Additional fallback: try using title name as key (legacy support)
            title_name = self.id_to_name(val)
            super_name = self.data.get("title2super", {}).get(title_name)
            if super_name:
                super_id = self.data.get("super2id", {}).get(super_name)
                if super_id:
                    return super_id
                
                # Try case variations
                for key in self.data.get("super2id", {}):
                    if key.lower() == super_name.lower():
                        return self.data["super2id"][key]
            
            category_name = self.data.get("title2category", {}).get(title_name)
            if category_name:
                super_name = self.data.get("category2super", {}).get(category_name)
                if super_name:
                    super_id = self.data.get("super2id", {}).get(super_name)
                    if super_id:
                        return super_id

        # Label ID â†’ category â†’ super-category
        if self.ranges["label"][0] <= val <= self.ranges["label"][1]:
            # Get label name, then find its category
            label_name = self.id_to_name(val)
            if label_name.lower() in self.structural_categories:
                # For structural labels, find the corresponding category
                category_name = label_name.lower()  # floor label -> floor category
                super_name = self.data.get("category2super", {}).get(category_name)
                if super_name:
                    super_id = self.data.get("super2id", {}).get(super_name)
                    if super_id:
                        return super_id
            else:
                # For non-structural labels, use label2category mapping if it exists
                category_id = self.data.get("label2category", {}).get(val_str)
                if category_id:
                    category_name = self.id_to_name(int(category_id))
                    super_name = self.data.get("category2super", {}).get(category_name)
                    if super_name:
                        super_id = self.data.get("super2id", {}).get(super_name)
                        if super_id:
                            return super_id

        # Category ID â†’ super-category
        if self.ranges["category"][0] <= val <= self.ranges["category"][1]:
            category_name = self.id_to_name(val)
            super_name = self.data.get("category2super", {}).get(category_name)
            if super_name:
                super_id = self.data.get("super2id", {}).get(super_name)
                if super_id:
                    return super_id

        # Room IDs don't have super-categories
        if self.ranges["room"][0] <= val <= self.ranges["room"][1]:
            return None

        return None


    # Additional helper method for debugging
    def debug_color_resolution(self, val):
        """
        Debug method to trace color resolution process.
        """
        print(f"\n--- Debug color resolution for: {val} ---")
        
        # Convert to ID if needed
        original_val = val
        if isinstance(val, str) and not val.isdigit():
            val = self.name_to_id(val)
            print(f"Converted '{original_val}' to ID: {val}")
        elif isinstance(val, str):
            val = int(val)
        
        if val == 0:
            print("Result: Unknown item, using default color")
            return
        
        name = self.id_to_name(val)
        print(f"ID {val} â†’ Name: '{name}'")
        
        # Check range
        for range_name, (start, end) in self.ranges.items():
            if start <= val <= end:
                print(f"Range: {range_name} ({start}-{end})")
                break
        
        # Check if structural
        if name.lower() in self.structural_categories:
            print(f"Structural element detected: {name}")
            category_id = self.data.get("category2id", {}).get(name.lower())
            print(f"Corresponding category ID (2000 range): {category_id}")
            if category_id:
                color = self.data.get("id2color", {}).get(str(category_id))
                print(f"Category color: {color}")
            return
        
        # Resolve super for non-structural
        super_id = self.resolve_super(val)
        print(f"Resolved super ID: {super_id}")
        
        if super_id:
            super_name = self.id_to_name(super_id)
            print(f"Super name: {super_name}")
            color = self.data.get("id2color", {}).get(str(super_id))
            print(f"Super color: {color}")
        
        final_color = self.get_color(original_val)
        print(f"Final color: {final_color}")
        print("--- End debug ---\n")
# ------------------------------
# Utilities
# ------------------------------

def _make_flat_mapping(items, base=0, unknown_name="Unknown"):
    """Return mapping {name: id} with explicit 0 reserved for Unknown."""
    items = sorted(list(items))
    mapping = {unknown_name: 0}
    mapping.update({name: base + i + 1 for i, name in enumerate(items)})
    return mapping, list(mapping.values())


def _invert_mapping(mapping: dict) -> dict:
    """Build reverse dict {id: name}."""
    return {v: k for k, v in mapping.items()}

# ------------------------------
# Taxonomy Builder
# ------------------------------

def build_taxonomy_full(model_info_path: Path, scenes_dir: Path):
    """
    Build taxonomy by scanning scene JSONs with Stage 1 resolution logic.
    - Supers come only from model_info.json (+Structure).
    - Collects labels, categories, and titles from furniture + room children.
    - Returns one dict with id mappings and categoryâ†’super mapping.
    """

    # --- load model_info.json ---
    with open(model_info_path, "r", encoding="utf-8") as f:
        model_info = json.load(f)

    # jid -> (category, super)
    jid_to_cat_super_map = {}
    category_set, super_set = set(), set()
    category2super = {}

    for model in model_info:
        # The key in model_info.json is 'model_id', which corresponds to 'jid' in scene files
        jid = model.get("model_id")
        cat = model.get("category") or "UnknownCategory"
        sup = model.get("super-category") or "UnknownSuper"
        if jid:
            jid_to_cat_super_map[jid] = (cat, sup)
        category_set.add(cat)
        super_set.add(sup)
        if cat != "UnknownCategory":
            category2super[cat] = sup

    # --- inject structural classes ---
    STRUCTURAL = {
        "floor": ("floor", "Structure"),
        "wall": ("wall", "Structure"),
        "ceiling": ("ceiling", "Structure"),
    }
    label_set = set() # only structural labels
    for lbl, (cat, sup) in STRUCTURAL.items():
        label_set.add(lbl)
        category_set.add(cat)
        super_set.add(sup)
        category2super[cat] = sup

    # --- scan scenes to get titles and build title->cat/super mappings ---
    title_set = set()
    title2super = {}
    title2category = {}

    scene_files = list(scenes_dir.glob("*.json"))
    for scene_file in tqdm(scene_files, desc="[Taxonomy] Scanning scenes"):
        with open(scene_file, "r", encoding="utf-8") as f:
            scene = json.load(f)

        def process_furniture(furniture_list):
            for furn in furniture_list:
                jid = furn.get("jid")
                title = furn.get("title")

                if title:
                    title_set.add(title)

                if jid and jid in jid_to_cat_super_map:
                    cat, sup = jid_to_cat_super_map[jid]
                    if title:
                        title2super[title] = sup
                        title2category[title] = cat

        process_furniture(scene.get("furniture", []))
        for room in scene.get("scene", {}).get("room", []):
            process_furniture(room.get("children", []))


    # --- build final mappings ---
    super2id, super_ids = _make_flat_mapping(super_set, base=1000)
    category2id, category_ids = _make_flat_mapping(category_set, base=2000)
    label2id, label_ids = _make_flat_mapping(label_set, base=4000)
    title2id, title_ids = _make_flat_mapping(title_set, base=5000)


    return {
        "super2id": super2id,
        "category2id": category2id,
        "label2id": label2id,
        "title2id": title2id,
        "category2super": category2super,
        "title2super": title2super,
        "title2category": title2category
    }

def build_room_taxonomy(scenes_dir: Path):
    """Collect unique room types and count empty rooms."""
    room_set = set()
    empty_count = 0
    scene_files = list(scenes_dir.glob("*.json"))
    for scene_file in tqdm(scene_files, desc="[Rooms] Scanning scenes"):
        with open(scene_file, "r", encoding="utf-8") as f:
            scene = json.load(f)
        for room in scene.get("scene", {}).get("room", []):
            if room.get("empty", 0) == 1:
                empty_count += 1
            rtype = room.get("type")
            if isinstance(rtype, str) and rtype.strip():
                room_set.add(rtype.strip())
            else:
                room_set.add("OtherRoom")

    room2id, room_ids = _make_flat_mapping(room_set, base=3000, unknown_name="UnknownRoom")
    return room2id, room_ids, empty_count


# ------------------------------
# Color Palette
# ------------------------------
def assign_colors(super2id: dict, category2id: dict, category2super: dict):
    """
    Assign colors:
      - Structural (floor, wall, ceiling) -> fixed distinct colors
      - Unknown -> gray
      - Supers -> anchor hues
      - Categories -> variations of super hue
    """

    id2color = {}

    # 1. Fixed structural colors
    STRUCTURAL_COLORS = {
        "floor": [50, 50, 50],      # dark gray
        "wall": [200, 200, 200],    # light gray
        "ceiling": [255, 255, 255], # white
    }

    for cat, supercat in category2super.items():
        if cat in STRUCTURAL_COLORS:
            cid = category2id[cat]
            id2color[str(cid)] = STRUCTURAL_COLORS[cat]

    # 2. Anchors for non-structural supers
    super_anchors = [
        (228, 26, 28),    # red
        (55, 126, 184),   # blue
        (77, 175, 74),    # green
        (152, 78, 163),   # purple
        (255, 127, 0),    # orange
        (255, 255, 51),   # yellow
        (166, 86, 40),    # brown
        (0, 191, 196),    # cyan
    ]

    supers = sorted(super2id.keys())
    anchor_index = 0

    for supercat in supers:
        sid = super2id[supercat]
        cats = [c for c, s in category2super.items() if s == supercat]

        # Unknown super
        if supercat.lower() in ("unknown", "others", "other"):
            id2color[str(sid)] = [127, 127, 127]
            for cat in cats:
                cid = category2id[cat]
                id2color[str(cid)] = [127, 127, 127]
            continue

        # Structural handled above
        if supercat == "Structure":
            id2color[str(sid)] = [0, 0, 0]  # black for structure super
            continue

        # Assign anchor color to this super
        base_rgb = super_anchors[anchor_index % len(super_anchors)]
        id2color[str(sid)] = list(base_rgb)
        anchor_index += 1

        # Categories under this super
        n = len(cats)
        if n == 0:
            continue
        if n == 1:
            cid = category2id[cats[0]]
            id2color[str(cid)] = list(base_rgb)
            continue

        import colorsys, numpy as np
        r, g, b = [x / 255 for x in base_rgb]
        h, s, v = colorsys.rgb_to_hsv(r, g, b)

        vals = np.linspace(0.5, 0.95, n)
        sats = np.linspace(0.6, 1.0, n)

        for i, cat in enumerate(sorted(cats)):
            if cat in STRUCTURAL_COLORS:
                continue  # already assigned
            cid = category2id[cat]
            new_s = sats[i % len(sats)]
            new_v = vals[i % len(vals)]
            rr, gg, bb = colorsys.hsv_to_rgb(h, new_s, new_v)
            col = [int(rr * 255), int(gg * 255), int(bb * 255)]
            id2color[str(cid)] = col

    return id2color

# ------------------------------
# Wrapper
# ------------------------------

def build_taxonomy(model_info_path: str, scenes_dir: str, out_path: str) -> None:
    model_info_path = Path(model_info_path)
    scenes_dir = Path(scenes_dir)

    if not model_info_path.exists():
        raise FileNotFoundError(f"model_info.json not found: {model_info_path}")

    taxonomy_dict = build_taxonomy_full(model_info_path, scenes_dir)
    room2id, _, empty_count = build_room_taxonomy(scenes_dir)

    print(f"  Found {len(room2id)} unique room types, {empty_count} empty rooms flagged")

    id2color = assign_colors(
        taxonomy_dict["super2id"],
        taxonomy_dict["category2id"],
        taxonomy_dict["category2super"],
    )

    taxonomy = {
        **taxonomy_dict,
        "room2id": room2id,
        "id2color": id2color,
        "id2label": _invert_mapping(taxonomy_dict["label2id"]),
        "id2category": _invert_mapping(taxonomy_dict["category2id"]),
        "id2super": _invert_mapping(taxonomy_dict["super2id"]),
        "id2room": _invert_mapping(room2id),
        "id2title": _invert_mapping(taxonomy_dict["title2id"]),
        "ranges": {
            "super": [1000, 1999],
            "category": [2000, 2999],
            "room": [3000, 3999],
            "label": [4000, 4999],
            "title": [5000, 5999],
        },
    }

    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(taxonomy, f, indent=2)

    print(f"[INFO] Saved taxonomy to {out_path}")
    print(f"  {len(taxonomy_dict['label2id'])} labels, "
          f"{len(taxonomy_dict['category2id'])} categories, "
          f"{len(taxonomy_dict['super2id'])} super-categories, "
          f"{len(room2id)} room types, "
          f"{len(taxonomy_dict['title2id'])} titles")


# ------------------------------
# CLI
# ------------------------------

def main():
    parser = argparse.ArgumentParser(description="Build taxonomy from 3D-FUTURE and 3D-FRONT.")
    parser.add_argument("--model-info", required=True, help="Path to 3D-FUTURE model_info.json")
    parser.add_argument("--scenes-dir", required=True, help="Path to 3D-FRONT scenes directory")
    parser.add_argument("--out", required=True, help="Output taxonomy.json path")
    args = parser.parse_args()

    print("[INFO] Building taxonomy")
    build_taxonomy(args.model_info, args.scenes_dir, args.out)


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\utils\utils.py
================================================================================

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
utils.py - Shared utilities for 3D scene processing pipeline (refactored)

Consolidated common functionality with reduced duplication.
"""

import json
import csv
import re
import glob
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import numpy as np

# ----------------------------
# File Discovery (consolidated)
# ----------------------------

def discover_files(root: Path, pattern: str = None, manifest: Path = None, 
                   column_name: str = "room_parquet") -> List[Path]:
    """
    Unified file discovery with all methods in one function.
    Priority: manifest > pattern > default patterns
    """
    # Method 1: From manifest
    if manifest and manifest.exists():
        files = []
        try:
            with open(manifest, newline='', encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row_num, row in enumerate(reader, start=2):  # start=2 because row 1 is header
                    if column_name in row and row[column_name]:
                        try:
                            path = Path(row[column_name]).expanduser().resolve()
                            if path.exists():
                                files.append(path)
                            else:
                                print(f"Warning: File in manifest row {row_num} doesn't exist: {path}")
                        except Exception as e:
                            print(f"Warning: Invalid path in manifest row {row_num}: {row[column_name]} - {e}")
            
            if not files:
                print(f"Warning: No valid files found in manifest {manifest} using column '{column_name}'")
                print(f"Available columns: {list(csv.DictReader(open(manifest)))}")
            
            return files
        except Exception as e:
            print(f"Error reading manifest {manifest}: {e}")
            # Fall through to other methods
    
    # Method 2: Pattern-based
    if pattern:
        files = sorted(root.rglob(pattern))
        if files:
            return files
        else:
            print(f"Warning: No files found with pattern '{pattern}' in {root}")
    
    # Method 3: Default patterns
    for default_pattern in [
        "part-*.parquet", "*_*[0-9].parquet", "rooms/*/*.parquet", "*_sem_pointcloud.parquet"
    ]:
        files = sorted(root.rglob(default_pattern))
        if files:
            print(f"Found {len(files)} files using default pattern '{default_pattern}'")
            return files
    
    print(f"Error: No files found in {root} using any method")
    return []

def gather_paths_from_sources(file_path: str = None, patterns: List[str] = None, 
                             list_file: str = None) -> List[Path]:
    """Gather paths from multiple sources, deduplicated."""
    all_paths = []
    
    # Single file
    if file_path:
        all_paths.append(Path(file_path))
    
    # Pattern list
    if patterns:
        for pat in patterns:
            expanded = [Path(p) for p in glob.glob(pat)]
            all_paths.extend(expanded if expanded else [Path(pat)])
    
    # List file (JSON array or line-separated)
    if list_file:
        path = Path(list_file)
        txt = path.read_text(encoding="utf-8").strip()
        
        # Try JSON first
        try:
            arr = json.loads(txt)
            if isinstance(arr, list):
                all_paths.extend(Path(p) for p in arr)
            else:
                raise ValueError("Not a list")
        except Exception:
            # Line-separated fallback
            for line in txt.splitlines():
                line = line.strip()
                if line and not line.startswith("#"):
                    all_paths.append(Path(line))
    
    # Deduplicate
    seen = set()
    unique = []
    for p in all_paths:
        resolved = p.resolve()
        if resolved not in seen:
            seen.add(resolved)
            unique.append(resolved)
    
    return unique

# ----------------------------
# ID Inference (consolidated)
# ----------------------------

def infer_ids_from_path(path: Path) -> Tuple[str, int]:
    """Infer scene_id from the file name by taking the part before the first underscore.
    Room_id is parsed if a numeric part follows, otherwise -1.
    """
    stem = path.stem  # filename without extension
    parts = stem.split("_", 1)  # split into [before_first_underscore, rest]
    scene_id = parts[0]

    room_id = -1
    if len(parts) > 1 and parts[1].isdigit():
        room_id = int(parts[1])

    return scene_id, room_id


# ----------------------------
# Aliases / Legacy compatibility
# ----------------------------

def infer_scene_id(path: Path) -> str:
    """Return scene_id only, derived from infer_ids_from_path."""
    scene_id, _ = infer_ids_from_path(path)
    return scene_id

def find_semantic_maps_json(start_path: Path) -> Optional[Path]:
    """Walk up from start_path to find semantic_maps.json."""
    for p in [start_path, *start_path.parents]:
        cand = p / "semantic_maps.json"
        if cand.exists():
            return cand
    return None

def get_floor_label_ids(maps_path: Path) -> Tuple[int, ...]:
    """Load semantic_maps.json and return IDs mapped to 'floor'."""
    maps = json.loads(maps_path.read_text(encoding="utf-8"))
    ids = set()

    for mapping_name, is_key_label in [("label2id", True), ("id2label", False)]:
        if mapping_name in maps:
            for key, value in maps[mapping_name].items():
                label = str(key if is_key_label else value).strip().lower()
                if label == "floor":
                    try:
                        ids.add(int(value if is_key_label else key))
                    except (ValueError, TypeError):
                        pass

    if not ids:
        raise RuntimeError(f"'floor' not found in {maps_path}")
    return tuple(sorted(ids))

# ----------------------------
# Semantic Maps (consolidated)
# ----------------------------

class SemanticMaps:
    """Consolidated semantic maps handling."""
    
    def __init__(self, start_path: Path):
        self.maps_path = self._find_maps_file(start_path)
        self._maps_data = None
    
    def _find_maps_file(self, start: Path) -> Optional[Path]:
        """Walk up from start to locate semantic_maps.json."""
        for p in [start, *start.parents]:
            cand = p / "semantic_maps.json"
            if cand.exists():
                return cand
        return None
    
    @property
    def data(self) -> Dict:
        """Lazy-load maps data."""
        if self._maps_data is None:
            if not self.maps_path:
                raise RuntimeError("semantic_maps.json not found")
            self._maps_data = json.loads(self.maps_path.read_text(encoding="utf-8"))
        return self._maps_data
    
    def get_floor_label_ids(self) -> Tuple[int, ...]:
        """Extract floor label IDs."""
        ids = set()
        
        # Check both label2id and id2label mappings
        for mapping_name, is_key_label in [("label2id", True), ("id2label", False)]:
            if mapping_name in self.data:
                for key, value in self.data[mapping_name].items():
                    label = str(key if is_key_label else value).strip().lower()
                    if label == "floor":
                        try:
                            ids.add(int(value if is_key_label else key))
                        except (ValueError, TypeError):
                            pass
        
        if not ids:
            raise RuntimeError(f"'floor' not found in {self.maps_path}")
        
        return tuple(sorted(ids))
    
    def get_color_palette(self) -> Dict[int, Tuple[int, int, int]]:
        """Load color palette."""
        if "id2color" not in self.data:
            raise RuntimeError("id2color missing in semantic_maps.json")
        
        return {int(k): tuple(v) for k, v in self.data["id2color"].items()}
    
    def update_with_values(self, values_by_category: Dict[str, List[str]], freeze: bool = False):
        """Update maps with new values."""
        if not self.maps_path:
            raise RuntimeError("Cannot update: semantic_maps.json not found")
        
        maps = self.data.copy()
        changed = False
        
        for category, values in values_by_category.items():
            key = f"{category}2id"
            current = {str(k): int(v) for k, v in maps.get(key, {}).items()}
            unique_values = sorted({v for v in values if v}, key=lambda s: s.lower())
            
            if freeze:
                unknown = [v for v in unique_values if v not in current]
                if unknown:
                    raise RuntimeError(f"freeze_maps ON; unseen in '{category}': {unknown[:20]}")
            else:
                next_id = (max(current.values()) + 1) if current else 1
                for value in unique_values:
                    if value not in current:
                        current[value] = next_id
                        next_id += 1
                        changed = True
            
            maps[key] = current
        
        if changed:
            self.maps_path.parent.mkdir(parents=True, exist_ok=True)
            self.maps_path.write_text(json.dumps(maps, indent=2), encoding="utf-8")
        
        self._maps_data = maps  # Update cache

# ----------------------------
# Configuration (simplified)
# ----------------------------

def load_config_with_profile(config_path: str = None, profile: str = None) -> Dict:
    """Load and resolve config in one step."""
    if not config_path:
        return {}
    
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"Config file not found: {path}")
    
    # Load based on extension
    if path.suffix.lower() in (".yml", ".yaml"):
        try:
            import yaml
            data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
        except ImportError as e:
            raise RuntimeError("YAML config requested but 'pyyaml' is not installed") from e
    else:
        data = json.loads(path.read_text(encoding="utf-8"))
    
    if not isinstance(data, dict):
        raise ValueError("Config must be a dictionary")
    
    # Apply profile if specified
    profile_name = profile or data.get("profile")
    if profile_name and "profiles" in data:
        if profile_name not in data["profiles"]:
            raise ValueError(f"Profile '{profile_name}' not found")
        
        base_config = {k: v for k, v in data.items() if k not in ("profiles", "profile")}
        base_config.update(data["profiles"][profile_name])
        return base_config
    
    return data

# ----------------------------
# Taxonomy (simplified)
# ----------------------------

def load_taxonomy_resolver(taxonomy_path: Path):
    """Load taxonomy and return resolver function (category â†’ super)."""
    if not taxonomy_path or not taxonomy_path.exists():
        return None

    data = json.loads(taxonomy_path.read_text(encoding="utf-8"))

    category2super = data.get("category2super", {})
    aliases = data.get("aliases", {})

    def resolve(raw_label: str, model_info: Dict = None) -> Dict[str, str]:
        candidate = (model_info or {}).get("category") or raw_label or "unknown"

        # Exact category
        if candidate in category2super:
            return {"category": candidate, "super": category2super[candidate]}

        # Alias fallback
        if candidate in aliases:
            alias = aliases[candidate]
            return {"category": alias, "super": category2super.get(alias, "Other")}

        return {"category": "", "super": "Other"}

    return resolve


def load_global_palette(taxonomy_path: Path) -> Dict[int, Tuple[int, int, int]]:
    """
    Load global color palette (id -> RGB tuple) from a taxonomy JSON file.

    Args:
        taxonomy_path: Path to the taxonomy/semantic_maps.json file.

    Returns:
        Dict mapping int label IDs to (R, G, B) tuples.
    """
    if not taxonomy_path.exists():
        raise FileNotFoundError(f"taxonomy file not found: {taxonomy_path}")

    data = json.loads(taxonomy_path.read_text(encoding="utf-8"))

    # Handle list-of-dicts format
    if isinstance(data, list):
        if not data:
            raise RuntimeError(f"taxonomy file {taxonomy_path} is empty")
        data = data[0]

    if "id2color" not in data:
        raise RuntimeError(f"id2color not found in {taxonomy_path}")

    return {int(k): tuple(v) for k, v in data["id2color"].items()}

# ----------------------------
# Room Metadata (simplified)
# ----------------------------

def load_room_meta(room_dir: Path):
    """Load the metadata JSON (room-level or scene-level)."""
    candidates = list(room_dir.glob("*_meta.json"))
    if not candidates:
        candidates = list(room_dir.glob("*_scene_info.json"))
    if not candidates:
        return None

    meta_path = candidates[0]
    meta = json.loads(meta_path.read_text(encoding="utf-8"))

    # Unwrap list
    if isinstance(meta, list) and len(meta) > 0:
        meta = meta[0]

    return meta

def extract_frame_from_meta(meta):
    """
    Extract origin, u, v, n, uv_bounds, yaw_auto, map_band
    from either a room-level *_meta.json or a scene-level *_scene_info.json.
    """
    # Handle list wrapper
    if isinstance(meta, list):
        if not meta:
            raise ValueError("Empty metadata list")
        meta = meta[0]

    # Case 1: Room-level meta.json
    if "origin_world" in meta:
        origin = np.array(meta["origin_world"], dtype=np.float32)
        u = np.array(meta["u_world"], dtype=np.float32)
        v = np.array(meta["v_world"], dtype=np.float32)
        n = np.array(meta["n_world"], dtype=np.float32)
        uv_bounds = tuple(meta["uv_bounds"])
        yaw_auto = float(meta.get("yaw_auto", 0.0))
        map_band = tuple(meta.get("map_band_m", [0.05, 0.50]))
        return origin, u, v, n, uv_bounds, yaw_auto, map_band

    # Case 2: Scene-level scene_info.json
    if "bounds" in meta:
        bounds = meta["bounds"]
        if not bounds or len(bounds) != 2:
            raise ValueError("Invalid bounds in scene_info.json")

        # Bounds are [[xmin,ymin,zmin],[xmax,ymax,zmax]]
        (xmin, ymin, zmin), (xmax, ymax, zmax) = bounds
        origin = np.array([(xmin + xmax) / 2, (ymin + ymax) / 2, (zmin + zmax) / 2], dtype=np.float32)

        # Default orthogonal frame
        u = np.array([1.0, 0.0, 0.0], dtype=np.float32)
        v = np.array([0.0, 1.0, 0.0], dtype=np.float32)
        n = np.array([0.0, 0.0, 1.0], dtype=np.float32)

        uv_bounds = (xmin, xmax, ymin, ymax)
        yaw_auto = 0.0
        map_band = (0.0, zmax - zmin)  # crude height range

        return origin, u, v, n, uv_bounds, yaw_auto, map_band

    raise KeyError("Unrecognized metadata format (no origin_world or bounds)")
# ----------------------------
# Common Helpers (kept)
# ----------------------------

def ensure_columns_exist(df, required_columns: List[str], source: str = "dataframe"):
    """Validate required columns exist."""
    missing = [col for col in required_columns if col not in df.columns]
    if missing:
        raise RuntimeError(f"Missing columns {missing} in {source}")

def safe_mkdir(path: Path, parents: bool = True, exist_ok: bool = True):
    """Safe directory creation."""
    try:
        path.mkdir(parents=parents, exist_ok=exist_ok)
    except Exception as e:
        raise RuntimeError(f"Failed to create directory {path}: {e}")

def write_json(data: Dict, path: Path, indent: int = 2):
    """Write JSON with error handling."""
    try:
        safe_mkdir(path.parent)
        path.write_text(json.dumps(data, indent=indent), encoding="utf-8")
    except Exception as e:
        raise RuntimeError(f"Failed to write JSON to {path}: {e}")

def create_progress_tracker(total: int, description: str = "Processing"):
    """Create simple progress function."""
    def update_progress(current: int, item_name: str = "", success: bool = True):
        status = "âœ“" if success else "âœ—"
        percentage = (current / total) * 100 if total > 0 else 0
        print(f"[{current}/{total}] ({percentage:.1f}%) {status} {description} {item_name}", flush=True)
    return update_progress


================================================================================
FILE: data_preperation\utils\validate_scenes.py
================================================================================

#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
scan_rooms.py

Scan all JSON scene files in --in_dir and identify which contain rooms.
Outputs two JSONs in --out_dir:
  - valid_files.json: { "scene_file.json": ["LivingRoom", "Bedroom", ...], ... }
  - invalid_files.json: ["scene_file.json", ...]
"""

import json
from pathlib import Path
import argparse
from tqdm import tqdm


def check_rooms(scene_path: Path):
    """Return list of room types if scene exposes rooms, else None."""
    try:
        data = json.loads(scene_path.read_text(encoding="utf-8"))
    except Exception as e:
        return None, f"load error: {e}"

    rooms = data.get("scene", {}).get("room") or data.get("scene", {}).get("rooms")
    if not rooms:  # empty or missing
        return None, None

    room_types = [r.get("type", "UnknownRoom") for r in rooms]
    return room_types, None


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_dir", required=True,
                    help="Directory with scene JSON files to scan")
    ap.add_argument("--out_dir", required=True,
                    help="Directory to write valid_files.json and invalid_files.json")
    args = ap.parse_args()

    in_dir = Path(args.in_dir)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    files = sorted(in_dir.glob("*.json"))
    valid = {}
    invalid = []

    for sp in tqdm(files, desc="Scanning scenes"):
        room_types, err = check_rooms(sp)
        if err:
            invalid.append(sp.name)
        elif room_types:  # non-empty list of rooms
            valid[sp.name] = room_types
        else:
            invalid.append(sp.name)

    # write outputs
    (out_dir / "valid_files.json").write_text(json.dumps(valid, indent=2), encoding="utf-8")
    (out_dir / "invalid_files.json").write_text(json.dumps(invalid, indent=2), encoding="utf-8")

    print(f"âœ… wrote {len(valid)} valid and {len(invalid)} invalid files to {out_dir}")


if __name__ == "__main__":
    main()


================================================================================
FILE: data_preperation\utils\visualize_palette.py
================================================================================

#!/usr/bin/env python3
"""
visualize_palette.py

Generates one PNG per super-category showing its categories and assigned colors,
plus an overview PNG showing all super-categories and special colors (wall/floor/ceiling),
plus a "chosen labels/colors" PNG showing the actual used palette.
"""

import json
import argparse
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
import numpy as np


# Set seaborn style for better aesthetics
sns.set_style("whitegrid")
sns.set_context("notebook", font_scale=1.1)


def visualize_chosen_palette(taxonomy_path: str, out_dir: str):
    """Generate a plot showing the actual used palette: all super-categories + wall."""
    with open(taxonomy_path, "r", encoding="utf-8") as f:
        taxonomy = json.load(f)

    super2id = taxonomy["super2id"]
    id2color = taxonomy["id2color"]
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # Get all super-categories
    sorted_supers = sorted(super2id.items(), key=lambda x: x[1])
    
    # Start with super-categories
    chosen_items = list(sorted_supers)
    
    # Add wall - try multiple possible keys
    wall_id = None
    if "wall_id" in taxonomy:
        wall_id = taxonomy["wall_id"]
    elif "wall" in taxonomy:
        wall_id = taxonomy["wall"]
    
    # Also check if wall is in category2id
    if wall_id is None and "category2id" in taxonomy:
        category2id = taxonomy["category2id"]
        if "wall" in category2id:
            wall_id = category2id["wall"]
    
    # ALWAYS add wall to the list
    if wall_id is not None:
        chosen_items.append(("wall", wall_id))
        print(f"[DEBUG] Added wall with ID: {wall_id}")
    else:
        print(f"[WARNING] Wall ID not found in taxonomy!")
        print(f"[DEBUG] Available taxonomy keys: {list(taxonomy.keys())}")
    
    n_items = len(chosen_items)
    print(f"[INFO] Chosen palette has {n_items} items (should be {len(sorted_supers)} super-categories + 1 wall)")
    
    # Calculate grid dimensions
    cols = min(4, n_items)
    rows = int(np.ceil(n_items / cols))
    
    # Create figure with seaborn styling
    fig, ax = plt.subplots(figsize=(14, max(8, rows * 2.5)))
    ax.axis("off")
    ax.set_aspect('equal')
    
    # Patch dimensions and spacing
    patch_width = 2.5
    patch_height = 1.8
    h_spacing = 3.5
    v_spacing = 3.0
    
    for idx, (name, item_id) in enumerate(chosen_items):
        row, col = divmod(idx, cols)
        x = col * h_spacing
        y = -(row * v_spacing)
        
        # Get color
        item_color = tuple(c / 255 for c in id2color.get(str(item_id), [127, 127, 127]))
        
        # Check if this is wall
        is_wall = name == "wall"
        
        # Draw patch with shadow effect (no black outline)
        shadow = mpatches.Rectangle(
            (x + 0.05, y - 0.05), patch_width, patch_height,
            facecolor='gray', alpha=0.3, edgecolor='none'
        )
        ax.add_patch(shadow)
        
        rect = mpatches.Rectangle(
            (x, y), patch_width, patch_height,
            facecolor=item_color, edgecolor='none'
        )
        ax.add_patch(rect)
        
        # Add text with better positioning
        display_name = name.upper() if is_wall else name
        fontsize = 10 if is_wall else 11
        
        ax.text(
            x + patch_width / 2, y + patch_height / 2,
            display_name,
            ha="center", va="center",
            fontsize=fontsize, fontweight="bold",
            bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8, edgecolor='none')
        )
        
        # Add ID label
        label_text = f"ID: {item_id}"
        if is_wall:
            label_text = f"(Wall) {label_text}"
        
        ax.text(
            x + patch_width / 2, y - 0.4,
            label_text,
            ha="center", va="top",
            fontsize=8 if is_wall else 9, 
            style='italic', color='#333333'
        )
    
    # Set limits with padding
    ax.set_xlim(-0.5, cols * h_spacing - 0.5)
    ax.set_ylim(-(rows * v_spacing + 0.5), patch_height + 1.0)
    
    plt.title("Chosen Labels/Colors (Used Palette)", fontsize=16, fontweight='bold', pad=20)
    
    out_path = out_dir / "palette_chosen_labels_colors.png"
    plt.savefig(out_path, dpi=300, bbox_inches="tight", facecolor='white')
    plt.close()
    print(f"[INFO] Saved chosen palette: {out_path}")


def visualize_super_overview(taxonomy_path: str, out_dir: str):
    """Generate an overview plot showing all super-categories and special colors."""
    with open(taxonomy_path, "r", encoding="utf-8") as f:
        taxonomy = json.load(f)

    super2id = taxonomy["super2id"]
    id2color = taxonomy["id2color"]
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # Sort super-categories by ID for consistent ordering
    sorted_supers = sorted(super2id.items(), key=lambda x: x[1])
    
    # Add special categories (wall/floor/ceiling) if they exist
    special_categories = []
    special_mapping = {
        "wall": taxonomy.get("wall_id"),
        "floor": taxonomy.get("floor_id"),
        "ceiling": taxonomy.get("ceiling_id")
    }
    
    # Also check alternative keys
    if special_mapping["wall"] is None:
        special_mapping["wall"] = taxonomy.get("wall")
    if special_mapping["floor"] is None:
        special_mapping["floor"] = taxonomy.get("floor")
    if special_mapping["ceiling"] is None:
        special_mapping["ceiling"] = taxonomy.get("ceiling")
    
    # Check category2id as fallback
    if "category2id" in taxonomy:
        category2id = taxonomy["category2id"]
        for name in ["wall", "floor", "ceiling"]:
            if special_mapping[name] is None and name in category2id:
                special_mapping[name] = category2id[name]
    
    for name, cat_id in special_mapping.items():
        if cat_id is not None:
            special_categories.append((name, cat_id))
    
    # Combine all items to display
    all_items = sorted_supers + special_categories
    n_items = len(all_items)
    
    # Calculate grid dimensions
    cols = min(4, n_items)
    rows = int(np.ceil(n_items / cols))
    
    # Create figure with seaborn styling
    fig, ax = plt.subplots(figsize=(14, max(8, rows * 2.5)))
    ax.axis("off")
    ax.set_aspect('equal')
    
    # Patch dimensions and spacing
    patch_width = 2.5
    patch_height = 1.8
    h_spacing = 3.5
    v_spacing = 3.0
    
    for idx, (name, item_id) in enumerate(all_items):
        row, col = divmod(idx, cols)
        x = col * h_spacing
        y = -(row * v_spacing)
        
        # Get color
        item_color = tuple(c / 255 for c in id2color.get(str(item_id), [127, 127, 127]))
        
        # Check if this is a special category
        is_special = name in ["wall", "floor", "ceiling"]
        
        # Draw patch with shadow effect (no black outline)
        shadow = mpatches.Rectangle(
            (x + 0.05, y - 0.05), patch_width, patch_height,
            facecolor='gray', alpha=0.3, edgecolor='none'
        )
        ax.add_patch(shadow)
        
        rect = mpatches.Rectangle(
            (x, y), patch_width, patch_height,
            facecolor=item_color, edgecolor='none'
        )
        ax.add_patch(rect)
        
        # Add text with better positioning
        # Add special indicator for wall/floor/ceiling
        display_name = name.upper() if is_special else name
        fontsize = 11 if not is_special else 10
        
        ax.text(
            x + patch_width / 2, y + patch_height / 2,
            display_name,
            ha="center", va="center",
            fontsize=fontsize, fontweight="bold",
            bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8, edgecolor='none')
        )
        
        # Add label for special categories
        label_text = f"ID: {item_id}"
        if is_special:
            label_text = f"(Special) {label_text}"
        
        ax.text(
            x + patch_width / 2, y - 0.4,
            label_text,
            ha="center", va="top",
            fontsize=8 if is_special else 9, 
            style='italic', color='#333333'
        )
    
    # Set limits with padding - added top padding to prevent cropping
    ax.set_xlim(-0.5, cols * h_spacing - 0.5)
    ax.set_ylim(-(rows * v_spacing + 0.5), patch_height + 1.0)
    
    plt.title("Super-Categories & Special Colors Overview", fontsize=16, fontweight='bold', pad=20)
    
    out_path = out_dir / "palette_super_categories_overview.png"
    plt.savefig(out_path, dpi=300, bbox_inches="tight", facecolor='white')
    plt.close()
    print(f"[INFO] Saved super-categories overview: {out_path}")


def visualize_per_super(taxonomy_path: str, out_dir: str):
    """Generate one visualization per super-category."""
    with open(taxonomy_path, "r", encoding="utf-8") as f:
        taxonomy = json.load(f)

    super2id = taxonomy["super2id"]
    category2id = taxonomy["category2id"]
    category2super = taxonomy["category2super"]
    id2color = taxonomy["id2color"]

    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    for supercat, sid in sorted(super2id.items(), key=lambda x: x[1]):
        # Categories belonging to this super
        categories = sorted([c for c, s in category2super.items() if s == supercat])
        
        if not categories:
            continue
        
        # Calculate dynamic figure size based on number of categories
        n_cats = len(categories)
        cols = 5
        rows = int(np.ceil(n_cats / cols))
        fig_height = max(8, 4 + rows * 2.5)
        
        fig, ax = plt.subplots(figsize=(14, fig_height))
        ax.axis("off")
        ax.set_aspect('equal')

        # Draw super-category block with enhanced styling
        scol = tuple(c / 255 for c in id2color.get(str(sid), [127, 127, 127]))
        
        # Shadow for super block
        super_shadow = mpatches.Rectangle(
            (0.05, 0.05), 3.5, 1.8,
            facecolor='gray', alpha=0.3, edgecolor='none'
        )
        ax.add_patch(super_shadow)
        
        # Main super block (no black outline)
        super_rect = mpatches.Rectangle(
            (0, 0.1), 3.5, 1.8,
            facecolor=scol, edgecolor='none'
        )
        ax.add_patch(super_rect)
        
        ax.text(
            1.75, 1.0,
            f"{supercat}",
            ha="center", va="center",
            fontsize=14, fontweight="bold",
            bbox=dict(boxstyle="round,pad=0.4", facecolor='white', alpha=0.9, edgecolor='none')
        )
        ax.text(
            1.75, -0.3,
            f"Super-Category ID: {sid}",
            ha="center", va="top",
            fontsize=10, style='italic', color='#444444'
        )

        # Draw categories in a grid with better spacing
        patch_width = 2.0
        patch_height = 1.5
        h_spacing = 2.8
        v_spacing = 2.5
        
        for i, cat in enumerate(categories):
            cid = category2id[cat]
            ccol = tuple(c / 255 for c in id2color.get(str(cid), [127, 127, 127]))
            
            row, col = divmod(i, cols)
            x = col * h_spacing
            y = -(row + 2) * v_spacing
            
            # Shadow effect
            shadow = mpatches.Rectangle(
                (x + 0.05, y - 0.05), patch_width, patch_height,
                facecolor='gray', alpha=0.2, edgecolor='none'
            )
            ax.add_patch(shadow)
            
            # Category patch
            rect = mpatches.Rectangle(
                (x, y), patch_width, patch_height,
                facecolor=ccol, edgecolor='black', linewidth=1.5
            )
            ax.add_patch(rect)
            
            # Category name (with wrapping for long names)
            cat_display = cat if len(cat) <= 20 else cat[:17] + "..."
            ax.text(
                x + patch_width / 2, y + patch_height / 2,
                cat_display,
                ha="center", va="center",
                fontsize=9, fontweight="semibold",
                bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.7, edgecolor='none')
            )
            
            # Category ID below patch
            ax.text(
                x + patch_width / 2, y - 0.3,
                f"ID: {cid}",
                ha="center", va="top",
                fontsize=8, color='#555555'
            )

        # Set limits with proper padding
        ax.set_xlim(-0.5, cols * h_spacing - 0.5)
        ax.set_ylim(-(rows + 2) * v_spacing - 1, 2.5)
        
        plt.title(
            f"Category Palette for: {supercat}",
            fontsize=15, fontweight='bold', pad=20
        )

        out_path = out_dir / f"palette_{supercat.replace('/', '_').replace(' ', '_')}.png"
        plt.savefig(out_path, dpi=300, bbox_inches="tight", facecolor='white')
        plt.close()
        print(f"[INFO] Saved {out_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Visualize taxonomy color palette (one PNG per super + overview)."
    )
    parser.add_argument("taxonomy", help="Path to taxonomy.json")
    parser.add_argument("--out-dir", required=True, help="Directory to save palette visualizations")
    args = parser.parse_args()

    # Generate chosen labels/colors (actual used palette)
    visualize_chosen_palette(args.taxonomy, args.out_dir)
    
    # Generate super-categories overview
    visualize_super_overview(args.taxonomy, args.out_dir)
    
    # Generate individual super-category visualizations
    visualize_per_super(args.taxonomy, args.out_dir)


================================================================================
FILE: modules\attention.py
================================================================================



================================================================================
FILE: modules\autoencoder.py
================================================================================

import torch
import torch.nn as nn
from typing import Any, Dict, List, Optional


# --- helpers ---
def make_layer(in_channels, out_channels, kernel_size=3, stride=1, padding=1,
               norm=None, act=None, dropout=0.0, transpose=False):
    """Create a single conv/deconv layer with optional norm, activation, dropout"""
    if transpose:
        output_padding = 0 if stride <= 1 else stride - 1
        conv = nn.ConvTranspose2d(
            in_channels, out_channels,
            kernel_size=kernel_size, stride=stride,
            padding=padding, output_padding=output_padding
        )
    else:
        conv = nn.Conv2d(
            in_channels, out_channels,
            kernel_size=kernel_size, stride=stride, padding=padding
        )

    layers = [conv]

    if norm == "batch":
        layers.append(nn.BatchNorm2d(out_channels))
    elif norm == "instance":
        layers.append(nn.InstanceNorm2d(out_channels))

    if act == "relu":
        layers.append(nn.ReLU(inplace=False))
    elif act == "leakyrelu":
        layers.append(nn.LeakyReLU(0.2, inplace=False))
    elif act == "tanh":
        layers.append(nn.Tanh())
    elif act == "sigmoid":
        layers.append(nn.Sigmoid())

    if dropout and dropout > 0:
        layers.append(nn.Dropout2d(dropout))

    return nn.Sequential(*layers)


# --- Encoder ---
class ConvEncoder(nn.Module):
    def __init__(self,
                 in_channels: int,
                 layers_cfg: List[Dict[str, Any]],
                 latent_dim: int,
                 image_size: int,
                 latent_channels: int,
                 latent_base: int,
                 global_norm: Optional[str] = None,
                 global_act: Optional[str] = None,
                 global_dropout: float = 0.0):
        super().__init__()

        layers = []
        prev = in_channels
        current_size = image_size

        for i, cfg in enumerate(layers_cfg):
            in_ch = prev
            out_ch = cfg["out_channels"]
            k = cfg.get("kernel_size", 3)
            s = cfg.get("stride", 1)
            p = cfg.get("padding", 1)
            norm = cfg.get("norm", global_norm)
            act = cfg.get("act", global_act)
            drop = cfg.get("dropout", global_dropout)

            layers.append(make_layer(prev, out_ch, k, s, p, norm, act, drop))
            prev = out_ch
            current_size //= s
            print(f"[Encoder] Layer {i}: in={in_ch}, out={out_ch}, size={current_size}x{current_size}")

        self.conv = nn.Sequential(*layers)
        
        # Final layer maps conv output to target latent cube shape
        # Input: prev channels at current_size x current_size
        # Output: latent_channels at latent_base x latent_base
        self.to_latent = nn.Sequential(
            nn.Conv2d(prev, latent_channels, kernel_size=1),
            nn.AdaptiveAvgPool2d((latent_base, latent_base))
        )
        
        self.latent_channels = latent_channels
        self.latent_base = latent_base
        self.conv_output_channels = prev
        self.conv_output_size = current_size
        
        print(f"[Encoder] Conv output: {prev}x{current_size}x{current_size}")
        print(f"[Encoder] Latent output: {latent_channels}x{latent_base}x{latent_base}")

    def forward(self, x):
        x = self.conv(x)
        z = self.to_latent(x)
        return z


# --- Decoder ---
class ConvDecoder(nn.Module):
    def __init__(self,
                 out_channels: int,
                 latent_dim: int,
                 encoder_layers_cfg: List[Dict[str, Any]],
                 image_size: int,
                 latent_channels: int,
                 latent_base: int,
                 global_norm: Optional[str] = None,
                 global_act: Optional[str] = None,
                 global_dropout: float = 0.0):
        super().__init__()

        self.latent_channels = latent_channels
        self.latent_base = latent_base
        
        # Calculate what the encoder's final conv output size would be
        start_size = image_size
        for cfg in encoder_layers_cfg:
            start_size //= cfg.get("stride", 1)
        
        start_channels = encoder_layers_cfg[-1]["out_channels"]
        
        print(f"[Decoder] Latent input: {latent_channels}x{latent_base}x{latent_base}")
        print(f"[Decoder] Decoder start: {start_channels}x{start_size}x{start_size}")
        
        # Map from latent cube to decoder starting point
        self.from_latent = nn.Sequential(
            nn.Upsample(size=(start_size, start_size), mode='bilinear', align_corners=False),
            nn.Conv2d(latent_channels, start_channels, kernel_size=1)
        )
        
        # Build deconv layers (mirror of encoder)
        layers = []
        prev_ch = start_channels
        current_size = start_size
        reversed_configs = list(reversed(encoder_layers_cfg))

        for i, cfg in enumerate(reversed_configs):
            in_ch = prev_ch
            out_ch = cfg["out_channels"]
            k = cfg.get("kernel_size", 3)
            s = cfg.get("stride", 1)
            p = cfg.get("padding", 1)
            norm = cfg.get("norm", global_norm)
            act = cfg.get("act", global_act)
            drop = cfg.get("dropout", global_dropout)

            layers.append(make_layer(in_ch, out_ch, k, s, p, norm, act, drop, transpose=True))
            current_size *= s
            print(f"[Decoder] Layer {i}: in={in_ch}, out={out_ch}, size={current_size}x{current_size}")
            prev_ch = out_ch

        self.deconv = nn.Sequential(*layers)
        self.final = nn.Conv2d(prev_ch, out_channels, kernel_size=3, padding=1)
        print(f"[Decoder] Final output: {out_channels}x{current_size}x{current_size}")

    def forward(self, z):
        x = self.from_latent(z)
        x = self.deconv(x)
        x = self.final(x)
        return torch.sigmoid(x)


# --- AutoEncoder wrapper ---
class AutoEncoder(nn.Module):
    def __init__(self, encoder: nn.Module, decoder: nn.Module):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def forward(self, x):
        z = self.encoder(x)
        x_recon = self.decoder(z)
        return x_recon

    @classmethod
    def from_config(cls, cfg: dict | str):
        import yaml
        if isinstance(cfg, str):
            with open(cfg, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f)

        enc_cfg = cfg["encoder"]
        dec_cfg = cfg["decoder"]

        encoder = ConvEncoder(
            in_channels=enc_cfg["in_channels"],
            layers_cfg=enc_cfg["layers"],
            latent_dim=enc_cfg["latent_dim"],
            image_size=enc_cfg["image_size"],
            latent_channels=enc_cfg["latent_channels"],
            latent_base=enc_cfg["latent_base"],
            global_norm=enc_cfg.get("global_norm"),
            global_act=enc_cfg.get("global_act"),
            global_dropout=enc_cfg.get("global_dropout", 0.0),
        )

        decoder = ConvDecoder(
            out_channels=dec_cfg["out_channels"],
            latent_dim=dec_cfg["latent_dim"],
            encoder_layers_cfg=enc_cfg["layers"],
            image_size=dec_cfg["image_size"],
            latent_channels=dec_cfg["latent_channels"],
            latent_base=dec_cfg["latent_base"],
            global_norm=dec_cfg.get("global_norm"),
            global_act=dec_cfg.get("global_act"),
            global_dropout=dec_cfg.get("global_dropout", 0.0),
        )

        return cls(encoder, decoder)


================================================================================
FILE: modules\condition_mixer.py
================================================================================

import torch
import torch.nn as nn
import torch.nn.functional as F

class ConditionMixer(nn.Module):
    def __init__(self, out_channels: int, target_size: tuple[int, int],
                 pov_channels: int = None, graph_channels: int = None):
        super().__init__()
        self.out_channels = out_channels
        self.target_size = target_size
        
        # Save current RNG state
        rng_state = torch.get_rng_state()
        torch.manual_seed(42)
        
        self.pov_proj = self._make_projection(pov_channels, out_channels) if pov_channels else None
        self.graph_proj = self._make_projection(graph_channels, out_channels) if graph_channels else None
        
        # Restore RNG state
        torch.set_rng_state(rng_state)

    def _make_projection(self, in_channels: int, out_channels: int) -> nn.ModuleDict:
        H, W = self.target_size
        return nn.ModuleDict({
            'linear': nn.Linear(in_channels, out_channels * H * W, bias=False),
            'conv': nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)
        })

    def project_condition(self, x: torch.Tensor, proj_module: nn.ModuleDict, out_channels: int) -> torch.Tensor:
        B, H, W = x.shape[0], *self.target_size
        if x.ndim == 2:
            return proj_module['linear'](x).view(B, out_channels, H, W)
        elif x.ndim == 4:
            x = proj_module['conv'](x)
            return F.interpolate(x, size=(H, W), mode="bilinear", align_corners=False) if x.shape[2:] != (H, W) else x
        raise ValueError(f"Unsupported condition shape: {x.shape}")

    def forward(self, conds: list[torch.Tensor | None], weights: torch.Tensor | None = None) -> torch.Tensor:
        raise NotImplementedError


class ConcatMixer(ConditionMixer):
    def __init__(self, out_channels: int, target_size: tuple[int, int],
                 pov_channels: int = None, graph_channels: int = None):
        # Split channels for concatenation
        self.pov_out_channels = out_channels // 2
        self.graph_out_channels = out_channels - self.pov_out_channels
        
        # Call parent init but don't create projections yet
        super().__init__(out_channels, target_size)
        
        # Save current RNG state
        rng_state = torch.get_rng_state()
        torch.manual_seed(42)
        
        # Create projections with correct output sizes
        self.pov_proj = self._make_projection(pov_channels, self.pov_out_channels) if pov_channels else None
        self.graph_proj = self._make_projection(graph_channels, self.graph_out_channels) if graph_channels else None
        
        # Restore RNG state
        torch.set_rng_state(rng_state)
    
    def forward(self, conds: list[torch.Tensor | None], weights=None) -> torch.Tensor:
        pov, graph = conds
        B = (pov if pov is not None else graph).shape[0]
        device = (pov if pov is not None else graph).device
        H, W = self.target_size
        
        pov_out = self.project_condition(pov, self.pov_proj, self.pov_out_channels) if pov is not None else torch.zeros(B, self.pov_out_channels, H, W, device=device)
        graph_out = self.project_condition(graph, self.graph_proj, self.graph_out_channels) if graph is not None else torch.zeros(B, self.graph_out_channels, H, W, device=device)
        return torch.cat([pov_out, graph_out], dim=1)

    
class WeightedMixer(ConditionMixer):
    def forward(self, conds: list[torch.Tensor | None], weights: torch.Tensor = None) -> torch.Tensor:
        pov, graph = conds
        B = (pov if pov is not None else graph).shape[0]
        device = (pov if pov is not None else graph).device
        H, W = self.target_size
        
        # Default to equal weights if not provided
        if weights is None:
            weights = torch.tensor([0.5, 0.5], device=device)
        
        pov_out = self.project_condition(pov, self.pov_proj, self.out_channels) * weights[0] if pov is not None else torch.zeros(B, self.out_channels, H, W, device=device)
        graph_out = self.project_condition(graph, self.graph_proj, self.out_channels) * weights[1] if graph is not None else torch.zeros(B, self.out_channels, H, W, device=device)
        return pov_out + graph_out


================================================================================
FILE: modules\datasets.py
================================================================================

import os
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import torch
import numpy as np

# ---------- Base Utilities ----------

def load_image(path, transform=None):
    img = Image.open(path).convert("RGB")
    if transform:
        img = transform(img)
    return img

def load_embedding(path):
    return torch.load(path) if path.endswith(".pt") else torch.from_numpy(np.load(path))


def compute_sample_weights(df: pd.DataFrame) -> torch.DoubleTensor:
    # create grouping key: scene uses 'scene', rooms use room_id
    keys = df.apply(lambda r: f"{r['type']}:{r['room_id']}" if r["type"] == "room" else "scene", axis=1)
    counts = keys.value_counts()
    weights = keys.map(lambda k: 1.0 / counts[k])
    weights = weights / weights.sum()
    return torch.DoubleTensor(weights.values)

# ---------- Layout Dataset ----------

class LayoutDataset(Dataset):
    def __init__(self, manifest_path, transform=None, mode="all", skip_empty=True, return_embeddings=False):
        self.df = pd.read_csv(manifest_path)
        self.transform = transform
        self.mode = mode
        self.skip_empty = skip_empty
        self.return_embeddings = return_embeddings

        if self.mode != "all":
            self.df = self.df[self.df["type"] == self.mode]

        if self.skip_empty:
            self.df = self.df[self.df["is_empty"] == False]

        self.entries = self.df.to_dict("records")

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        row = self.entries[idx]

        try:
            if self.return_embeddings:
                # Use embedding_path if available, otherwise fall back to layout_path
                path = row.get("embedding_path", row["layout_path"])
                layout = load_embedding(path)
            else:
                path = row["layout_path"]
                layout = load_image(path, self.transform)
        except Exception:
            # skip only broken or unreadable files
            return None

        return {
            "scene_id": row["scene_id"],
            "room_id": row["room_id"],
            "type": row["type"],
            "is_empty": row["is_empty"],
            "path": path,
            "layout": layout,
        }

# ---------- POV Dataset ----------

class PovDataset(Dataset):
    def __init__(self, manifest_path, transform=None, pov_type="seg", skip_empty=True, return_embeddings=False):
        self.df = pd.read_csv(manifest_path)
        self.transform = transform
        self.pov_type = pov_type
        self.skip_empty = skip_empty
        self.return_embeddings = return_embeddings

        self.df = self.df[self.df["type"] == pov_type]

        if self.skip_empty:
            self.df = self.df[self.df["is_empty"] == 0]

        self.entries = self.df.to_dict("records")

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        row = self.entries[idx]
        sample = {
            "scene_id": row["scene_id"],
            "room_id": row["room_id"],
            "type": row["type"],
            "is_empty": row["is_empty"],
            "path": row["pov_path"]
        }

        if not row["is_empty"]:
            if self.return_embeddings:
                sample["pov"] = load_embedding(row["pov_path"])
            else:
                sample["pov"] = load_image(row["pov_path"], self.transform)
        else:
            sample["pov"] = None

        return sample

# ---------- Graph Dataset ----------

class GraphDataset(Dataset):
    def __init__(self, manifest_path, return_embeddings=False):
        self.df = pd.read_csv(manifest_path)
        self.return_embeddings = return_embeddings
        self.entries = self.df.to_dict("records")

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        row = self.entries[idx]
        sample = {
            "scene_id": row["scene_id"],
            "room_id": row["room_id"],
            "type": row["type"],
            "is_empty": row["is_empty"],
            "path": row["graph_path"]  # assuming your manifest has this
        }

        if not row["is_empty"]:
            if self.return_embeddings:
                sample["graph"] = load_embedding(row["graph_path"])
            else:
                # load raw graph here (JSON, adjacency, etc.)
                with open(row["graph_path"], "r") as f:
                    sample["graph"] = f.read()
        else:
            sample["graph"] = None

        return sample



def make_dataloaders(layout_manifest, pov_manifest, graph_manifest, batch_size=32, transform=None):
    layout_ds = LayoutDataset(layout_manifest, transform=transform, mode="all")
    pov_ds = PovDataset(pov_manifest, transform=transform, pov_type="seg")
    graph_ds = GraphDataset(graph_manifest)

    layout_loader = DataLoader(layout_ds, batch_size=batch_size, shuffle=True)
    pov_loader = DataLoader(pov_ds, batch_size=batch_size, shuffle=True)
    graph_loader = DataLoader(graph_ds, batch_size=batch_size, shuffle=True)

    return layout_loader, pov_loader, graph_loader


from torch.utils.data._utils.collate import default_collate

def collate_skip_none(batch):
    batch = [b for b in batch if b is not None and b.get("layout") is not None]
    if not batch:
        return None
    return default_collate(batch)



def main():
    import os
    import torchvision.transforms as T

    layout_manifest = r"C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\indexes\layouts.csv"
    pov_manifest = r"C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\indexes\povs.csv"
    graph_manifest = "path/to/graph_manifest.csv"  # not required yet

    transform = T.Compose([
        T.Resize((128, 128)),
        T.ToTensor()
    ])

    # --- Layout Dataset ---
    print("Testing LayoutDataset...")
    layout_ds = LayoutDataset(layout_manifest, transform=transform, mode="all")
    print(f"Loaded {len(layout_ds)} layout samples")
    sample = layout_ds[0]
    for k, v in sample.items():
        if hasattr(v, "shape"):
            print(k, v.shape)
        else:
            print(k, v)

    # --- POV Dataset (seg) ---
    print("\nTesting PovDataset (seg)...")
    pov_ds = PovDataset(pov_manifest, transform=transform, pov_type="seg")
    print(f"Loaded {len(pov_ds)} pov samples (seg)")
    sample = pov_ds[0]
    for k, v in sample.items():
        if hasattr(v, "shape"):
            print(k, v.shape)
        else:
            print(k, v)

    # --- Graph Dataset (optional) ---
    if os.path.exists(graph_manifest):
        print("\nTesting GraphDataset...")
        graph_ds = GraphDataset(graph_manifest, return_embeddings=False)
        print(f"Loaded {len(graph_ds)} graph samples")
        sample = graph_ds[0]
        for k, v in sample.items():
            if isinstance(v, str) and len(v) > 60:
                print(k, v[:60] + "...")
            else:
                print(k, v)
    else:
        print("\nGraph manifest not found, skipping GraphDataset test.")

    # --- Dataloaders (layout + pov only) ---
    print("\nTesting Dataloaders...")
    layout_loader = DataLoader(layout_ds, batch_size=4, shuffle=True)
    pov_loader = DataLoader(pov_ds, batch_size=4, shuffle=True)

    batch = next(iter(layout_loader))
    print("Layout batch keys:", batch.keys())
    if "layout" in batch:
        print("Layout batch tensor shape:", batch["layout"].shape)





if __name__ == "__main__":
    main()


================================================================================
FILE: modules\diffusion.py
================================================================================

import torch
import torch.nn as nn
from pathlib import Path
from typing import Optional, Union
import yaml

from modules.scheduler import *
from modules.unet import UNet
from modules.autoencoder import AutoEncoder
from tqdm import tqdm

class LatentDiffusion(nn.Module):
    """
    Inference-only Latent Diffusion wrapper.
    Loads scheduler, UNet, and AutoEncoder (decoder only).
    Uses latent size specified in the config.
    """

    def __init__(
        self,
        unet: UNet,
        scheduler: NoiseScheduler,
        autoencoder: Optional[AutoEncoder] = None,
        latent_shape: Optional[tuple[int, int, int]] = None,
    ):
        super().__init__()
        self.unet = unet
        self.scheduler = scheduler
        self.autoencoder = autoencoder
        self.latent_shape = latent_shape

    @torch.no_grad()
    def sample(
        self,
        batch_size: int = 1,
        image: bool = False,
        cond: Optional[torch.Tensor] = None,
        num_steps: Optional[int] = None,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
    ) -> torch.Tensor:
        """
        Generate samples from pure noise in latent space using the same DDPM
        update rule as training.

        Args:
            batch_size: number of samples
            image: if True, decode latents to RGB images
            cond: conditioning tensor [B, C_cond, H, W] or None
            num_steps: optional override for scheduler steps
            device: device string
        Returns:
            Latents [B, C, H, W] or decoded images [B, 3, H*, W*]
        """
        assert self.latent_shape is not None, "latent_shape must be set"

        self.eval()
        self.unet.eval()

        C, H, W = self.latent_shape
        x_t = torch.randn(batch_size, C, H, W, device=device) # create initial noise

        steps = num_steps or self.scheduler.num_steps # set number of steps 
        timesteps = torch.linspace(steps - 1, 0, steps, dtype=torch.long, device=device) # set actual steps vector

        print("Generating...")
        for t in tqdm(timesteps, desc="Diffusion sampling", total=len(timesteps)): # for each step in reverse
            t_batch = torch.full((batch_size,), t, device=device, dtype=torch.long)
            noise_pred = self.unet(x_t, t_batch, cond)

            if t > 0:
                # get cooeficcients from scheduler 
                alpha_t = self.scheduler.alphas[t]
                alpha_bar_t = self.scheduler.alpha_bars[t]
                alpha_bar_prev = self.scheduler.alpha_bars[t - 1]
                beta_t = self.scheduler.betas[t]

                # Predict x0 and clamp for stability
                pred_x0 = (x_t - torch.sqrt(1 - alpha_bar_t) * noise_pred) / torch.sqrt(alpha_bar_t)
                pred_x0 = torch.clamp(pred_x0, -3, 3)

                # Compute DDPM mean
                coef1 = torch.sqrt(alpha_bar_prev) * beta_t / (1 - alpha_bar_t)
                coef2 = torch.sqrt(alpha_t) * (1 - alpha_bar_prev) / (1 - alpha_bar_t)
                mean = coef1 * pred_x0 + coef2 * x_t

                # Add variance noise
                noise = torch.randn_like(x_t)
                variance = ((1 - alpha_bar_prev) / (1 - alpha_bar_t)) * beta_t
                x_t = mean + torch.sqrt(variance) * noise
            else:
                alpha_bar_t = self.scheduler.alpha_bars[t]
                x_t = (x_t - torch.sqrt(1 - alpha_bar_t) * noise_pred) / torch.sqrt(alpha_bar_t)

        if image:
            assert self.autoencoder is not None, "AutoEncoder required for image decoding"
            print("Decoding...")
            x_t = self.autoencoder.decoder(x_t)

        return x_t


    @classmethod
    def from_config(
        cls,
        config_path: Union[str, Path],
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
    ):
        """
        Build inference model from master YAML config:
        ---
        latent:
          base: 64
          channels: 4

        scheduler:
          type: LinearScheduler
          num_steps: 1000

        autoencoder:
          config: configs/ae.yaml
          checkpoint: checkpoints/ae.pt

        unet:
          config: configs/unet.yaml
          checkpoint: checkpoints/unet.pt
        """
        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)

        # Scheduler
        sched_cfg = config["scheduler"]
        sched_class = globals()[sched_cfg["type"]]
        scheduler = sched_class(num_steps=sched_cfg["num_steps"]).to(device)

        # Autoencoder
        ae_cfg_path = config["autoencoder"]["config"]
        ae_ckpt_path = config["autoencoder"].get("checkpoint", None)
        autoencoder = AutoEncoder.from_config(ae_cfg_path).to(device)
        if ae_ckpt_path:
            autoencoder.load_state_dict(torch.load(ae_ckpt_path, map_location=device))
        autoencoder.eval()

        # UNet
        unet_cfg_path = config["unet"]["config"]
        unet_ckpt_path = config["unet"].get("checkpoint", None)
        with open(unet_cfg_path, "r", encoding="utf-8") as f:
            unet_cfg = yaml.safe_load(f)
            if "unet" in unet_cfg:
                unet_cfg = unet_cfg["unet"]

        unet = UNet(**unet_cfg).to(device)
        if unet_ckpt_path:
            ckpt = torch.load(unet_ckpt_path, map_location=device)
            # handle both plain and wrapped checkpoints
            if "state_dict" in ckpt:
                state = ckpt["state_dict"]
            elif "unet_state_dict" in ckpt:
                state = ckpt["unet_state_dict"]
            else:
                state = ckpt
            unet.load_state_dict(state, strict=False)
        unet.eval()


        # Latent shape from config (preferred) or AE config
        if "latent" in config:
            C = config["latent"]["channels"]
            H = W = config["latent"]["base"]
        else:
            with open(ae_cfg_path, "r", encoding="utf-8") as f:
                ae_cfg = yaml.safe_load(f)
            C = ae_cfg["encoder"]["latent_channels"]
            H = W = ae_cfg["encoder"]["latent_base"]

        return cls(unet=unet, scheduler=scheduler, autoencoder=autoencoder, latent_shape=(C, H, W)).to(device)


================================================================================
FILE: modules\scheduler.py
================================================================================

import torch
from abc import ABC, abstractmethod

class NoiseScheduler(ABC):
    def __init__(self, num_steps):
        self.num_steps = num_steps
        self.alphas, self.betas = self.build_schedule(num_steps)
        self.alpha_bars = torch.cumprod(self.alphas, dim=0)

    @abstractmethod
    def build_schedule(self, num_steps):
        pass
    
    def to(self, device):
        """Move scheduler tensors to device"""
        self.alphas = self.alphas.to(device)
        self.betas = self.betas.to(device)
        self.alpha_bars = self.alpha_bars.to(device)
        return self

    def add_noise(self, x0, t, noise):
        # Reshape for broadcasting: [B] -> [B, 1, 1, 1]
        sqrt_alpha_bar = self.alpha_bars[t].sqrt().view(-1, 1, 1, 1)
        sqrt_one_minus = (1 - self.alpha_bars[t]).sqrt().view(-1, 1, 1, 1)
        return sqrt_alpha_bar * x0 + sqrt_one_minus * noise, noise

class LinearScheduler(NoiseScheduler):
    def build_schedule(self, num_steps):
        betas = torch.linspace(1e-4, 0.02, num_steps)
        alphas = 1.0 - betas
        return alphas, betas

class CosineScheduler(NoiseScheduler):
    def build_schedule(self, num_steps):
        # cosine decay formula (simplified)
        steps = torch.arange(0, num_steps + 1, dtype=torch.float32)
        alpha_bars = torch.cos(((steps / num_steps) + 0.008) / 1.008 * 3.14159 / 2) ** 2
        betas = 1 - (alpha_bars[1:] / alpha_bars[:-1])
        alphas = 1 - betas
        return alphas, betas




class SquaredCosineScheduler(NoiseScheduler):
    """Improved Cosine scheduler - better signal preservation"""
    def build_schedule(self, num_steps):
        steps = torch.arange(0, num_steps + 1, dtype=torch.float32)
        alpha_bars = torch.cos(((steps / num_steps) + 0.008) / 1.008 * 3.14159 / 2) ** 4
        betas = 1 - (alpha_bars[1:] / alpha_bars[:-1])
        betas = torch.clamp(betas, 0, 0.999)
        alphas = 1 - betas
        return alphas, betas


class SigmoidScheduler(NoiseScheduler):
    """Sigmoid-based schedule - smooth transition"""
    def build_schedule(self, num_steps):
        steps = torch.linspace(-6, 6, num_steps)
        alpha_bars = torch.sigmoid(steps)
        alpha_bars = (alpha_bars - alpha_bars.min()) / (alpha_bars.max() - alpha_bars.min())
        alpha_bars = torch.flip(alpha_bars, [0])
        
        betas = torch.zeros(num_steps)
        betas[0] = 1 - alpha_bars[0]
        betas[1:] = 1 - (alpha_bars[1:] / alpha_bars[:-1])
        betas = torch.clamp(betas, 0, 0.999)
        alphas = 1 - betas
        return alphas, betas

class ExponentialScheduler(NoiseScheduler):
    """Exponential decay - fast at start, slow at end"""
    def build_schedule(self, num_steps):
        steps = torch.arange(num_steps, dtype=torch.float32)
        alpha_bars = torch.exp(-5 * steps / num_steps)
        
        betas = torch.zeros(num_steps)
        betas[0] = 1 - alpha_bars[0]
        betas[1:] = 1 - (alpha_bars[1:] / alpha_bars[:-1])
        betas = torch.clamp(betas, 1e-4, 0.999)
        alphas = 1 - betas
        return alphas, betas

class QuadraticScheduler(NoiseScheduler):
    """Quadratic schedule - middle ground between linear and cosine"""
    def build_schedule(self, num_steps):
        betas = torch.linspace(1e-4**0.5, 0.02**0.5, num_steps) ** 2
        alphas = 1.0 - betas
        return alphas, betas


================================================================================
FILE: modules\unet.py
================================================================================

import math
import torch
import torch.nn as nn
from typing import Optional, List

"""
config file format:

# The U-Net is defined by the autoencoder latent shape [C, H, W].
# Example: latent = [8, 32, 32] â†’ C=8, H=W=32

unet:
  in_channels: 8        <- set equal to latent_channels (C)
  cond_channels: 16     <- condition channels to concat (0 if none)
  out_channels: 8       <- must equal latent_channels (predict noise)
  base_channels: 64     <- internal width, scales capacity (not tied to latent)
  depth: 4              <- how many downsamples. must satisfy H/2^depth >= 1.
                           (for H=32, max depth=5)
  num_res_blocks: 2     <- residual blocks per stage
  time_dim: 128         <- timestep embedding size
  norm: batch           <- normalization
  act: relu             <- activation

Relation:
- Latent [C,H,W] fixes in_channels, out_channels, and spatial H,W.
- Config only changes internal computation, not I/O resolution.
"""

# --- sinusoidal time embedding ---
def timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:
    """
    Sinusoidal timestep embeddings like in DDPM/Transformer.
    timesteps: [B] int64
    Returns: [B, dim]
    """
    device = timesteps.device
    half_dim = dim // 2
    freqs = torch.exp(
        -math.log(10000.0) * torch.arange(0, half_dim, device=device).float() / half_dim
    )
    args = timesteps[:, None].float() * freqs[None]
    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)
    if dim % 2 == 1:
        emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)
    return emb


class TimeEmbedding(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        self.fc1 = nn.Linear(dim, dim * 4)
        self.act = nn.SiLU()
        self.fc2 = nn.Linear(dim * 4, dim)

    def forward(self, t: torch.Tensor):
        emb = timestep_embedding(t, self.fc1.in_features)
        return self.fc2(self.act(self.fc1(emb)))


# --- residual block with time conditioning ---
class ResidualBlock(nn.Module):
    def __init__(self, in_ch, out_ch, time_dim, norm=None, act=None):
        super().__init__()
        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)
        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)

        self.time_proj = nn.Linear(time_dim, out_ch)
        if norm == 'batch':
            self.norm1 = nn.BatchNorm2d(out_ch) if norm == "batch" else nn.Identity()
            self.norm2 = nn.BatchNorm2d(out_ch) if norm == "batch" else nn.Identity()
        elif norm == "group":
            num_groups = min(32, out_ch // 4) if out_ch >= 8 else 1
            self.norm1 = nn.GroupNorm(num_groups=num_groups, num_channels=out_ch)
            self.norm2 = nn.GroupNorm(num_groups=num_groups, num_channels=out_ch)
        else:
            self.norm1 = nn.Identity()
            self.norm2 = nn.Identity()

        if act == "relu":
            self.act = nn.ReLU(inplace=True)
        elif act == "leakyrelu":
            self.act = nn.LeakyReLU(0.2, inplace=True)
        else:
            self.act = nn.SiLU()  # default

        self.skip = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()

    def forward(self, x, t_emb):
        h = self.conv1(x)
        h = h + self.time_proj(t_emb)[:, :, None, None]
        h = self.norm1(h)
        h = self.act(h)

        h = self.conv2(h)
        h = self.norm2(h)
        return h + self.skip(x)


class DownBlock(nn.Module):
    def __init__(self, in_ch, out_ch, time_dim, num_res_blocks=1, norm=None, act=None):
        super().__init__()
        blocks = [ResidualBlock(in_ch, out_ch, time_dim, norm, act)]
        for _ in range(num_res_blocks - 1):
            blocks.append(ResidualBlock(out_ch, out_ch, time_dim, norm, act))
        self.res = nn.Sequential(*blocks)
        self.pool = nn.MaxPool2d(2)

    def forward(self, x, t_emb):
        h = self.res[0](x, t_emb)
        for b in self.res[1:]:
            h = b(h, t_emb)
        return self.pool(h), h


class UpBlock(nn.Module):
    def __init__(self, in_ch, out_ch, time_dim, num_res_blocks=1, norm=None, act=None):
        super().__init__()
        self.up = nn.ConvTranspose2d(in_ch, out_ch, 2, stride=2)
        blocks = [ResidualBlock(out_ch * 2, out_ch, time_dim, norm, act)]
        for _ in range(num_res_blocks - 1):
            blocks.append(ResidualBlock(out_ch, out_ch, time_dim, norm, act))
        self.res = nn.ModuleList(blocks)

    def forward(self, x, skip, t_emb):
        x = self.up(x)
        x = torch.cat([x, skip], dim=1)
        for b in self.res:
            x = b(x, t_emb)
        return x


# --- U-Net denoiser ---
class UNet(nn.Module):
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 cond_channels: int = 0,
                 base_channels: int = 64,
                 depth: int = 4,
                 num_res_blocks: int = 1,
                 time_dim: int = 128,
                 norm: Optional[str] = None,
                 act: Optional[str] = None):
        super().__init__()

        self.time_mlp = TimeEmbedding(time_dim)

        total_in = in_channels + cond_channels

        # encoder
        self.downs = nn.ModuleList()
        prev_ch = total_in
        feats = []
        for i in range(depth):
            ch = base_channels * (2 ** i)
            self.downs.append(DownBlock(prev_ch, ch, time_dim, num_res_blocks, norm, act))
            feats.append(ch)
            prev_ch = ch

        # bottleneck
        self.bottleneck = ResidualBlock(prev_ch, prev_ch, time_dim, norm, act)

        # decoder
        self.ups = nn.ModuleList()
        for ch in reversed(feats):
            self.ups.append(UpBlock(prev_ch, ch, time_dim, num_res_blocks, norm, act))
            prev_ch = ch

        # final conv
        self.final = nn.Conv2d(prev_ch, out_channels, 1)

    def forward(self, x_t, t, cond=None):
        if cond is not None:
            x_t = torch.cat([x_t, cond], dim=1)

        t_emb = self.time_mlp(t)  # [B, time_dim]

        skips = []
        for down in self.downs:
            x_t, skip = down(x_t, t_emb)
            skips.append(skip)

        x_t = self.bottleneck(x_t, t_emb)

        for up, skip in zip(self.ups, reversed(skips)):
            x_t = up(x_t, skip, t_emb)

        return self.final(x_t)

    @classmethod
    def from_config(cls, cfg: dict | str, latent_channels: Optional[int] = None, latent_base: Optional[int] = None):
        """
        Build U-Net from config file and latent shape.
        latent_channels: C from autoencoder
        latent_base: H=W from autoencoder (used to validate depth)
        """
        import yaml
        if isinstance(cfg, str):
            with open(cfg, "r", encoding="utf-8") as f:
                cfg = yaml.safe_load(f)
            cfg = cfg["unet"]

        if latent_channels is not None:
            cfg["in_channels"] = latent_channels
            cfg["out_channels"] = latent_channels

        if latent_base is not None:
            max_depth = int(math.log2(latent_base))
            if cfg["depth"] > max_depth:
                raise ValueError(f"Depth {cfg['depth']} too large for latent_base {latent_base}. "
                                 f"Max allowed = {max_depth}")

        return cls(**cfg)


================================================================================
FILE: modules\unified_dataset.py
================================================================================

import os
import json
import pandas as pd
import torch
from torch.utils.data import Dataset
from PIL import Image
import numpy as np


# ---------- Utility loaders ----------

def load_image(path, transform=None):
    img = Image.open(path).convert("RGB")
    if transform:
        img = transform(img)
    return img


def load_embedding(path):
    if path.endswith(".pt"):
        return torch.load(path)
    elif path.endswith(".npy"):
        return torch.from_numpy(np.load(path))
    else:
        raise ValueError(f"Unsupported embedding format: {path}")


def load_graph(path, use_embeddings=False):
    if use_embeddings:
        return load_embedding(path)
    with open(path, "r") as f:
        return json.load(f)


def valid_path(x):
    invalid = {"", "false", "0", "none"}
    return isinstance(x, str) and str(x).strip().lower() not in invalid


# ---------- Unified Layout Dataset ----------

class UnifiedLayoutDataset(Dataset):
    """
    Unified dataset combining room and scene manifests.
    Each item returns dict(pov, graph, layout) for diffusion training.
    - Room samples: pov, graph, layout all valid.
    - Scene samples: pov=None, graph+layout valid.
    
    Args:
        room_manifest: Path to room manifest CSV
        scene_manifest: Path to scene manifest CSV
        use_embeddings: If True, load embeddings instead of raw data
        pov_mode: Which POV to use - 'seg', 'tex', or None (no POV filtering)
        transform: Optional transform for images
        device: Device to load tensors to
    """

    def __init__(self, room_manifest, scene_manifest, use_embeddings=False, 
                 pov_type=None, transform=None, device=None):
        self.use_embeddings = use_embeddings
        self.pov_type = pov_type  # 'seg', 'tex', or None
        self.transform = transform
        self.device = device

        # Load both manifests
        room_df = pd.read_csv(room_manifest)
        scene_df = pd.read_csv(scene_manifest)

        # --- Standardize schemas ---
        room_df = room_df.rename(columns={
            "ROOM_GRAPH_PATH": "GRAPH_PATH",
            "ROOM_GRAPH_EMBEDDING_PATH": "GRAPH_EMBEDDING_PATH",
            "ROOM_LAYOUT_PATH": "LAYOUT_PATH",
            "ROOM_LAYOUT_EMBEDDING_PATH": "LAYOUT_EMBEDDING_PATH"
        })

        scene_df["ROOM_ID"] = ""
        scene_df["POV_TYPE"] = ""
        scene_df["POV_PATH"] = ""
        scene_df["POV_EMBEDDING_PATH"] = ""

        # --- Common column order ---
        cols = [
            "SCENE_ID", "ROOM_ID", "POV_TYPE", "POV_PATH", "POV_EMBEDDING_PATH",
            "GRAPH_PATH", "GRAPH_EMBEDDING_PATH", "LAYOUT_PATH", "LAYOUT_EMBEDDING_PATH"
        ]
        room_df = room_df[cols]
        scene_df = scene_df[cols]

        # --- Filter by POV mode if specified ---
        if pov_type is not None:
            # Only keep room samples with matching POV type
            room_df = room_df[room_df["POV_TYPE"] == pov_type].reset_index(drop=True)
            print(f"Filtered to POV mode '{pov_type}': {len(room_df)} room samples", flush=True)

        # --- Merge manifests ---
        df = pd.concat([room_df, scene_df], ignore_index=True)

        # --- Filter invalid paths ---
        mask = (
            df["GRAPH_PATH"].apply(valid_path)
            & df["GRAPH_EMBEDDING_PATH"].apply(valid_path)
            & df["LAYOUT_PATH"].apply(valid_path)
            & df["LAYOUT_EMBEDDING_PATH"].apply(valid_path)
        )
        df = df[mask].reset_index(drop=True)
        
        self.df = df
        self.entries = df.to_dict("records")
        
        # Print dataset statistics
        num_with_pov = (df["POV_PATH"].apply(valid_path)).sum()
        num_without_pov = len(df) - num_with_pov
        print(f"Dataset: {len(df)} total samples ({num_with_pov} with POV, {num_without_pov} without)", flush=True)

    def __len__(self):
        return len(self.entries)

    def __getitem__(self, idx):
        row = self.entries[idx]

        # ----- POV -----
        pov = None
        pov_path = row["POV_EMBEDDING_PATH"] if self.use_embeddings else row["POV_PATH"]
        if valid_path(pov_path):
            pov = load_embedding(pov_path) if self.use_embeddings else load_image(pov_path, self.transform)
            if self.device and isinstance(pov, torch.Tensor):
                pov = pov.to(self.device)

        # ----- Graph -----
        graph_path = row["GRAPH_EMBEDDING_PATH"] if self.use_embeddings else row["GRAPH_PATH"]
        graph = load_graph(graph_path, use_embeddings=self.use_embeddings)
        if self.device and isinstance(graph, torch.Tensor):
            graph = graph.to(self.device)

        # ----- Layout -----
        layout_path = row["LAYOUT_EMBEDDING_PATH"] if self.use_embeddings else row["LAYOUT_PATH"]
        layout = load_embedding(layout_path) if self.use_embeddings else load_image(layout_path, self.transform)
        if self.device and isinstance(layout, torch.Tensor):
            layout = layout.to(self.device)

        return {
            "scene_id": row["SCENE_ID"],
            "room_id": row["ROOM_ID"] if row["ROOM_ID"] else None,
            "pov_type": row["POV_TYPE"] if row["POV_TYPE"] else None,
            "pov": pov,
            "graph": graph,
            "layout": layout
        }


def collate_fn(batch):
    """Custom collate function to handle None POV values in batches"""
    # Collect all non-None POVs
    pov_list = [b['pov'] for b in batch if b['pov'] is not None]
    
    return {
        'scene_id': [b['scene_id'] for b in batch],
        'room_id': [b['room_id'] for b in batch],
        'pov_type': [b['pov_type'] for b in batch],
        'pov': torch.stack(pov_list) if pov_list else None,
        'graph': torch.stack([b['graph'] for b in batch]),
        'layout': torch.stack([b['layout'] for b in batch]),
    }


================================================================================
FILE: pipeline\pipeline.py
================================================================================



================================================================================
FILE: scripts\analyze_reconstruction.py
================================================================================

#!/usr/bin/env python3
"""
Multi-Model Autoencoder Reconstruction Analysis
Compare reconstruction quality across multiple trained models.
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import yaml
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
import warnings
warnings.filterwarnings('ignore')

sns.set_style("whitegrid")
sns.set_context("notebook", font_scale=1.1)
plt.rcParams['figure.dpi'] = 300


def parse_model_name(exp_dir_name):
    """Extract model info from directory name."""
    import re
    pattern = r'(?:ae_)?diff_(\d+)ch_(\d+)x(\d+)_(vanilla|skip|medium|deep)'
    match = re.search(pattern, exp_dir_name.lower())
    
    if match:
        channels, base, _, arch = match.groups()
        if arch == 'skip':
            arch = 'medium'
        return {
            'channels': int(channels),
            'base': int(base),
            'arch': arch,
            'name': str(channels) + 'ch_' + str(base) + 'x' + str(base) + '_' + arch
        }
    return None


def load_autoencoder_model(config_path, checkpoint_path, device='cpu'):
    """Load an autoencoder model."""
    sys.path.append(str(Path(__file__).parent.parent / "modules"))
    from autoencoder import AutoEncoder
    
    try:
        # Load config file
        with open(config_path, 'r') as f:
            full_config = yaml.safe_load(f)
        
        # Debug: print config structure
        print("      Config keys: " + str(list(full_config.keys())))
        
        # Extract model config if wrapped
        if 'model_cfg' in full_config:
            model_config = full_config['model_cfg']
            print("      Extracted model_cfg, keys: " + str(list(model_config.keys())))
        else:
            model_config = full_config
            print("      Using full config directly, keys: " + str(list(model_config.keys())))
        
        # Verify structure
        if 'encoder' not in model_config:
            print("      ERROR: No 'encoder' key found!")
            print("      Available keys: " + str(list(model_config.keys())))
            return None
        
        if 'decoder' not in model_config:
            print("      ERROR: No 'decoder' key found!")
            print("      Available keys: " + str(list(model_config.keys())))
            return None
        
        print("      Config structure verified, creating model...")
        
        # Pass the extracted config dict directly
        ae = AutoEncoder.from_config(model_config)
        ae.load_state_dict(torch.load(checkpoint_path, map_location=device))
        ae.to(device)
        ae.eval()
        return ae
    except Exception as e:
        print("      ERROR: " + str(e))
        import traceback
        traceback.print_exc()
        return None


def load_test_images(image_paths, image_size=512):
    """Load test images."""
    transform = transforms.Compose([
        transforms.Resize((image_size, image_size)),
        transforms.ToTensor()
    ])
    
    images = []
    names = []
    for img_path in image_paths:
        img = Image.open(img_path).convert('RGB')
        img_tensor = transform(img)
        images.append(img_tensor)
        names.append(Path(img_path).stem)
    
    return torch.stack(images), names


def load_taxonomy_colors(taxonomy_path=None):
    """Load super-category colors from taxonomy file."""
    import json
    
    if not taxonomy_path or not Path(taxonomy_path).exists():
        print("WARNING: No taxonomy file provided or file not found")
        return {}
    
    with open(taxonomy_path, 'r') as f:
        taxonomy = json.load(f)
    
    # Only use super-categories (id2super), not fine-grained categories
    colors_dict = {}
    id2super = taxonomy.get('id2super', {})
    id2color = taxonomy.get('id2color', {})
    
    for cat_id in id2super.keys():
        if cat_id in id2color:
            cat_name = id2super[cat_id]
            colors_dict[cat_name] = np.array(id2color[cat_id]) / 255.0
    
    return colors_dict


def compute_color_class_preservation(original, reconstruction, target_colors):
    """
    Compute how well specific class colors are preserved.
    Returns per-class color preservation metrics.
    """
    orig_np = original.numpy().transpose(1, 2, 0)  # H x W x 3
    recon_np = reconstruction.numpy().transpose(1, 2, 0)
    
    metrics = {}
    
    for class_name, target_rgb in target_colors.items():
        # Find pixels close to this target color
        color_diff = np.linalg.norm(orig_np - target_rgb, axis=2)
        mask = color_diff < 0.15  # Pixels within threshold of target color
        
        if mask.sum() == 0:
            continue
        
        # Get original and reconstructed pixels for this class
        orig_pixels = orig_np[mask]
        recon_pixels = recon_np[mask]
        
        # Compute metrics for this class
        color_error = np.mean(np.abs(orig_pixels - recon_pixels), axis=0)
        color_mse = np.mean((orig_pixels - recon_pixels) ** 2, axis=0)
        
        # How well is the target color maintained?
        orig_distance = np.mean(np.linalg.norm(orig_pixels - target_rgb, axis=1))
        recon_distance = np.mean(np.linalg.norm(recon_pixels - target_rgb, axis=1))
        
        metrics[class_name] = {
            'pixel_count': int(mask.sum()),
            'mae_r': float(color_error[0]),
            'mae_g': float(color_error[1]),
            'mae_b': float(color_error[2]),
            'mae_overall': float(color_error.mean()),
            'orig_color_distance': float(orig_distance),
            'recon_color_distance': float(recon_distance),
            'color_drift': float(recon_distance - orig_distance)
        }
    
    return metrics


def compute_reconstruction_metrics(original, reconstruction):
    """Compute comprehensive reconstruction metrics."""
    diff = torch.abs(original - reconstruction)
    
    metrics = {
        'overall_mae': diff.mean().item(),
        'overall_mse': ((original - reconstruction) ** 2).mean().item(),
    }
    
    # Per-channel metrics
    for i, channel in enumerate(['red', 'green', 'blue']):
        metrics[channel + '_mae'] = diff[:, i].mean().item()
        metrics[channel + '_mse'] = ((original[:, i] - reconstruction[:, i]) ** 2).mean().item()
        
        # Correlation
        orig_flat = original[:, i].flatten()
        recon_flat = reconstruction[:, i].flatten()
        orig_mean = orig_flat.mean()
        recon_mean = recon_flat.mean()
        
        numerator = ((orig_flat - orig_mean) * (recon_flat - recon_mean)).sum()
        denominator = torch.sqrt(((orig_flat - orig_mean) ** 2).sum() * 
                                 ((recon_flat - recon_mean) ** 2).sum())
        
        metrics[channel + '_corr'] = (numerator / denominator).item()
    
    return metrics


def find_models_to_compare(experiments_dir, num_models=10):
    """Find top N models based on training loss."""
    experiments_dir = Path(experiments_dir)
    
    model_info = []
    for exp_dir in experiments_dir.iterdir():
        if not exp_dir.is_dir():
            continue
        
        config_file = exp_dir / "config_used.yml"
        checkpoint_file = exp_dir / "best.pt"
        metrics_file = exp_dir / "metrics.csv"
        
        if not (config_file.exists() and checkpoint_file.exists() and metrics_file.exists()):
            continue
        
        # Get best loss
        metrics_df = pd.read_csv(metrics_file)
        best_loss = metrics_df['loss'].min()
        
        # Parse model name
        model_params = parse_model_name(exp_dir.name)
        if model_params is None:
            continue
        
        model_info.append({
            'exp_dir': exp_dir,
            'config': config_file,
            'checkpoint': checkpoint_file,
            'best_loss': best_loss,
            'name': model_params['name'],
            'arch': model_params['arch'],
            'channels': model_params['channels'],
            'base': model_params['base']
        })
    
    # Sort by best loss and take top N
    model_info.sort(key=lambda x: x['best_loss'])
    return model_info[:num_models]


def plot_architecture_family_comparison(family_name, models_data, sample_image_name, output_dir):
    """
    Create a single comprehensive figure comparing all models in an architecture family.
    Shows: 1 input + N reconstructions + N difference heatmaps for one test image.
    
    Args:
        family_name: 'vanilla', 'medium', or 'deep'
        models_data: list of model data dicts filtered to this family
        sample_image_name: name of the test image to visualize
        output_dir: where to save the figure
    """
    if not models_data:
        print(f"    No models found for family: {family_name}")
        return
    
    # Get the input image (same for all models)
    input_img = None
    reconstructions = []
    model_names = []
    
    for model_data in models_data:
        if sample_image_name in model_data['images']:
            result = model_data['images'][sample_image_name]
            if input_img is None:
                input_img = result['original']
            reconstructions.append(result['reconstruction'])
            model_names.append(model_data['name'])
    
    if input_img is None or len(reconstructions) == 0:
        print(f"    No data available for {family_name} / {sample_image_name}")
        return
    
    n_models = len(reconstructions)
    
    # Create figure: 1 input + n_models outputs + n_models diffs
    # Layout: top row = input (spans multiple columns) + outputs, bottom row = diffs
    fig = plt.figure(figsize=(20, 8))
    gs = fig.add_gridspec(2, n_models + 1, hspace=0.3, wspace=0.2)
    
    fig.suptitle(f'Architecture Family: {family_name.upper()} | Image: {sample_image_name}', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    # Convert input to numpy
    orig_np = input_img.permute(1, 2, 0).numpy()
    
    # Top-left: Input image (spans first column, both rows)
    ax_input = fig.add_subplot(gs[:, 0])
    ax_input.imshow(orig_np)
    ax_input.set_title('INPUT IMAGE', fontsize=12, fontweight='bold', pad=10)
    ax_input.axis('off')
    
    # Top row: Reconstructions
    for i, (recon, model_name) in enumerate(zip(reconstructions, model_names)):
        ax = fig.add_subplot(gs[0, i + 1])
        recon_np = recon.permute(1, 2, 0).numpy()
        ax.imshow(recon_np)
        
        # Calculate MAE for this reconstruction
        diff = np.abs(orig_np - recon_np)
        mae = np.mean(diff)
        
        # Shortened model name for display
        short_name = model_name.replace(f'_{family_name}', '').replace('ch_', 'ch\n')
        ax.set_title(f'{short_name}\nMAE: {mae:.4f}', fontsize=9)
        ax.axis('off')
    
    # Bottom row: Difference heatmaps
    for i, (recon, model_name) in enumerate(zip(reconstructions, model_names)):
        ax = fig.add_subplot(gs[1, i + 1])
        recon_np = recon.permute(1, 2, 0).numpy()
        diff = np.abs(orig_np - recon_np)
        overall_diff = diff.mean(axis=2)
        
        im = ax.imshow(overall_diff, cmap='hot', vmin=0, vmax=0.5)
        ax.set_title('Error Heatmap', fontsize=9)
        ax.axis('off')
        
        # Add colorbar to the rightmost heatmap
        if i == len(reconstructions) - 1:
            cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
            cbar.set_label('Absolute Error', rotation=270, labelpad=15, fontsize=9)
    
    plt.tight_layout()
    
    # Save
    safe_family = family_name.replace('/', '_')
    safe_img = sample_image_name.replace('/', '_')
    output_file_png = output_dir / f'family_comparison_{safe_family}_{safe_img}.png'
    output_file_svg = output_dir / f'family_comparison_{safe_family}_{safe_img}.svg'
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print(f"    Saved: {output_file_png}")
    plt.close()


def generate_family_comparison_plots(all_models_data, output_dir):
    """
    Generate one comprehensive plot per architecture family.
    Uses the first test image that all models have processed.
    """
    print("  Creating architecture family comparison plots...")
    
    # Group models by architecture family
    families = {}
    for model_data in all_models_data:
        arch = model_data['arch']
        if arch not in families:
            families[arch] = []
        families[arch].append(model_data)
    
    # Find a common test image that all models have
    all_image_names = set()
    for model_data in all_models_data:
        if all_image_names:
            all_image_names &= set(model_data['images'].keys())
        else:
            all_image_names = set(model_data['images'].keys())
    
    if not all_image_names:
        print("    WARNING: No common test images across all models")
        return
    
    # Use the first common image
    sample_image = sorted(list(all_image_names))[0]
    print(f"    Using test image: {sample_image}")
    
    # Generate one plot per family
    for family_name in ['vanilla', 'medium', 'deep']:
        if family_name in families:
            # Sort models by name for consistent ordering
            family_models = sorted(families[family_name], key=lambda x: x['name'])
            plot_architecture_family_comparison(
                family_name, 
                family_models, 
                sample_image, 
                output_dir
            )
        else:
            print(f"    No models found for family: {family_name}")


def plot_class_color_preservation(all_models_data, taxonomy_colors, output_dir):
    """Plot class color preservation: one plot for latent sizes, one for architectures."""
    print("  Creating class color preservation analysis...")
    
    # Aggregate metrics across all models and images
    class_metrics = {}
    
    for model_data in all_models_data:
        model_name = model_data['name']
        arch = model_data['arch']
        channels = model_data['channels']
        base = model_data['base']
        
        for img_name, result in model_data['images'].items():
            if 'class_metrics' not in result:
                continue
            
            for class_name, metrics in result['class_metrics'].items():
                if class_name not in class_metrics:
                    class_metrics[class_name] = []
                
                class_metrics[class_name].append({
                    'model': model_name,
                    'arch': arch,
                    'channels': channels,
                    'base': base,
                    'mae': metrics['mae_overall'],
                    'drift': metrics['color_drift']
                })
    
    if not class_metrics:
        print("    No class metrics available")
        return
    
    # Convert to DataFrame for easier plotting
    all_metrics = []
    for class_name, metrics_list in class_metrics.items():
        for m in metrics_list:
            all_metrics.append({
                'class': class_name,
                'model': m['model'],
                'arch': m['arch'],
                'channels': m['channels'],
                'base': m['base'],
                'mae': m['mae'],
                'drift': m['drift']
            })
    
    df = pd.DataFrame(all_metrics)
    
    if df.empty:
        print("    No data for class color preservation")
        return
    
    class_names = sorted(df['class'].unique())
    
    # Plot 1: Comparison by architecture (8 channels - middle ground)
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('Class Color Preservation by Architecture (8 Channels)', fontsize=14, fontweight='bold')
    
    # Focus on 8 channel models for fair comparison
    ch8_df = df[df['channels'] == 8]
    
    if not ch8_df.empty:
        ax = axes[0]
        arch_list = sorted(ch8_df['arch'].unique())
        x = np.arange(len(class_names))
        width = 0.8 / max(len(arch_list), 1)
        
        for i, arch in enumerate(arch_list):
            arch_data = ch8_df[ch8_df['arch'] == arch]
            mae_by_class = [arch_data[arch_data['class'] == c]['mae'].mean() 
                           if not arch_data[arch_data['class'] == c].empty else 0 
                           for c in class_names]
            ax.bar(x + i * width, mae_by_class, width, 
                  label=arch.capitalize(), alpha=0.8)
        
        ax.set_xlabel('Class')
        ax.set_ylabel('Mean Absolute Error')
        ax.set_title('Color Error by Architecture (8 Channels)')
        ax.set_xticks(x + width * (len(arch_list) - 1) / 2)
        ax.set_xticklabels(class_names, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        
        ax = axes[1]
        for i, arch in enumerate(arch_list):
            arch_data = ch8_df[ch8_df['arch'] == arch]
            drift_by_class = [arch_data[arch_data['class'] == c]['drift'].mean() 
                             if not arch_data[arch_data['class'] == c].empty else 0 
                             for c in class_names]
            ax.plot(x, drift_by_class, marker='o', 
                   label=arch.capitalize(), linewidth=2)
        
        ax.set_xlabel('Class')
        ax.set_ylabel('Color Drift')
        ax.set_title('Color Drift by Architecture')
        ax.set_xticks(x)
        ax.set_xticklabels(class_names, rotation=45, ha='right')
        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)
        ax.legend()
        ax.grid(True, alpha=0.3)
    else:
        for ax in axes:
            ax.text(0.5, 0.5, 'No 8-channel data available', 
                   ha='center', va='center', transform=ax.transAxes)
    
    plt.tight_layout()
    output_file_png = output_dir / 'class_preservation_by_architecture.png'
    output_file_svg = output_dir / 'class_preservation_by_architecture.svg'
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print("    Saved: " + str(output_file_png))
    plt.close()
    
    # Plot 2: Comparison by latent channels (base=32)
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('Class Color Preservation by Latent Channels (Base=32)', fontsize=14, fontweight='bold')
    
    # Focus on base=32 models for fair comparison
    base32_df = df[df['base'] == 32]
    
    if not base32_df.empty:
        ax = axes[0]
        channels_list = sorted(base32_df['channels'].unique())
        x = np.arange(len(class_names))
        width = 0.8 / max(len(channels_list), 1)
        
        for i, ch in enumerate(channels_list):
            ch_data = base32_df[base32_df['channels'] == ch]
            mae_by_class = [ch_data[ch_data['class'] == c]['mae'].mean() 
                           if not ch_data[ch_data['class'] == c].empty else 0 
                           for c in class_names]
            ax.bar(x + i * width, mae_by_class, width, 
                  label=str(ch) + ' channels', alpha=0.8)
        
        ax.set_xlabel('Class')
        ax.set_ylabel('Mean Absolute Error')
        ax.set_title('Color Error by Latent Channels (Base=32)')
        ax.set_xticks(x + width * (len(channels_list) - 1) / 2)
        ax.set_xticklabels(class_names, rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3, axis='y')
        
        ax = axes[1]
        for i, ch in enumerate(channels_list):
            ch_data = base32_df[base32_df['channels'] == ch]
            drift_by_class = [ch_data[ch_data['class'] == c]['drift'].mean() 
                             if not ch_data[ch_data['class'] == c].empty else 0 
                             for c in class_names]
            ax.plot(x, drift_by_class, marker='o', 
                   label=str(ch) + ' channels', linewidth=2)
        
        ax.set_xlabel('Class')
        ax.set_ylabel('Color Drift')
        ax.set_title('Color Drift by Latent Channels')
        ax.set_xticks(x)
        ax.set_xticklabels(class_names, rotation=45, ha='right')
        ax.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)
        ax.legend()
        ax.grid(True, alpha=0.3)
    else:
        for ax in axes:
            ax.text(0.5, 0.5, 'No base=32 data available', 
                   ha='center', va='center', transform=ax.transAxes)
    
    plt.tight_layout()
    output_file_png = output_dir / 'class_preservation_by_channels.png'
    output_file_svg = output_dir / 'class_preservation_by_channels.svg'
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print("    Saved: " + str(output_file_png))
    plt.close()


def plot_aggregate_metrics(all_models_data, output_dir):
    """Plot aggregate metrics across all models and images."""
    print("  Creating aggregate comparison plots...")
    
    # Prepare data
    rows = []
    for model_data in all_models_data:
        for img_name, result in model_data['images'].items():
            row = {
                'model': model_data['name'],
                'architecture': model_data['arch'],
                'channels': model_data['channels'],
                'base': model_data['base'],
                'image': img_name
            }
            row.update(result['metrics'])
            rows.append(row)
    
    if not rows:
        print("  WARNING: No data to plot")
        return pd.DataFrame()
    
    df = pd.DataFrame(rows)
    
    # Check if we have the required columns
    if 'model' not in df.columns or 'overall_mae' not in df.columns:
        print("  ERROR: Missing required columns in dataframe")
        print("  Available columns: " + str(df.columns.tolist()))
        return df
    
    # Plot 1: Overall MAE and Architecture comparison
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('Model Performance Comparison', fontsize=14, fontweight='bold')
    
    ax = axes[0]
    model_mae = df.groupby('model')['overall_mae'].mean().sort_values()
    colors = ['red' if 'vanilla' in m else 'green' if 'medium' in m else 'blue' 
              for m in model_mae.index]
    model_mae.plot(kind='barh', ax=ax, color=colors, alpha=0.7)
    ax.set_xlabel('Mean Absolute Error')
    ax.set_title('Overall Reconstruction Error by Model')
    ax.grid(True, alpha=0.3, axis='x')
    
    ax = axes[1]
    if 'architecture' in df.columns and df['architecture'].notna().any():
        arch_mae = df.groupby('architecture')['overall_mae'].agg(['mean', 'std'])
        arch_mae['mean'].plot(kind='bar', ax=ax, yerr=arch_mae['std'], 
                              capsize=5, alpha=0.7)
        ax.set_xlabel('Architecture')
        ax.set_ylabel('Mean Absolute Error')
        ax.set_title('Error by Architecture Type')
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_xticklabels(ax.get_xticklabels(), rotation=0)
    else:
        ax.text(0.5, 0.5, 'Architecture data not available', 
                ha='center', va='center', transform=ax.transAxes)
    
    plt.tight_layout()
    output_file = output_dir / 'aggregate_comparison.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print("    Saved: " + str(output_file))
    plt.close()
    
    # Plot 2: Color channel comparison
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('Color Channel Analysis', fontsize=14, fontweight='bold')
    
    ax = axes[0]
    channel_cols = ['red_mae', 'green_mae', 'blue_mae']
    if all(col in df.columns for col in channel_cols):
        channel_means = df[channel_cols].mean()
        bars = ax.bar(['Red', 'Green', 'Blue'], channel_means, 
                      color=['red', 'green', 'blue'], alpha=0.7)
        ax.set_ylabel('Mean Absolute Error')
        ax.set_title('Average Error by Color Channel (all models)')
        ax.grid(True, alpha=0.3, axis='y')
        for bar, val in zip(bars, channel_means):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    str(round(val, 5)), ha='center', va='bottom')
    else:
        ax.text(0.5, 0.5, 'Channel data not available', 
                ha='center', va='center', transform=ax.transAxes)
    
    ax = axes[1]
    corr_cols = ['red_corr', 'green_corr', 'blue_corr']
    if all(col in df.columns for col in corr_cols):
        corr_means = df[corr_cols].mean()
        bars = ax.bar(['Red', 'Green', 'Blue'], corr_means,
                      color=['red', 'green', 'blue'], alpha=0.7)
        ax.set_ylabel('Correlation')
        ax.set_title('Color Preservation by Channel (all models)')
        ax.set_ylim([0, 1])
        ax.grid(True, alpha=0.3, axis='y')
        for bar, val in zip(bars, corr_means):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                    str(round(val, 4)), ha='center', va='bottom')
    else:
        ax.text(0.5, 0.5, 'Correlation data not available', 
                ha='center', va='center', transform=ax.transAxes)
    
    plt.tight_layout()
    output_file = output_dir / 'color_channel_analysis.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print("    Saved: " + str(output_file))
    plt.close()
    
    # Plot 3: Detailed model ranking
    fig, ax = plt.subplots(1, 1, figsize=(14, max(8, len(df['model'].unique()) * 0.4)))
    
    # Calculate composite score (lower is better)
    model_scores = df.groupby('model').agg({
        'overall_mae': 'mean',
        'red_corr': 'mean',
        'green_corr': 'mean',
        'blue_corr': 'mean'
    })
    model_scores['avg_corr'] = model_scores[['red_corr', 'green_corr', 'blue_corr']].mean(axis=1)
    model_scores['composite_score'] = model_scores['overall_mae'] * (2 - model_scores['avg_corr'])
    model_scores = model_scores.sort_values('composite_score')
    
    y_pos = np.arange(len(model_scores))
    bars = ax.barh(y_pos, model_scores['composite_score'], alpha=0.7)
    ax.set_yticks(y_pos)
    ax.set_yticklabels(model_scores.index)
    ax.set_xlabel('Composite Score (lower is better)')
    ax.set_title('Model Ranking by Reconstruction Quality')
    ax.grid(True, alpha=0.3, axis='x')
    
    # Color bars by rank
    cmap = plt.cm.RdYlGn_r
    for i, bar in enumerate(bars):
        bar.set_color(cmap(i / len(bars)))
    
    plt.tight_layout()
    output_file = output_dir / 'model_ranking.png'
    plt.savefig(output_file, dpi=300, bbox_inches='tight')
    print("    Saved: " + str(output_file))
    plt.close()
    
    return df


def main():
    parser = argparse.ArgumentParser(description="Compare autoencoder models")
    parser.add_argument("--experiments_dir", type=str, required=True)
    parser.add_argument("--test_images", type=str, nargs='+', required=True)
    parser.add_argument("--output_dir", type=str, default="model_comparison")
    parser.add_argument("--num_models", type=int, default=10)
    parser.add_argument("--device", type=str, default="cpu", choices=["cpu", "cuda", "mps"])
    parser.add_argument("--taxonomy_path", type=str, default=None, help="Path to taxonomy.json file")
    parser.add_argument("--batch_size", type=int, default=100, help="Number of images to process at once")
    
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*80, flush=True)
    print("MULTI-MODEL RECONSTRUCTION COMPARISON", flush=True)
    print("="*80, flush=True)
    print("Experiments directory: " + args.experiments_dir, flush=True)
    print("Test images: " + str(len(args.test_images)), flush=True)
    print("Batch size: " + str(args.batch_size), flush=True)
    print("Number of models to compare: " + str(args.num_models), flush=True)
    print("Device: " + args.device, flush=True)
    print("="*80, flush=True)
    print()
    
    # Load taxonomy colors
    print("[0/6] Loading taxonomy colors...", flush=True)
    taxonomy_colors = load_taxonomy_colors(args.taxonomy_path)
    print("  Loaded " + str(len(taxonomy_colors)) + " class colors", flush=True)
    print()
    
    # Find models
    print("[1/6] Finding models to compare...", flush=True)
    models_to_compare = find_models_to_compare(args.experiments_dir, args.num_models)
    print("  Found " + str(len(models_to_compare)) + " models", flush=True)
    for m in models_to_compare:
        print("    - " + m['name'] + " (loss: " + str(round(m['best_loss'], 6)) + ")", flush=True)
    print()
    
    # Get image paths
    print("[2/6] Preparing test images...", flush=True)
    image_paths = args.test_images
    print("  Total images to process: " + str(len(image_paths)), flush=True)
    visualization_image_path = image_paths[0]  # First image for visualization
    print()
    
    # Process each model for STATISTICS ONLY
    print("[3/6] Processing models (computing statistics)...", flush=True)
    all_models_data = []
    
    for model_idx, model_info in enumerate(models_to_compare):
        print(f"\n  [{model_idx+1}/{len(models_to_compare)}] Processing model: {model_info['name']}", flush=True)
        
        try:
            model = load_autoencoder_model(
                str(model_info['config']),
                str(model_info['checkpoint']),
                args.device
            )
            
            if model is None:
                continue
            
            model_results = {
                'name': model_info['name'],
                'arch': model_info['arch'],
                'channels': model_info['channels'],
                'base': model_info['base'],
                'model': model,  # Keep model for later visualization
                'config': model_info['config'],
                'checkpoint': model_info['checkpoint'],
                'images': {}
            }
            
            # Process images in batches - STATISTICS ONLY
            num_batches = (len(image_paths) - 1) // args.batch_size + 1
            
            for batch_idx in range(0, len(image_paths), args.batch_size):
                batch_end = min(batch_idx + args.batch_size, len(image_paths))
                batch_paths = image_paths[batch_idx:batch_end]
                batch_num = batch_idx // args.batch_size + 1
                
                print(f"    Batch {batch_num}/{num_batches} ({len(batch_paths)} images)...", end=' ', flush=True)
                
                # Load batch
                test_images, image_names = load_test_images(batch_paths)
                
                # Process each image in batch
                for i, img_name in enumerate(image_names):
                    img = test_images[i:i+1].to(args.device)
                    
                    with torch.no_grad():
                        recon = model(img)
                    
                    # Compute metrics
                    metrics = compute_reconstruction_metrics(
                        img.cpu(), recon.cpu()
                    )
                    
                    # Class-specific color metrics
                    class_metrics = compute_color_class_preservation(
                        test_images[i],
                        recon.cpu().squeeze(0),
                        taxonomy_colors
                    )
                    
                    # Store ONLY metrics (no tensors!)
                    model_results['images'][img_name] = {
                        'metrics': metrics,
                        'class_metrics': class_metrics
                    }
                
                print("Done", flush=True)
                
                # Free memory
                del test_images
                if args.device == 'cuda':
                    torch.cuda.empty_cache()
            
            all_models_data.append(model_results)
            print("    Model completed", flush=True)
            
        except Exception as e:
            print(f"    ERROR: {str(e)}", flush=True)
            import traceback
            traceback.print_exc()
            continue
    
    print("\n  Successfully processed " + str(len(all_models_data)) + " models", flush=True)
    print()
    
    # Now generate visualizations by running inference on ONE image
    print("[4/6] Generating visualization data (running inference on sample image)...", flush=True)
    
    # Load the visualization image once
    viz_images, viz_names = load_test_images([visualization_image_path])
    viz_img_tensor = viz_images[0]
    viz_img_name = viz_names[0]
    
    print(f"  Using image: {viz_img_name}", flush=True)
    
    # Run inference on this one image for each model
    for model_data in all_models_data:
        print(f"    Processing {model_data['name']}...", end=' ', flush=True)
        
        model = model_data['model']
        img = viz_img_tensor.unsqueeze(0).to(args.device)
        
        with torch.no_grad():
            recon = model(img)
        
        # Store visualization data
        model_data['visualization'] = {
            'image_name': viz_img_name,
            'original': viz_img_tensor.clone(),
            'reconstruction': recon.cpu().squeeze(0).clone()
        }
        
        # Clean up model reference to free memory
        del model_data['model']
        
        print("Done", flush=True)
    
    if args.device == 'cuda':
        torch.cuda.empty_cache()
    
    print()
    
    # Generate comparison plots
    print("[5/6] Generating comparison visualizations...", flush=True)
    
    # Architecture family comparisons (3 comprehensive plots)
    generate_family_comparison_plots_from_viz(all_models_data, output_dir)
    
    # Aggregate plots
    df = plot_aggregate_metrics(all_models_data, output_dir)
    
    # Class color preservation analysis
    plot_class_color_preservation(all_models_data, taxonomy_colors, output_dir)
    
    # Save metrics to CSV
    print("[6/6] Saving results...", flush=True)
    csv_file = output_dir / 'comparison_metrics.csv'
    df.to_csv(csv_file, index=False)
    print("  Saved metrics CSV: " + str(csv_file), flush=True)
    
    print("\n" + "="*80, flush=True)
    print("COMPARISON COMPLETE", flush=True)
    print("="*80, flush=True)
    print("\nResults saved to: " + str(output_dir), flush=True)
    print("\nGenerated files:", flush=True)
    print("  family_comparison_*.png - Per-family comprehensive comparisons (3 files)", flush=True)
    print("  aggregate_comparison.png - Overall metrics comparison", flush=True)
    print("  color_channel_analysis.png - Channel-specific analysis", flush=True)
    print("  model_ranking.png - Ranked models by quality", flush=True)
    print("  class_preservation_by_architecture.png - Class-specific color analysis by arch", flush=True)
    print("  class_preservation_by_channels.png - Class-specific color analysis by channels", flush=True)
    print("  comparison_metrics.csv - All metrics data", flush=True)
    print()


def generate_family_comparison_plots_from_viz(all_models_data, output_dir):
    """
    Generate one comprehensive plot per architecture family using pre-computed visualizations.
    """
    print("  Creating architecture family comparison plots...", flush=True)
    
    # Group models by architecture family
    families = {}
    for model_data in all_models_data:
        arch = model_data['arch']
        if arch not in families:
            families[arch] = []
        families[arch].append(model_data)
    
    # Get the visualization image name
    viz_img_name = all_models_data[0]['visualization']['image_name'] if all_models_data else None
    if not viz_img_name:
        print("    WARNING: No visualization data available")
        return
    
    print(f"    Using test image: {viz_img_name}", flush=True)
    
    # Generate one plot per family
    for family_name in ['vanilla', 'medium', 'deep']:
        if family_name in families:
            # Sort models by name for consistent ordering
            family_models = sorted(families[family_name], key=lambda x: x['name'])
            plot_architecture_family_comparison_from_viz(
                family_name, 
                family_models, 
                output_dir
            )
        else:
            print(f"    No models found for family: {family_name}", flush=True)


def plot_architecture_family_comparison_from_viz(family_name, models_data, output_dir):
    """
    Create a single comprehensive figure comparing all models in an architecture family.
    Uses pre-computed visualization data.
    """
    if not models_data:
        print(f"    No models found for family: {family_name}")
        return
    
    # Get the input image and reconstructions from visualization data
    input_img = models_data[0]['visualization']['original']
    img_name = models_data[0]['visualization']['image_name']
    
    reconstructions = []
    model_names = []
    
    for model_data in models_data:
        reconstructions.append(model_data['visualization']['reconstruction'])
        model_names.append(model_data['name'])
    
    n_models = len(reconstructions)
    
    # Create figure
    fig = plt.figure(figsize=(20, 8))
    gs = fig.add_gridspec(2, n_models + 1, hspace=0.3, wspace=0.2)
    
    fig.suptitle(f'Architecture Family: {family_name.upper()} | Image: {img_name}', 
                 fontsize=16, fontweight='bold', y=0.98)
    
    # Convert input to numpy
    orig_np = input_img.permute(1, 2, 0).numpy()
    
    # Top-left: Input image (spans first column, both rows)
    ax_input = fig.add_subplot(gs[:, 0])
    ax_input.imshow(orig_np)
    ax_input.set_title('INPUT IMAGE', fontsize=12, fontweight='bold', pad=10)
    ax_input.axis('off')
    
    # Top row: Reconstructions
    for i, (recon, model_name) in enumerate(zip(reconstructions, model_names)):
        ax = fig.add_subplot(gs[0, i + 1])
        recon_np = recon.permute(1, 2, 0).numpy()
        ax.imshow(recon_np)
        
        # Calculate MAE
        diff = np.abs(orig_np - recon_np)
        mae = np.mean(diff)
        
        # Shortened model name
        short_name = model_name.replace(f'_{family_name}', '').replace('ch_', 'ch\n')
        ax.set_title(f'{short_name}\nMAE: {mae:.4f}', fontsize=9)
        ax.axis('off')
    
    # Bottom row: Difference heatmaps
    for i, (recon, model_name) in enumerate(zip(reconstructions, model_names)):
        ax = fig.add_subplot(gs[1, i + 1])
        recon_np = recon.permute(1, 2, 0).numpy()
        diff = np.abs(orig_np - recon_np)
        overall_diff = diff.mean(axis=2)
        
        im = ax.imshow(overall_diff, cmap='hot', vmin=0, vmax=0.5)
        ax.set_title('Error Heatmap', fontsize=9)
        ax.axis('off')
        
        # Add colorbar to rightmost heatmap
        if i == len(reconstructions) - 1:
            cbar = plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)
            cbar.set_label('Absolute Error', rotation=270, labelpad=15, fontsize=9)
    
    plt.tight_layout()
    
    # Save
    safe_family = family_name.replace('/', '_')
    safe_img = img_name.replace('/', '_')
    output_file_png = output_dir / f'family_comparison_{safe_family}_{safe_img}.png'
    output_file_svg = output_dir / f'family_comparison_{safe_family}_{safe_img}.svg'
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print(f"    Saved: {output_file_png}", flush=True)
    plt.close()

if __name__ == "__main__":
    main()


================================================================================
FILE: scripts\evaluate_ae_sweep.py
================================================================================

#!/usr/bin/env python3
import os
import sys
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import yaml
import re
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

try:
    import torch
    import torch.nn as nn
    from torchvision import transforms
    from PIL import Image
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("Warning: PyTorch not available. Skipping reconstruction analysis.")

sns.set_style("whitegrid")
sns.set_context("notebook", font_scale=1.1)
plt.rcParams['figure.dpi'] = 300


def parse_config_name(config_name):
    pattern = r'(?:ae_)?diff_(\d+)ch_(\d+)x(\d+)_(vanilla|skip|medium|deep)'
    match = re.search(pattern, config_name)
    
    if match:
        channels, base_w, base_h, arch = match.groups()
        if arch == 'skip':
            arch = 'medium'
        return {
            'latent_channels': int(channels),
            'latent_base': int(base_w),
            'latent_spatial': base_w + 'x' + base_h,
            'latent_dim': int(channels) * int(base_w) * int(base_h),
            'architecture': arch
        }
    return None


def load_experiment_results(experiments_dir):
    experiments_dir = Path(experiments_dir)
    all_results = []
    
    print("Scanning directory: " + str(experiments_dir))
    print("=" * 80)
    
    exp_dirs = [d for d in experiments_dir.iterdir() if d.is_dir()]
    print("Found " + str(len(exp_dirs)) + " experiment directories")
    
    for exp_dir in exp_dirs:
        metrics_file = exp_dir / "metrics.csv"
        config_file = exp_dir / "config_used.yml"
        
        if not metrics_file.exists():
            print("Skipping " + exp_dir.name + " (no metrics.csv)")
            continue
        
        metrics_df = pd.read_csv(metrics_file)
        
        config_params = {}
        if config_file.exists():
            with open(config_file, 'r') as f:
                config = yaml.safe_load(f)
                
            if 'model_cfg' in config and 'encoder' in config['model_cfg']:
                enc_cfg = config['model_cfg']['encoder']
                config_params = {
                    'latent_dim': enc_cfg.get('latent_dim'),
                    'latent_channels': enc_cfg.get('latent_channels'),
                    'latent_base': enc_cfg.get('latent_base'),
                    'image_size': enc_cfg.get('image_size'),
                    'num_layers': len(enc_cfg.get('layers', []))
                }
                
                if config_params['latent_base']:
                    base_str = str(config_params['latent_base'])
                    config_params['latent_spatial'] = base_str + 'x' + base_str
                
                dir_name = exp_dir.name.lower()
                if 'vanilla' in dir_name:
                    config_params['architecture'] = 'vanilla'
                elif 'skip' in dir_name or 'medium' in dir_name:
                    config_params['architecture'] = 'medium'
                elif 'deep' in dir_name:
                    config_params['architecture'] = 'deep'
                else:
                    if config_params['num_layers'] == 3:
                        config_params['architecture'] = 'vanilla'
                    elif config_params['num_layers'] == 4:
                        config_params['architecture'] = 'medium'
                    elif config_params['num_layers'] == 5:
                        config_params['architecture'] = 'deep'
        
        if not config_params or 'architecture' not in config_params:
            parsed = parse_config_name(exp_dir.name)
            if parsed:
                config_params.update(parsed)
        
        for col, val in config_params.items():
            metrics_df[col] = val
        
        metrics_df['experiment_name'] = exp_dir.name
        metrics_df['experiment_dir'] = str(exp_dir)
        
        all_results.append(metrics_df)
        print("Loaded " + exp_dir.name + ": " + str(len(metrics_df)) + " epochs")
    
    if not all_results:
        raise ValueError("No valid experiment results found!")
    
    df = pd.concat(all_results, ignore_index=True)
    
    print("=" * 80)
    print("Total experiments loaded: " + str(df['experiment_name'].nunique()))
    print("Total epochs: " + str(len(df)))
    print()
    
    return df


def plot_training_curves(df, output_dir):
    print("Generating training curves...")
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('Training Loss Curves - Overview', fontsize=16, fontweight='bold')
    
    ax = axes[0]
    for exp_name in df['experiment_name'].unique():
        exp_data = df[df['experiment_name'] == exp_name]
        ax.plot(exp_data['epoch'], exp_data['loss'], alpha=0.6, linewidth=1.5, label=exp_name)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title('All Experiments')
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    ax.grid(True, alpha=0.3)
    
    ax = axes[1]
    if 'architecture' in df.columns:
        for arch in df['architecture'].dropna().unique():
            arch_data = df[df['architecture'] == arch]
            for exp_name in arch_data['experiment_name'].unique():
                exp_data = arch_data[arch_data['experiment_name'] == exp_name]
                label_text = arch + ": " + exp_name
                ax.plot(exp_data['epoch'], exp_data['loss'], label=label_text, alpha=0.7, linewidth=1.5)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title('By Architecture')
    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_file_png = output_dir / "training_curves_overview.png"
    output_file_svg = output_dir / "training_curves_overview.svg"
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print("  Saved: " + str(output_file_png))
    print("  Saved: " + str(output_file_svg))
    plt.close()
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('Training Loss Curves - Hyperparameters', fontsize=16, fontweight='bold')
    
    ax = axes[0]
    if 'latent_channels' in df.columns:
        for channels in sorted(df['latent_channels'].dropna().unique()):
            channel_data = df[df['latent_channels'] == channels]
            mean_loss = channel_data.groupby('epoch')['loss'].mean()
            std_loss = channel_data.groupby('epoch')['loss'].std()
            epochs = mean_loss.index
            label_text = str(int(channels)) + ' channels'
            ax.plot(epochs, mean_loss, label=label_text, linewidth=2)
            ax.fill_between(epochs, mean_loss - std_loss, mean_loss + std_loss, alpha=0.2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title('By Latent Channels (Mean +/- Std)')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    ax = axes[1]
    if 'latent_spatial' in df.columns:
        for spatial in sorted(df['latent_spatial'].dropna().unique()):
            spatial_data = df[df['latent_spatial'] == spatial]
            mean_loss = spatial_data.groupby('epoch')['loss'].mean()
            std_loss = spatial_data.groupby('epoch')['loss'].std()
            epochs = mean_loss.index
            ax.plot(epochs, mean_loss, label=str(spatial), linewidth=2)
            ax.fill_between(epochs, mean_loss - std_loss, mean_loss + std_loss, alpha=0.2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss')
    ax.set_title('By Latent Spatial Size (Mean +/- Std)')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    output_file_png = output_dir / "training_curves_hyperparams.png"
    output_file_svg = output_dir / "training_curves_hyperparams.svg"
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print("  Saved: " + str(output_file_png))
    print("  Saved: " + str(output_file_svg))
    plt.close()


def plot_final_performance(df, output_dir):
    print("Generating final performance comparisons...")
    
    final_results = df.loc[df.groupby('experiment_name')['epoch'].idxmax()]
    best_results = df.loc[df.groupby('experiment_name')['loss'].idxmin()]
    best_results = best_results.rename(columns={'loss': 'best_loss', 'epoch': 'best_epoch'})
    
    comparison = final_results.merge(
        best_results[['experiment_name', 'best_loss', 'best_epoch']], 
        on='experiment_name', 
        suffixes=('', '_best')
    )
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('Final Performance - Best Loss Analysis', fontsize=16, fontweight='bold')
    
    ax = axes[0]
    if 'architecture' in comparison.columns:
        arch_order = comparison.groupby('architecture')['best_loss'].median().sort_values().index
        sns.boxplot(data=comparison, x='architecture', y='best_loss', order=arch_order, ax=ax)
        sns.swarmplot(data=comparison, x='architecture', y='best_loss', order=arch_order, 
                     color='black', alpha=0.5, size=4, ax=ax)
        ax.set_title('Best Loss by Architecture')
        ax.set_xlabel('Architecture')
        ax.set_ylabel('Best Loss')
    
    ax = axes[1]
    if 'latent_channels' in comparison.columns:
        comparison_sorted = comparison.sort_values('latent_channels')
        sns.boxplot(data=comparison_sorted, x='latent_channels', y='best_loss', ax=ax)
        sns.swarmplot(data=comparison_sorted, x='latent_channels', y='best_loss', 
                     color='black', alpha=0.5, size=4, ax=ax)
        ax.set_title('Best Loss by Latent Channels')
        ax.set_xlabel('Latent Channels')
        ax.set_ylabel('Best Loss')
    
    plt.tight_layout()
    output_file_png = output_dir / "final_performance_loss.png"
    output_file_svg = output_dir / "final_performance_loss.svg"
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print("  Saved: " + str(output_file_png))
    print("  Saved: " + str(output_file_svg))
    plt.close()
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('Final Performance - Spatial Size & Convergence', fontsize=16, fontweight='bold')
    
    ax = axes[0]
    if 'latent_spatial' in comparison.columns:
        spatial_order = comparison.groupby('latent_spatial')['best_loss'].median().sort_values().index
        sns.boxplot(data=comparison, x='latent_spatial', y='best_loss', order=spatial_order, ax=ax)
        sns.swarmplot(data=comparison, x='latent_spatial', y='best_loss', order=spatial_order,
                     color='black', alpha=0.5, size=4, ax=ax)
        ax.set_title('Best Loss by Latent Spatial Size')
        ax.set_xlabel('Latent Spatial Size')
        ax.set_ylabel('Best Loss')
    
    ax = axes[1]
    if 'architecture' in comparison.columns:
        sns.boxplot(data=comparison, x='architecture', y='best_epoch', ax=ax)
        sns.swarmplot(data=comparison, x='architecture', y='best_epoch', 
                     color='black', alpha=0.5, size=4, ax=ax)
        ax.set_title('Epoch of Best Performance')
        ax.set_xlabel('Architecture')
        ax.set_ylabel('Best Epoch')
    
    plt.tight_layout()
    output_file_png = output_dir / "final_performance_convergence.png"
    output_file_svg = output_dir / "final_performance_convergence.svg"
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print("  Saved: " + str(output_file_png))
    print("  Saved: " + str(output_file_svg))
    plt.close()


def plot_heatmaps(df, output_dir):
    print("Generating heatmaps...")
    
    best_results = df.loc[df.groupby('experiment_name')['loss'].idxmin()]
    
    if 'architecture' not in best_results.columns or 'latent_channels' not in best_results.columns:
        print("  Skipping heatmaps: missing required columns")
        return
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle('Performance Heatmaps (Best Loss)', fontsize=16, fontweight='bold')
    
    architectures = sorted(best_results['architecture'].unique())
    
    for idx, arch in enumerate(architectures):
        arch_data = best_results[best_results['architecture'] == arch]
        
        if 'latent_channels' in arch_data.columns and 'latent_base' in arch_data.columns:
            pivot = arch_data.pivot_table(
                values='loss',
                index='latent_base',
                columns='latent_channels',
                aggfunc='mean'
            )
            
            ax = axes[idx]
            sns.heatmap(pivot, annot=True, fmt='.4f', cmap='YlOrRd_r', 
                       cbar_kws={'label': 'Best Loss'}, ax=ax)
            ax.set_title('Architecture: ' + arch.capitalize())
            ax.set_xlabel('Latent Channels')
            ax.set_ylabel('Latent Base Size')
    
    plt.tight_layout()
    output_file_png = output_dir / "performance_heatmaps.png"
    output_file_svg = output_dir / "performance_heatmaps.svg"
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print("  Saved: " + str(output_file_png))
    print("  Saved: " + str(output_file_svg))
    plt.close()


def plot_convergence_analysis(df, output_dir):
    print("Generating convergence analysis...")
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('Convergence Analysis - Variance & Stability', fontsize=16, fontweight='bold')
    
    ax = axes[0]
    if 'architecture' in df.columns:
        for arch in df['architecture'].dropna().unique():
            arch_data = df[df['architecture'] == arch]
            variance = arch_data.groupby('epoch')['loss'].var()
            ax.plot(variance.index, variance.values, label=arch.capitalize(), linewidth=2)
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Loss Variance')
    ax.set_title('Loss Variance Across Configurations')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    ax = axes[1]
    if 'std_loss' in df.columns:
        final_stability = df.loc[df.groupby('experiment_name')['epoch'].idxmax()]
        if 'architecture' in final_stability.columns:
            sns.boxplot(data=final_stability, x='architecture', y='std_loss', ax=ax)
            ax.set_title('Training Stability (Final Epoch Std)')
            ax.set_xlabel('Architecture')
            ax.set_ylabel('Batch Loss Std Dev')
    
    plt.tight_layout()
    output_file_png = output_dir / "convergence_variance.png"
    output_file_svg = output_dir / "convergence_variance.svg"
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print("  Saved: " + str(output_file_png))
    print("  Saved: " + str(output_file_svg))
    plt.close()
    
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))
    fig.suptitle('Convergence Analysis - Speed & Improvement', fontsize=16, fontweight='bold')
    
    ax = axes[0]
    convergence_epochs = []
    for exp_name in df['experiment_name'].unique():
        exp_data = df[df['experiment_name'] == exp_name].sort_values('epoch')
        best_loss = exp_data['loss'].min()
        threshold = best_loss * 1.05
        
        converged = exp_data[exp_data['loss'] <= threshold]
        if not converged.empty:
            conv_epoch = converged.iloc[0]['epoch']
            convergence_epochs.append({
                'experiment_name': exp_name,
                'convergence_epoch': conv_epoch,
                'architecture': exp_data.iloc[0].get('architecture'),
                'latent_channels': exp_data.iloc[0].get('latent_channels')
            })
    
    if convergence_epochs:
        conv_df = pd.DataFrame(convergence_epochs)
        if 'architecture' in conv_df.columns:
            sns.boxplot(data=conv_df, x='architecture', y='convergence_epoch', ax=ax)
            ax.set_title('Convergence Speed (Epochs to 95% of Best)')
            ax.set_xlabel('Architecture')
            ax.set_ylabel('Epochs')
    
    ax = axes[1]
    improvement_rates = []
    for exp_name in df['experiment_name'].unique():
        exp_data = df[df['experiment_name'] == exp_name].sort_values('epoch')
        if len(exp_data) > 1:
            initial_loss = exp_data.iloc[0]['loss']
            final_loss = exp_data.iloc[-1]['loss']
            epochs = exp_data.iloc[-1]['epoch']
            rate = (initial_loss - final_loss) / epochs
            improvement_rates.append({
                'experiment_name': exp_name,
                'improvement_rate': rate,
                'architecture': exp_data.iloc[0].get('architecture')
            })
    
    if improvement_rates:
        imp_df = pd.DataFrame(improvement_rates)
        if 'architecture' in imp_df.columns:
            sns.boxplot(data=imp_df, x='architecture', y='improvement_rate', ax=ax)
            ax.set_title('Average Improvement Rate')
            ax.set_xlabel('Architecture')
            ax.set_ylabel('Loss Decrease per Epoch')
    
    plt.tight_layout()
    output_file_png = output_dir / "convergence_speed.png"
    output_file_svg = output_dir / "convergence_speed.svg"
    plt.savefig(output_file_png, dpi=300, bbox_inches='tight')
    plt.savefig(output_file_svg, format='svg', bbox_inches='tight')
    print("  Saved: " + str(output_file_png))
    print("  Saved: " + str(output_file_svg))
    plt.close()


def generate_summary_table(df, output_dir):
    print("Generating summary table...")
    
    best_results = df.loc[df.groupby('experiment_name')['loss'].idxmin()]
    final_results = df.loc[df.groupby('experiment_name')['epoch'].idxmax()]
    
    summary_data = []
    for exp_name in df['experiment_name'].unique():
        exp_data = df[df['experiment_name'] == exp_name]
        best_row = best_results[best_results['experiment_name'] == exp_name].iloc[0]
        final_row = final_results[final_results['experiment_name'] == exp_name].iloc[0]
        
        summary_data.append({
            'Experiment': exp_name,
            'Architecture': best_row.get('architecture', 'N/A'),
            'Latent Channels': best_row.get('latent_channels', 'N/A'),
            'Latent Spatial': best_row.get('latent_spatial', 'N/A'),
            'Latent Dim': best_row.get('latent_dim', 'N/A'),
            'Best Loss': best_row['loss'],
            'Best Epoch': best_row['epoch'],
            'Final Loss': final_row['loss'],
            'Total Epochs': final_row['epoch'],
            'Improvement': best_row['loss'] - final_row['loss'],
            'Experiment Dir': best_row.get('experiment_dir', '')
        })
    
    summary_df = pd.DataFrame(summary_data)
    summary_df = summary_df.sort_values('Best Loss')
    
    output_file = output_dir / "summary_table.csv"
    summary_df.to_csv(output_file, index=False)
    print("  Saved: " + str(output_file))
    
    print("\n" + "="*80)
    print("TOP 10 CONFIGURATIONS (by Best Loss):")
    print("="*80)
    print(summary_df.head(10).to_string(index=False))
    print()
    
    return summary_df


def main():
    parser = argparse.ArgumentParser(description="Evaluate autoencoder sweep experiments")
    parser.add_argument("--experiments_dir", type=str, required=True)
    parser.add_argument("--output_dir", type=str, default="evaluation_results")
    parser.add_argument("--test_image", type=str, default=None)
    parser.add_argument("--device", type=str, default="cpu", choices=["cpu", "cuda", "mps"])
    
    args = parser.parse_args()
    
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*80)
    print("AUTOENCODER SWEEP EVALUATION")
    print("="*80)
    print("Experiments directory: " + args.experiments_dir)
    print("Output directory: " + str(output_dir))
    if args.test_image:
        print("Test image: " + args.test_image)
        print("Device: " + args.device)
    print()
    
    df = load_experiment_results(args.experiments_dir)
    
    print("\nGenerating visualizations...")
    print("="*80)
    
    plot_training_curves(df, output_dir)
    plot_final_performance(df, output_dir)
    plot_heatmaps(df, output_dir)
    plot_convergence_analysis(df, output_dir)
    summary_df = generate_summary_table(df, output_dir)
    
    print("="*80)
    print("EVALUATION COMPLETE")
    print("="*80)
    print("\nAll results saved to: " + str(output_dir))
    print("\nGenerated files:")
    print("  training_curves_overview.png / .svg")
    print("  training_curves_hyperparams.png / .svg")
    print("  final_performance_loss.png / .svg")
    print("  final_performance_convergence.png / .svg")
    print("  performance_heatmaps.png / .svg")
    print("  convergence_variance.png / .svg")
    print("  convergence_speed.png / .svg")
    print("  summary_table.csv")
    print()


if __name__ == "__main__":
    main()


================================================================================
FILE: scripts\test_autoencoder_identity.py
================================================================================

"""
test_autoencoder_identity.py
Check if a trained AutoEncoder behaves like an identity mapping.
Saves metrics and diagnostic plots to tests/identity_check_<model_name>/
"""

from __future__ import annotations
import os, sys, argparse, yaml, random, time, json
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from sklearn.decomposition import PCA
from torchvision import utils, transforms
from pathlib import Path

# optional SSIM
try:
    from torchmetrics.image import structural_similarity_index_measure as ssim_tm
    TORCHMETRICS_AVAILABLE = True
except Exception:
    from skimage.metrics import structural_similarity as ssim_sk
    TORCHMETRICS_AVAILABLE = False

sys.path.append(str(Path(__file__).parent.parent / "modules"))
from datasets import LayoutDataset
from autoencoder import AutoEncoder


# ----------------- Utility -----------------
def ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)


def parse_model_name(config_path: str) -> str:
    base = os.path.basename(config_path)
    name = os.path.splitext(base)[0]
    if "_" in name:
        return "_".join(name.split("_")[1:])  # drop leading 'config'
    return name


def to_numpy(t: torch.Tensor) -> np.ndarray:
    return t.detach().cpu().numpy()


# ----------------- Main Class -----------------
class IdentityTester:
    def __init__(self, cfg, checkpoint_path, manifest_path, device="auto", batch_size=64):
        self.device = self._get_device(device)
        self.cfg = cfg
        self.batch_size = batch_size
        self.model = AutoEncoder.from_config(cfg).to(self.device)
        self._load_checkpoint(checkpoint_path)

        transform = transforms.Compose([
            transforms.Resize((cfg["encoder"]["image_size"], cfg["encoder"]["image_size"])),
            transforms.ToTensor()
        ])
        self.dataset = LayoutDataset(manifest_path, transform=transform, mode="all", skip_empty=True)
        print(f"[DATASET] Loaded {len(self.dataset)} samples")

    def _get_device(self, device_str):
        if device_str == "auto":
            if torch.cuda.is_available():
                print("[DEVICE] Using CUDA")
                return "cuda"
            elif torch.backends.mps.is_available():
                print("[DEVICE] Using MPS")
                return "mps"
            return "cpu"
        return device_str

    def _load_checkpoint(self, path):
        ckpt = torch.load(path, map_location=self.device)
        self.model.load_state_dict(ckpt)
        self.model.eval()
        print(f"[MODEL] Loaded checkpoint from {path}")

    # ---------- Tests ----------
    def run_tests(self, outdir: str):
        ensure_dir(outdir)
        print(f"[OUTPUT] Results will be saved to {outdir}")

        loader = torch.utils.data.DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)
        batch = next(iter(loader))
        imgs = batch["layout"].to(self.device)
        with torch.no_grad():
            z = self.model.encoder(imgs)
            recon = self.model.decoder(z)

        # Metrics
        mse = nn.MSELoss()(recon, imgs).item()
        l1 = nn.L1Loss()(recon, imgs).item()
        ssim = self._compute_ssim(recon, imgs)
        latent_var = float(torch.var(z).item())
        latent_mean = float(torch.mean(z).item())
        print(f"[METRICS] MSE={mse:.6f}  L1={l1:.6f}  SSIM={ssim:.6f}")
        print(f"[LATENT] mean={latent_mean:.4f}, var={latent_var:.4f}")

        # Pairwise latent difference
        diffs = []
        for i in range(len(z) - 1):
            diffs.append(torch.mean(torch.abs(z[i] - z[i + 1])).item())
        latent_diff_mean = float(np.mean(diffs))
        print(f"[LATENT] avg pairwise |Î”z|={latent_diff_mean:.6f}")

        # Save reconstruction grid
        self._save_reconstruction_grid(imgs, recon, outdir)
        # Latent histogram
        self._plot_latent_histogram(z, outdir)
        # PCA
        self._plot_pca(z, outdir)
        # Interpolation
        self._interpolate(z, outdir)
        # Random latent decode
        self._random_decode(z, outdir)

        metrics = {
            "mse": mse,
            "l1": l1,
            "ssim": ssim,
            "latent_mean": latent_mean,
            "latent_var": latent_var,
            "latent_diff_mean": latent_diff_mean
        }
        with open(os.path.join(outdir, "metrics.json"), "w") as f:
            json.dump(metrics, f, indent=2)
        print("[DONE] Metrics and plots saved")

    # ---------- Helpers ----------
    def _compute_ssim(self, recon, imgs):
        if TORCHMETRICS_AVAILABLE:
            metric = ssim_tm().to(self.device)
            return float(metric(recon, imgs).item())
        else:
            img_np = to_numpy(imgs[0].permute(1, 2, 0))
            rec_np = to_numpy(recon[0].permute(1, 2, 0))
            return float(ssim_sk(img_np, rec_np, channel_axis=2, data_range=1.0))

    def _save_reconstruction_grid(self, imgs, recon, outdir):
        comparison = torch.cat([imgs[:4], recon[:4]], dim=0)
        grid = utils.make_grid(comparison, nrow=4, value_range=(0, 1))
        path = os.path.join(outdir, "reconstruction_grid.png")
        utils.save_image(grid, path)
        print(f"[SAVE] reconstruction grid -> {path}")

    def _plot_latent_histogram(self, z, outdir):
        z_np = to_numpy(z).flatten()
        plt.figure(figsize=(6, 4))
        sns.histplot(z_np, bins=80, kde=True)
        plt.title("Latent Value Distribution")
        plt.tight_layout()
        path = os.path.join(outdir, "latent_hist.png")
        plt.savefig(path)
        plt.close()
        print(f"[SAVE] latent histogram -> {path}")

    def _plot_pca(self, z, outdir):
        z_flat = z.view(z.size(0), -1).cpu().numpy()
        pca = PCA(n_components=min(10, z_flat.shape[1]))
        pca.fit(z_flat)
        var_ratio = pca.explained_variance_ratio_
        plt.figure(figsize=(6, 4))
        sns.barplot(x=np.arange(len(var_ratio)), y=var_ratio)
        plt.title("PCA Explained Variance Ratio (first 10)")
        plt.tight_layout()
        path = os.path.join(outdir, "latent_pca.png")
        plt.savefig(path)
        plt.close()
        print(f"[SAVE] latent PCA plot -> {path}")

    def _interpolate(self, z, outdir):
        if z.size(0) < 2:
            return
        z1, z2 = z[0], z[1]
        alphas = torch.linspace(0, 1, 5).to(self.device)
        imgs = []
        with torch.no_grad():
            for a in alphas:
                zi = (1 - a) * z1 + a * z2
                imgs.append(self.model.decoder(zi.unsqueeze(0))[0])
        grid = utils.make_grid(torch.stack(imgs), nrow=5, value_range=(0, 1))
        path = os.path.join(outdir, "latent_interpolation.png")
        utils.save_image(grid, path)
        print(f"[SAVE] latent interpolation -> {path}")

    def _random_decode(self, z, outdir):
        shape = z.shape
        rand_z = torch.randn_like(z)
        with torch.no_grad():
            rand_img = self.model.decoder(rand_z)
        grid = utils.make_grid(rand_img[:4], nrow=4, value_range=(0, 1))
        path = os.path.join(outdir, "random_latent_decode.png")
        utils.save_image(grid, path)
        print(f"[SAVE] random latent decode -> {path}")


# ----------------- CLI -----------------
def main():
    parser = argparse.ArgumentParser(description="Test AutoEncoder for identity mapping behavior")
    parser.add_argument("--config", type=str, required=True)
    parser.add_argument("--checkpoint", type=str, required=True)
    parser.add_argument("--layout_manifest", type=str, required=True)
    parser.add_argument("--device", type=str, default="auto", choices=["auto", "cuda", "cpu", "mps"])
    parser.add_argument("--batch_size", type=int, default=64)
    args = parser.parse_args()

    with open(args.config, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    model_suffix = parse_model_name(args.config)
    outdir = os.path.join("tests", f"identity_check_{model_suffix}")
    ensure_dir(outdir)

    print(f"[START] Testing model: {model_suffix}")
    print(f"Results -> {outdir}\n")

    tester = IdentityTester(cfg, args.checkpoint, args.layout_manifest, args.device, args.batch_size)
    tester.run_tests(outdir)


if __name__ == "__main__":
    main()


================================================================================
FILE: training\sampling_utils.py
================================================================================

"""
Sampling utilities for diffusion model inference during training.
"""

import torch
import torch.nn.functional as F
from pathlib import Path
from torchvision.utils import save_image


@torch.no_grad()
def generate_samples_unconditioned(diffusion_model, exp_dir, epoch, num_samples, device):
    """
    Generate and save sample images without conditioning using LatentDiffusion.
    Uses fixed noise for consistency across epochs.
    
    Args:
        diffusion_model: LatentDiffusion model
        exp_dir: Experiment directory
        epoch: Current epoch number
        num_samples: Number of samples to generate
        device: Device string
    """
    diffusion_model.eval()
    exp_dir = Path(exp_dir)
    
    # Load or create fixed latents for reproducibility
    fixed_latents_path = exp_dir / "fixed_latents.pt"
    if fixed_latents_path.exists():
        # Load existing fixed noise
        fixed_noise = torch.load(fixed_latents_path).to(device)
    else:
        # Create and save fixed noise
        torch.manual_seed(1234)
        torch.cuda.manual_seed_all(1234)
        C, H, W = diffusion_model.latent_shape
        fixed_noise = torch.randn(num_samples, C, H, W)
        torch.save(fixed_noise.cpu(), fixed_latents_path)
        fixed_noise = fixed_noise.to(device)
    
    # Generate samples using LatentDiffusion
    images = diffusion_model.sample(
        batch_size=num_samples,
        image=True,  # Decode to images
        cond=None,
        device=device
    )
    
    # Save images
    sample_path = exp_dir / 'samples' / f'epoch_{epoch+1}.png'
    save_image(images, sample_path, nrow=int(num_samples**0.5), normalize=True)
    print(f"Saved samples to {sample_path}", flush=True)
    
    diffusion_model.train()


@torch.no_grad()
def generate_samples_conditioned(diffusion_model, mixer, samples, exp_dir, epoch, device):
    """
    Generate conditioned samples and compare with targets using LatentDiffusion.
    
    Args:
        diffusion_model: LatentDiffusion model
        mixer: Condition mixer
        samples: List of sample dicts with 'pov', 'graph', 'layout' keys
        exp_dir: Experiment directory
        epoch: Current epoch number
        device: Device string
    
    Returns:
        MSE between generated and target latents
    """
    diffusion_model.eval()
    diffusion_model.autoencoder.eval()
    exp_dir = Path(exp_dir)
    
    num_samples = len(samples)
    
    # Prepare conditioning
    cond_povs = [s["pov"] for s in samples if s["pov"] is not None]
    cond_pov = torch.stack(cond_povs).to(device) if cond_povs else None
    cond_graph = torch.stack([s["graph"] for s in samples]).to(device)
    
    # Build mixed conditioning
    conds = [c for c in [cond_pov, cond_graph] if c is not None]
    cond = mixer(conds)
    
    # Target latents for comparison
    target_latents = torch.stack([s["layout"] for s in samples]).to(device)
    
    # Generate latents using LatentDiffusion
    latents = diffusion_model.sample(
        batch_size=num_samples,
        image=False,  # Get latents, not decoded images
        cond=cond,
        device=device
    )
    
    print(f"Sampled latent stats - min: {latents.min():.4f}, max: {latents.max():.4f}, "
          f"mean: {latents.mean():.4f}, std: {latents.std():.4f}", flush=True)
    
    # Compute MSE with target
    mse = F.mse_loss(latents, target_latents).item()
    print(f"MSE between generated and target latents: {mse:.6f}", flush=True)
    
    # Decode both generated and target
    pred_images = diffusion_model.autoencoder.decoder(latents)
    target_images = diffusion_model.autoencoder.decoder(target_latents)
    
    # Save side-by-side (pred on top, target on bottom)
    both = torch.cat([pred_images, target_images], dim=0)
    save_path = exp_dir / "samples" / f"epoch_{epoch+1:04d}_samples.png"
    save_image(both, save_path, nrow=num_samples, normalize=True)
    print(f"Saved samples with targets to {save_path}", flush=True)
    
    diffusion_model.train()
    return mse


================================================================================
FILE: training\training_utils.py
================================================================================

"""
Training utilities for diffusion model training.
Shared functions for experiment management, checkpointing, and visualization.
"""

import json
import shutil
import yaml
from pathlib import Path
from datetime import datetime
import torch
from tqdm import tqdm
import torch.nn.functional as F


# ============================================================================
# Configuration Management
# ============================================================================

def load_experiment_config(config_path):
    """Load experiment config from YAML"""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def save_experiment_config(config, exp_dir):
    """Save experiment config to experiment directory"""
    exp_dir = Path(exp_dir)
    config_path = exp_dir / 'experiment_config.yaml'
    with open(config_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False, sort_keys=False)
    print(f"Saved experiment config to {config_path}", flush=True)


def setup_experiment_dir(exp_dir, unet_config=None, resume=False):
    """
    Setup experiment directory structure.
    If resuming and dir exists, use existing.
    Otherwise, create new structure and copy configs.
    """
    exp_dir = Path(exp_dir)
    
    if exp_dir.exists() and not resume:
        raise ValueError(
            f"Experiment directory {exp_dir} already exists. "
            "Use --resume to continue training or choose a different path."
        )
    
    if exp_dir.exists() and resume:
        print(f"Resuming experiment from {exp_dir}", flush=True)
        return exp_dir
    
    # Create directory structure
    exp_dir.mkdir(parents=True, exist_ok=True)
    for subdir in ['checkpoints', 'samples', 'configs', 'logs']:
        (exp_dir / subdir).mkdir(exist_ok=True)
    
    # Copy U-Net config to experiment directory
    if unet_config:
        unet_config_path = Path(unet_config)
        shutil.copy(unet_config_path, exp_dir / 'configs' / unet_config_path.name)
    
    print(f"Created experiment directory: {exp_dir}", flush=True)
    return exp_dir


# ============================================================================
# Checkpoint Management
# ============================================================================

def save_checkpoint(exp_dir, epoch, state_dict, training_stats, 
                   val_loss, best_loss, is_best=False, save_periodic=False):
    """
    Save checkpoints (latest, best, and optional periodic).
    
    Args:
        exp_dir: Experiment directory
        epoch: Current epoch number
        state_dict: Dictionary with model states (unet, optimizer, etc.)
        training_stats: Training statistics dictionary
        val_loss: Current validation loss
        best_loss: Best validation loss so far
        is_best: Whether this is the best model
        save_periodic: Whether to save a periodic checkpoint
    """
    ckpt_dir = Path(exp_dir) / 'checkpoints'
    ckpt_dir.mkdir(parents=True, exist_ok=True)
    
    # Prepare checkpoint data
    checkpoint = {
        'epoch': epoch,
        'val_loss': val_loss,
        'best_loss': best_loss,
        'training_stats': training_stats,
        **state_dict
    }
    
    # Save latest checkpoint (full)
    latest_path = ckpt_dir / 'latest.pt'
    torch.save(checkpoint, latest_path)
    
    # Save best checkpoint
    if is_best:
        best_path = ckpt_dir / 'best.pt'
        torch.save(checkpoint, best_path)
        print(f"Saved best checkpoint with loss: {val_loss:.6f}", flush=True)
    
    # Save periodic lightweight checkpoint (model only)
    if save_periodic:
        periodic_checkpoint = {
            'epoch': epoch,
            'val_loss': val_loss,
            **{k: v for k, v in state_dict.items() if 'state_dict' in k or k in ['unet', 'mixer']}
        }
        periodic_path = ckpt_dir / f'epoch_{epoch+1}.pt'
        torch.save(periodic_checkpoint, periodic_path)
        print(f"Saved periodic checkpoint at epoch {epoch+1}", flush=True)


def load_checkpoint(checkpoint_path, models_dict, optimizer=None, scheduler_lr=None):
    """
    Load checkpoint and restore model states.
    
    Args:
        checkpoint_path: Path to checkpoint file
        models_dict: Dict mapping names to model objects (e.g., {'unet': unet, 'mixer': mixer})
        optimizer: Optimizer to restore (optional)
        scheduler_lr: LR scheduler to restore (optional)
    
    Returns:
        Tuple of (start_epoch, best_loss, training_stats)
    """
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # Load model states
    for name, model in models_dict.items():
        if name in checkpoint:
            model.load_state_dict(checkpoint[name])
        elif f'{name}_state_dict' in checkpoint:
            model.load_state_dict(checkpoint[f'{name}_state_dict'])
    
    # Load optimizer state
    if optimizer and 'opt' in checkpoint:
        optimizer.load_state_dict(checkpoint['opt'])
    elif optimizer and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    
    # Load scheduler state
    if scheduler_lr and 'sched' in checkpoint:
        if checkpoint['sched'] is not None:
            scheduler_lr.load_state_dict(checkpoint['sched'])
    elif scheduler_lr and 'scheduler_state_dict' in checkpoint:
        if checkpoint['scheduler_state_dict'] is not None:
            scheduler_lr.load_state_dict(checkpoint['scheduler_state_dict'])
    
    epoch = checkpoint['epoch']
    best_loss = checkpoint.get('best_loss', float('inf'))
    training_stats = checkpoint.get('training_stats', {
        'epochs': [], 'train_loss': [], 'val_loss': [],
        'learning_rate': [], 'timestamps': []
    })
    
    print(f"Loaded checkpoint from epoch {epoch}, best_loss: {best_loss:.6f}", flush=True)
    return epoch, best_loss, training_stats


# ============================================================================
# Training Statistics
# ============================================================================

def save_training_stats(exp_dir, training_stats):
    """Save training statistics to JSON and generate plots"""
    exp_dir = Path(exp_dir)
    stats_path = exp_dir / 'logs' / 'training_stats.json'
    
    # Save JSON
    with open(stats_path, 'w') as f:
        json.dump(training_stats, f, indent=2)
    
    # Generate plots if matplotlib available
    try:
        import matplotlib.pyplot as plt
        
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))
        
        # Loss plot
        if 'train_loss' in training_stats and training_stats['train_loss']:
            axes[0].plot(training_stats['epochs'], training_stats['train_loss'], 
                        label='Train Loss', marker='o', markersize=3)
        if 'val_loss' in training_stats and training_stats['val_loss']:
            axes[0].plot(training_stats['epochs'], training_stats['val_loss'], 
                        label='Val Loss', marker='s', markersize=3)
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Loss')
        axes[0].set_title('Training and Validation Loss')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Learning rate plot
        if 'learning_rate' in training_stats and training_stats['learning_rate']:
            axes[1].plot(training_stats['epochs'], training_stats['learning_rate'], 
                        label='Learning Rate', color='orange', marker='o', markersize=3)
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('Learning Rate')
            axes[1].set_title('Learning Rate Schedule')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plot_path = exp_dir / 'logs' / 'training_curves.png'
        plt.savefig(plot_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"Saved training plots to {plot_path}", flush=True)
    except ImportError:
        print("Matplotlib not available, skipping plots", flush=True)


def init_training_stats():
    """Initialize empty training statistics dictionary"""
    return {
        'epochs': [],
        'train_loss': [],
        'val_loss': [],
        'learning_rate': [],
        'timestamps': []
    }


def update_training_stats(training_stats, epoch, train_loss, val_loss, learning_rate):
    """Update training statistics with current epoch data"""
    training_stats['epochs'].append(epoch + 1)
    training_stats['train_loss'].append(train_loss)
    training_stats['val_loss'].append(val_loss)
    training_stats['learning_rate'].append(learning_rate)
    training_stats['timestamps'].append(datetime.now().isoformat())
    return training_stats


# ============================================================================
# Training Loop Helpers
# ============================================================================

def train_epoch_unconditioned(unet, scheduler, dataloader, optimizer, device, epoch):
    """Train for one epoch without conditioning"""
    unet.train()
    total_loss = 0
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
    for batch in pbar:
        if batch is None:
            continue
        
        latents = batch['layout'].to(device)
        B = latents.shape[0]
        
        # Add noise
        t = torch.randint(0, scheduler.num_steps, (B,), device=device)
        noise = torch.randn_like(latents)
        noisy_latents, _ = scheduler.add_noise(latents, t, noise)
        
        # Predict noise
        noise_pred = unet(noisy_latents, t, cond=None)
        loss = F.mse_loss(noise_pred, noise)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(unet.parameters(), max_norm=1.0)
        optimizer.step()
        
        total_loss += loss.item()
        pbar.set_postfix({'loss': loss.item()})
    
    return total_loss / len(dataloader)


def train_epoch_conditioned(unet, scheduler, mixer, dataloader, optimizer, device, epoch):
    """Train for one epoch with conditioning"""
    unet.train()
    total_loss = 0
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}")
    for batch in pbar:
        latents = batch["layout"].to(device)
        cond_pov = batch["pov"].to(device) if batch["pov"] is not None else None
        cond_graph = batch["graph"].to(device)
        
        B = latents.size(0)
        t = torch.randint(0, scheduler.num_steps, (B,), device=device)
        noise = torch.randn_like(latents)
        noisy_latents, _ = scheduler.add_noise(latents, t, noise)
        
        # Build conditioning
        conds = [c for c in [cond_pov, cond_graph] if c is not None]
        cond = mixer(conds)
        
        # Predict noise with conditioning
        noise_pred = unet(noisy_latents, t, cond=cond)
        loss = F.mse_loss(noise_pred, noise)
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)
        optimizer.step()
        
        total_loss += loss.item()
        pbar.set_postfix({'loss': loss.item()})
    
    return total_loss / len(dataloader)


@torch.no_grad()
def validate_unconditioned(unet, scheduler, dataloader, device):
    """Compute validation loss without conditioning"""
    unet.eval()
    total_loss = 0
    
    for batch in dataloader:
        if batch is None:
            continue
        
        latents = batch['layout'].to(device)
        B = latents.shape[0]
        
        t = torch.randint(0, scheduler.num_steps, (B,), device=device)
        noise = torch.randn_like(latents)
        noisy_latents, _ = scheduler.add_noise(latents, t, noise)
        
        noise_pred = unet(noisy_latents, t, cond=None)
        loss = F.mse_loss(noise_pred, noise)
        total_loss += loss.item()
    
    unet.train()
    return total_loss / len(dataloader)


@torch.no_grad()
def validate_conditioned(unet, scheduler, mixer, dataloader, device):
    """Compute validation loss with conditioning"""
    unet.eval()
    total_loss = 0
    
    for batch in dataloader:
        latents = batch["layout"].to(device)
        cond_pov = batch["pov"].to(device) if batch["pov"] is not None else None
        cond_graph = batch["graph"].to(device)
        
        B = latents.size(0)
        t = torch.randint(0, scheduler.num_steps, (B,), device=device)
        noise = torch.randn_like(latents)
        noisy_latents, _ = scheduler.add_noise(latents, t, noise)
        
        # Build conditioning
        conds = [c for c in [cond_pov, cond_graph] if c is not None]
        cond = mixer(conds)
        
        noise_pred = unet(noisy_latents, t, cond=cond)
        loss = F.mse_loss(noise_pred, noise)
        total_loss += loss.item()
    
    unet.train()
    return total_loss / len(dataloader)


# ============================================================================
# Dataset Utilities
# ============================================================================

def save_split_files(exp_dir, train_df, val_df):
    """Save train/val split information to CSV files"""
    exp_dir = Path(exp_dir)
    train_file = exp_dir / "trained_on.csv"
    val_file = exp_dir / "evaluated_on.csv"
    train_df.to_csv(train_file, index=False)
    val_df.to_csv(val_file, index=False)
    print(f"Saved split files: {train_file}, {val_file}", flush=True)


================================================================================
FILE: training\train_autoencoder.py
================================================================================

from __future__ import annotations
import sys, os
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))
import argparse
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, utils
from datetime import datetime
import pandas as pd
import time
from tqdm import tqdm
import numpy as np

from modules.datasets import LayoutDataset
from modules.autoencoder import AutoEncoder


def print_separator(char="-", length=80):
    print(char * length)


def get_loss(name: str):
    name = name.lower()
    print(f"[LOSS] Initializing loss function: {name}")
    if name == "mse":
        return nn.MSELoss()
    if name == "l1":
        return nn.L1Loss()
    if name == "huber":
        return nn.SmoothL1Loss()
    if name == "bce":
        return nn.BCEWithLogitsLoss()
    raise ValueError(f"Unknown loss {name}")


def count_parameters(model):
    """Count trainable parameters"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def get_model_size(model):
    """Get model size in MB"""
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    size_mb = (param_size + buffer_size) / 1024 / 1024
    return size_mb


def evaluate(model, loader, criterion, device):
    """Evaluate model on validation set"""
    model.eval()
    total_loss = 0.0
    total_samples = 0
    batch_losses = []
    
    with torch.no_grad():
        for batch in loader:
            if batch["layout"] is None:
                continue
            
            imgs = batch["layout"].to(device)
            batch_size = imgs.size(0)
            
            recon = model(imgs)
            loss = criterion(recon, imgs)
            
            batch_loss = loss.item()
            batch_losses.append(batch_loss)
            total_loss += batch_loss * batch_size
            total_samples += batch_size
    
    if total_samples == 0:
        return float('inf'), [], 0
    
    avg_loss = total_loss / total_samples
    return avg_loss, batch_losses, total_samples


def train(cfg, args):
    print_separator("=")
    print("AUTOENCODER TRAINING SESSION")
    print_separator("=")
    print(f"[TIME] Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # ----- Device -----
    print_separator()
    print("[DEVICE] Setting up compute device...")
    if args.device == "auto":
        if torch.cuda.is_available():
            device = "cuda"
            print(f"  âœ“ CUDA available - using GPU")
            print(f"    GPU Name: {torch.cuda.get_device_name(0)}")
            print(f"    GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        elif torch.backends.mps.is_available():
            device = "mps"
            print(f"  âœ“ MPS available - using Apple Silicon GPU")
        else:
            device = "cpu"
            print(f"  âš  No GPU available - using CPU")
    else:
        device = args.device
        print(f"  â†’ Manually specified device: {device}")
    print(f"[DEVICE] Using device: {device}")
    print()

    # ----- Model -----
    print_separator()
    print("[MODEL] Building AutoEncoder from config...")
    print(f"  Config file: {args.config}")
    
    start_time = time.time()
    ae = AutoEncoder.from_config(cfg).to(device)
    load_time = time.time() - start_time
    
    print(f"  âœ“ Model built successfully in {load_time:.2f} seconds")
    print(f"\n[MODEL] Architecture Summary:")
    print(f"  â†’ Total parameters: {count_parameters(ae):,}")
    print(f"  â†’ Model size: {get_model_size(ae):.2f} MB")
    print(f"  â†’ Latent dimension: {cfg['encoder']['latent_dim']}")
    print(f"  â†’ Input image size: {cfg['encoder']['image_size']}x{cfg['encoder']['image_size']}")
    print(f"  â†’ Input channels: {cfg['encoder']['in_channels']}")
    print(f"  â†’ Output channels: {cfg['decoder']['out_channels']}")
    
    # Print encoder architecture
    print(f"\n  Encoder layers:")
    for i, layer_cfg in enumerate(cfg['encoder']['layers']):
        print(f"    Layer {i+1}: {layer_cfg.get('out_channels')} channels, "
              f"stride={layer_cfg.get('stride', 1)}, "
              f"norm={layer_cfg.get('norm', 'none')}, "
              f"act={layer_cfg.get('act', 'none')}")
    print()

    # ----- Dataset -----
    print_separator()
    print("[DATASET] Loading dataset...")
    print(f"  Manifest: {args.layout_manifest}")
    print(f"  Mode: {args.layout_mode}")
    print(f"  Keep empty: {args.keep_empty}")
    print(f"  Return embeddings: False")
    print(f"  Train/Eval split: {args.train_split:.2%} / {1-args.train_split:.2%}")
    
    # ----- Transform -----
    print(f"\n[TRANSFORM] Setting up data transformations...")
    print(f"  â†’ Resize to: {args.resize}x{args.resize}")
    print(f"  â†’ Convert to tensor")
    transform = transforms.Compose([
        transforms.Resize((args.resize, args.resize)),
        transforms.ToTensor()
    ])
    
    print(f"\n[DATASET] Initializing LayoutDataset...")
    full_dataset = LayoutDataset(
        args.layout_manifest,
        transform=transform,
        mode=args.layout_mode,
        skip_empty=not args.keep_empty,
        return_embeddings=False
    )
    print(f"  âœ“ Full dataset loaded with {len(full_dataset)} samples")
    
    # Split dataset into train and eval
    print(f"\n[SPLIT] Splitting dataset...")
    total_size = len(full_dataset)
    train_size = int(args.train_split * total_size)
    eval_size = total_size - train_size
    
    # Set random seed for reproducibility
    if args.seed is not None:
        torch.manual_seed(args.seed)
        print(f"  Random seed: {args.seed}")
    
    train_dataset, eval_dataset = torch.utils.data.random_split(
        full_dataset, [train_size, eval_size]
    )
    
    print(f"  âœ“ Training set: {len(train_dataset)} samples ({len(train_dataset)/total_size:.1%})")
    print(f"  âœ“ Evaluation set: {len(eval_dataset)} samples ({len(eval_dataset)/total_size:.1%})")
    
    print(f"\n[DATALOADER] Creating DataLoaders...")
    print(f"  Batch size: {args.batch_size}")
    print(f"  Training batches: {len(train_dataset) // args.batch_size}")
    print(f"  Evaluation batches: {len(eval_dataset) // args.batch_size}")
    
    train_loader = torch.utils.data.DataLoader(
        train_dataset, batch_size=args.batch_size, shuffle=True
    )
    eval_loader = torch.utils.data.DataLoader(
        eval_dataset, batch_size=args.batch_size, shuffle=False
    )
    print(f"  âœ“ DataLoaders ready")
    print()

    # ----- Training setup -----
    print_separator()
    print("[TRAINING] Setting up training components...")
    
    criterion = get_loss(args.loss)
    print(f"  âœ“ Loss function: {args.loss}")
    
    print(f"\n[OPTIMIZER] Initializing Adam optimizer...")
    print(f"  Learning rate: {args.lr}")
    optimizer = optim.Adam(ae.parameters(), lr=args.lr)
    print(f"  âœ“ Optimizer ready")
    print()

    # ----- Run directory -----
    print_separator()
    print("[OUTPUT] Setting up output directory...")
    exp_id = datetime.now().strftime("%Y%m%d-%H%M%S")
    run_dir = os.path.join(args.outdir, f"{exp_id}_{args.name}")
    
    # Make sure we create a unique directory if it somehow exists
    original_run_dir = run_dir
    counter = 1
    while os.path.exists(run_dir):
        run_dir = f"{original_run_dir}_v{counter}"
        counter += 1
    
    os.makedirs(run_dir, exist_ok=True)
    print(f"  Output directory: {run_dir}")
    print(f"  Experiment ID: {exp_id}")
    print(f"  Job name: {args.name}")
    
    # Log structure that will be created
    print(f"\n[DIRECTORY STRUCTURE]")
    print(f"  {run_dir}/")
    print(f"    â”œâ”€â”€ config_used.yml    (training configuration)")
    print(f"    â”œâ”€â”€ metrics.csv        (loss metrics)")
    print(f"    â”œâ”€â”€ best.pt           (best model checkpoint)")
    print(f"    â”œâ”€â”€ last.pt           (final model checkpoint)")
    if args.save_checkpoint_every > 0 and not args.keep_only_best:
        print(f"    â”œâ”€â”€ epoch*.pt         (periodic checkpoints)")
    if args.save_images:
        print(f"    â”œâ”€â”€ inputs_epoch*.png  (input samples)")
        print(f"    â””â”€â”€ recons_epoch*.png  (reconstructions)")
    
    # save config + CLI args
    print(f"\n[CONFIG] Saving configuration...")
    full_cfg = {"model_cfg": cfg, "cli_args": vars(args)}
    config_path = os.path.join(run_dir, "config_used.yml")
    with open(config_path, "w") as f:
        yaml.safe_dump(full_cfg, f)
    print(f"  âœ“ Config saved to: {config_path}")
    
    # metrics log
    metrics_path = os.path.join(run_dir, "metrics.csv")
    metrics = []
    print(f"  âœ“ Metrics will be saved to: {metrics_path}")
    
    # Image saving info
    if args.save_images:
        print(f"\n[IMAGES] Image saving enabled")
        print(f"  Save every: {args.save_every} epochs")
    else:
        print(f"\n[IMAGES] Image saving disabled (use --save_images to enable)")
    
    # Checkpoint saving info
    print(f"\n[CHECKPOINT POLICY]")
    print(f"  â†’ Best model selection: Based on evaluation loss")
    if args.keep_only_best:
        print(f"  â†’ Only keeping best.pt and last.pt")
    elif args.save_checkpoint_every > 0:
        print(f"  â†’ Saving checkpoints every {args.save_checkpoint_every} epochs")
        print(f"  â†’ Always saving best.pt and last.pt")
    else:
        print(f"  â†’ Only saving best.pt and last.pt (no periodic checkpoints)")
    print()

    # ----- Training loop -----
    print_separator("=")
    print("STARTING TRAINING")
    print_separator("=")
    print(f"[TRAINING] Training for {args.epochs} epochs")
    print()
    
    best_eval_loss = float("inf")
    total_start_time = time.time()
    
    for epoch in range(args.epochs):
        epoch_start_time = time.time()
        print(f"[EPOCH {epoch+1}/{args.epochs}] " + "="*50)
        
        # ===== TRAINING PHASE =====
        ae.train()
        total_train_loss = 0.0
        train_batch_losses = []
        num_skipped = 0
        
        # Progress bar for batches
        pbar = tqdm(train_loader, desc=f"Training {epoch+1}", unit="batch", 
                    bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]')
        
        for batch_idx, batch in enumerate(pbar):
            if batch["layout"] is None:
                num_skipped += 1
                continue
                
            imgs = batch["layout"].to(device)
            batch_size = imgs.size(0)
            
            # Forward pass
            optimizer.zero_grad()
            recon = ae(imgs)
            loss = criterion(recon, imgs)
            
            # Backward pass
            loss.backward()
            optimizer.step()
            
            # Track loss
            batch_loss = loss.item()
            train_batch_losses.append(batch_loss)
            total_train_loss += batch_loss * batch_size
            
            # Update progress bar
            pbar.set_postfix({'loss': f'{batch_loss:.4f}', 
                             'avg_loss': f'{np.mean(train_batch_losses):.4f}'})
            
            # Verbose batch logging every 10 batches
            if (batch_idx + 1) % 10 == 0:
                print(f"    Batch {batch_idx+1}/{len(train_loader)}: "
                      f"Loss={batch_loss:.4f}, "
                      f"Running avg={np.mean(train_batch_losses):.4f}")
        
        pbar.close()
        
        # Calculate training statistics
        avg_train_loss = total_train_loss / len(train_dataset)
        
        print(f"\n[TRAINING PHASE COMPLETE]")
        print(f"  â†’ Average Train Loss: {avg_train_loss:.6f}")
        print(f"  â†’ Min batch loss: {min(train_batch_losses):.6f}")
        print(f"  â†’ Max batch loss: {max(train_batch_losses):.6f}")
        print(f"  â†’ Std batch loss: {np.std(train_batch_losses):.6f}")
        if num_skipped > 0:
            print(f"  âš  Skipped {num_skipped} empty batches")
        
        # ===== EVALUATION PHASE =====
        print(f"\n[EVALUATION PHASE]")
        print(f"  Evaluating on {len(eval_dataset)} samples...")
        
        eval_start_time = time.time()
        avg_eval_loss, eval_batch_losses, eval_samples = evaluate(
            ae, eval_loader, criterion, device
        )
        eval_time = time.time() - eval_start_time
        
        if eval_samples > 0:
            print(f"  â†’ Average Eval Loss: {avg_eval_loss:.6f}")
            print(f"  â†’ Min batch loss: {min(eval_batch_losses):.6f}")
            print(f"  â†’ Max batch loss: {max(eval_batch_losses):.6f}")
            print(f"  â†’ Std batch loss: {np.std(eval_batch_losses):.6f}")
            print(f"  â†’ Evaluation time: {eval_time:.2f} seconds")
        else:
            print(f"  âš  No valid evaluation samples!")
            avg_eval_loss = float('inf')
        
        # Epoch statistics
        epoch_time = time.time() - epoch_start_time
        
        print(f"\n[EPOCH {epoch+1} COMPLETE]")
        print(f"  â†’ Epoch time: {epoch_time:.2f} seconds")
        print(f"  â†’ Training samples/second: {len(train_dataset)/(epoch_time-eval_time):.2f}")
        
        # Check if best model (based on eval loss)
        is_best = avg_eval_loss < best_eval_loss
        if is_best:
            improvement = best_eval_loss - avg_eval_loss
            print(f"  â˜… NEW BEST MODEL! Eval loss improved by {improvement:.6f}")
            print(f"    (previous best: {best_eval_loss:.6f}, new best: {avg_eval_loss:.6f})")
            best_eval_loss = avg_eval_loss
        else:
            diff = avg_eval_loss - best_eval_loss
            print(f"  â†’ Not best model (eval loss {diff:.6f} above best)")
        
        # Save metrics
        metrics.append({
            "epoch": epoch+1, 
            "train_loss": avg_train_loss,
            "eval_loss": avg_eval_loss,
            "train_min_loss": min(train_batch_losses),
            "train_max_loss": max(train_batch_losses),
            "train_std_loss": np.std(train_batch_losses),
            "eval_min_loss": min(eval_batch_losses) if eval_batch_losses else None,
            "eval_max_loss": max(eval_batch_losses) if eval_batch_losses else None,
            "eval_std_loss": np.std(eval_batch_losses) if eval_batch_losses else None,
            "epoch_time": epoch_time,
            "eval_time": eval_time
        })
        pd.DataFrame(metrics).to_csv(metrics_path, index=False)
        print(f"  âœ“ Metrics saved")
        
        # Save checkpoints
        print(f"\n[CHECKPOINTS]")
        
        # Always save last checkpoint
        torch.save(ae.state_dict(), os.path.join(run_dir, "last.pt"))
        print(f"  âœ“ Updated last.pt")
        
        # Save best checkpoint if this is the best epoch
        if is_best:
            torch.save(ae.state_dict(), os.path.join(run_dir, "best.pt"))
            print(f"  âœ“ Updated best.pt (eval_loss={best_eval_loss:.6f})")
        
        # Save numbered checkpoint only if specified
        if args.save_checkpoint_every > 0 and (epoch+1) % args.save_checkpoint_every == 0:
            if not args.keep_only_best:
                ckpt_path = os.path.join(run_dir, f"epoch{epoch+1}.pt")
                torch.save(ae.state_dict(), ckpt_path)
                print(f"  âœ“ Saved epoch checkpoint: epoch{epoch+1}.pt")
            else:
                print(f"  â†’ Skipped epoch checkpoint (keep_only_best=True)")
        
        # Save sample images
        if args.save_images and (epoch+1) % args.save_every == 0:
            print(f"\n[IMAGES] Saving sample reconstructions...")
            
            with torch.no_grad():
                ae.eval()
                
                # Get a fresh batch for visualization from eval set
                sample_batch = next(iter(eval_loader))
                while sample_batch["layout"] is None:
                    sample_batch = next(iter(eval_loader))
                
                sample_imgs = sample_batch["layout"].to(device)[:8]
                sample_recon = ae(sample_imgs)
                
                # Create side-by-side comparison: top row = input, bottom row = reconstruction
                comparison = torch.cat([sample_imgs[:4], sample_recon[:4]], dim=0)
                comparison_grid = utils.make_grid(comparison, nrow=4, normalize=False, value_range=(0, 1))
                
                # Also save separate grids
                imgs_grid = utils.make_grid(sample_imgs, nrow=4, normalize=False, value_range=(0, 1))
                recons_grid = utils.make_grid(sample_recon, nrow=4, normalize=False, value_range=(0, 1))
                
                # Save all versions
                comparison_path = os.path.join(run_dir, f"comparison_epoch{epoch+1}.png")
                input_path = os.path.join(run_dir, f"inputs_epoch{epoch+1}.png")
                recon_path = os.path.join(run_dir, f"recons_epoch{epoch+1}.png")
                
                utils.save_image(comparison_grid, comparison_path)
                utils.save_image(imgs_grid, input_path)
                utils.save_image(recons_grid, recon_path)
                
                print(f"  âœ“ Saved comparison: {comparison_path} (top=input, bottom=output)")
                print(f"  âœ“ Saved input samples: {input_path}")
                print(f"  âœ“ Saved reconstructions: {recon_path}")
                
                # Calculate reconstruction metrics
                mse = nn.MSELoss()(sample_recon, sample_imgs).item()
                print(f"  â†’ Sample reconstruction MSE: {mse:.6f}")
        
        # Estimated time remaining
        elapsed_time = time.time() - total_start_time
        avg_epoch_time = elapsed_time / (epoch + 1)
        remaining_epochs = args.epochs - (epoch + 1)
        eta = avg_epoch_time * remaining_epochs
        
        print(f"\n[PROGRESS] {epoch+1}/{args.epochs} epochs complete")
        print(f"  Total elapsed: {elapsed_time/60:.2f} minutes")
        print(f"  ETA: {eta/60:.2f} minutes")
        print()
    
    # ----- Training complete -----
    total_time = time.time() - total_start_time
    
    print_separator("=")
    print("TRAINING COMPLETE")
    print_separator("=")
    print(f"[SUMMARY]")
    print(f"  Total training time: {total_time/60:.2f} minutes")
    print(f"  Average epoch time: {total_time/args.epochs:.2f} seconds")
    print(f"  Final train loss: {avg_train_loss:.6f}")
    print(f"  Final eval loss: {avg_eval_loss:.6f}")
    print(f"  Best eval loss: {best_eval_loss:.6f}")
    print(f"  Results directory: {run_dir}")
    print()
    print(f"[FILES SAVED]")
    
    # Count actual checkpoint files
    checkpoint_count = 2  # best.pt and last.pt
    if args.save_checkpoint_every > 0 and not args.keep_only_best:
        checkpoint_count += (args.epochs // args.save_checkpoint_every)
    
    print(f"  âœ“ Model checkpoints: best.pt, last.pt")
    if checkpoint_count > 2:
        print(f"    + {checkpoint_count - 2} periodic checkpoint(s)")
    print(f"  âœ“ Configuration: config_used.yml")
    print(f"  âœ“ Training metrics: metrics.csv")
    
    if args.save_images:
        num_img_saves = (args.epochs // args.save_every)
        print(f"  âœ“ Sample images: {num_img_saves} sets of reconstructions")
    print()
    print(f"[END] Training session finished at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print_separator("=")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train an AutoEncoder model with detailed logging")
    
    # manifests
    parser.add_argument("--layout_manifest", type=str, required=True,
                        help="Path to the layout manifest CSV file")
    
    # config + job name
    parser.add_argument("--config", type=str, required=True,
                        help="Path to the model configuration YAML file")
    parser.add_argument("--name", type=str, default="job",
                        help="Name for this training job (default: job)")
    
    # training params
    parser.add_argument("--batch_size", type=int, default=32,
                        help="Batch size for training (default: 32)")
    parser.add_argument("--epochs", type=int, default=2,
                        help="Number of training epochs (default: 2)")
    parser.add_argument("--lr", type=float, default=1e-3,
                        help="Learning rate for Adam optimizer (default: 1e-3)")
    parser.add_argument("--resize", type=int, default=512,
                        help="Size to resize images to (default: 512)")
    parser.add_argument("--loss", type=str, default="mse",
                        choices=["mse", "l1", "huber", "bce"],
                        help="Loss function to use (default: mse)")
    
    # dataset split
    parser.add_argument("--train_split", type=float, default=0.8,
                        help="Fraction of dataset to use for training (default: 0.8)")
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed for dataset split (default: 42)")
    
    # checkpoint params
    parser.add_argument("--save_checkpoint_every", type=int, default=0,
                        help="Save checkpoint every N epochs (0 = only save best and last)")
    parser.add_argument("--keep_only_best", action="store_true",
                        help="Only keep best.pt and last.pt checkpoints")
    
    # dataset
    parser.add_argument("--layout_mode", type=str, default="all",
                        help="Layout mode for dataset (default: all)")
    parser.add_argument("--keep_empty", action="store_true",
                        help="Keep empty layouts in dataset")
    
    # device + output
    parser.add_argument("--device", type=str, default="auto",
                        choices=["auto", "cuda", "cpu", "mps"],
                        help="Device to train on (default: auto)")
    parser.add_argument("--outdir", type=str, default="runs",
                        help="Output directory for training runs (default: runs)")
    parser.add_argument("--save_images", action="store_true", default=True,
                        help="Save sample reconstruction images during training (default: True)")
    parser.add_argument("--save_every", type=int, default=1,
                        help="Save images every N epochs (default: 1)")
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("AUTOENCODER TRAINING LAUNCHER")
    print("="*80)
    print("\n[ARGUMENTS] Parsed command line arguments:")
    for arg, value in vars(args).items():
        print(f"  {arg}: {value}")
    print()
    
    print("[CONFIG] Loading model configuration...")
    with open(args.config, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)
    print("==========================================")
    print("Loaded AutoEncoder configuration:")
    print(yaml.dump(cfg, sort_keys=False, indent=2))

    if 'latent_channels' in cfg['encoder']:
        print(f"  â†’ Latent channels: {cfg['encoder']['latent_channels']}")
    if 'latent_base' in cfg['encoder']:
        print(f"  â†’ Latent base size: {cfg['encoder']['latent_base']}x{cfg['encoder']['latent_base']}")
    print("==========================================")

    print("  âœ“ Configuration loaded successfully")
    print()
    
    # Start training
    train(cfg, args)


================================================================================
FILE: training\train_base_diffusion.py
================================================================================

"""
Training script for conditioned latent diffusion model.
Refactored version using modular utilities.
"""

import argparse
import random
from pathlib import Path
import yaml
import torch
from torch.utils.data import DataLoader, random_split
from torch.optim import Adam
from torch.optim.lr_scheduler import CosineAnnealingLR

from modules.unet import UNet
from modules.scheduler import *
from modules.autoencoder import AutoEncoder
from modules.unified_dataset import UnifiedLayoutDataset
from modules.condition_mixer import ConcatMixer, WeightedMixer, LearnedWeightedMixer
from modules.diffusion import LatentDiffusion

# Import utilities
from training_utils import (
    load_experiment_config, save_experiment_config, setup_experiment_dir,
    save_checkpoint, load_checkpoint, save_training_stats, init_training_stats,
    update_training_stats, train_epoch_conditioned, validate_conditioned,
    save_split_files
)
from sampling_utils import generate_samples_conditioned


def collate_fn(batch):
    """Handle batches where some samples have None POV"""
    return {
        'scene_id': [b['scene_id'] for b in batch],
        'room_id': [b['room_id'] for b in batch],
        'pov': torch.stack([b['pov'] for b in batch if b['pov'] is not None]) if any(b['pov'] is not None for b in batch) else None,
        'graph': torch.stack([b['graph'] for b in batch]),
        'layout': torch.stack([b['layout'] for b in batch]),
    }


def main():
    parser = argparse.ArgumentParser(description='Train Conditioned Latent Diffusion Model')
    parser.add_argument("--exp_config", required=True, help="Path to experiment config YAML")
    parser.add_argument("--room_manifest", required=True, help="Path to room manifest")
    parser.add_argument("--scene_manifest", required=True, help="Path to scene manifest")
    parser.add_argument("--pov_type", default=None, help="POV type (e.g., 'rendered', 'semantic')")
    parser.add_argument("--mixer_type", choices=["concat", "weighted", "learned"], 
                        default=None, help="Override mixer type from config")
    parser.add_argument("--resume", action="store_true", help="Resume training")
    args = parser.parse_args()

    # Load and setup experiment
    config = load_experiment_config(args.exp_config)
    exp_dir = setup_experiment_dir(
        config["experiment"]["exp_dir"], 
        config["unet"]["config_path"], 
        args.resume
    )
    
    if not args.resume:
        save_experiment_config(config, exp_dir)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}", flush=True)

    # ========================================================================
    # Load Models
    # ========================================================================
    
    print(f"Loading U-Net from {config['unet']['config_path']}", flush=True)
    unet = UNet.from_config(config["unet"]["config_path"]).to(device)
    
    print(f"Loading {config['scheduler']['type']} with {config['scheduler']['num_steps']} steps", flush=True)
    scheduler_class = globals()[config["scheduler"]["type"]]
    scheduler = scheduler_class(num_steps=config["scheduler"]["num_steps"]).to(device)
    
    print(f"Loading Autoencoder from {config['autoencoder']['config_path']}", flush=True)
    autoencoder = AutoEncoder.from_config(config["autoencoder"]["config_path"]).to(device)
    if config["autoencoder"]["checkpoint_path"]:
        autoencoder.load_state_dict(
            torch.load(config["autoencoder"]["checkpoint_path"], map_location=device)
        )
    autoencoder.eval()

    # Create LatentDiffusion wrapper
    latent_shape = tuple(config["latent_shape"])
    diffusion_model = LatentDiffusion(
        unet=unet,
        scheduler=scheduler,
        autoencoder=autoencoder,
        latent_shape=latent_shape
    ).to(device)

    # ========================================================================
    # Load Dataset
    # ========================================================================
    
    print(f"Loading unified dataset...", flush=True)
    dataset = UnifiedLayoutDataset(
        args.room_manifest, 
        args.scene_manifest, 
        use_embeddings=True,
        pov_type=args.pov_type
    )
            
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    print(f"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}", flush=True)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config["training"]["batch_size"], 
        shuffle=True, 
        num_workers=4,
        collate_fn=collate_fn
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config["training"]["batch_size"], 
        shuffle=False, 
        num_workers=4,
        collate_fn=collate_fn
    )

    # Save split info
    train_df = dataset.df.iloc[train_dataset.indices]
    val_df = dataset.df.iloc[val_dataset.indices]
    save_split_files(exp_dir, train_df, val_df)

    # ========================================================================
    # Create Mixer
    # ========================================================================
    
    latent_hw = tuple(config["latent_shape"][-2:])  # (H, W)

    # Get UNet conditioning channels from config
    with open(config["unet"]["config_path"], 'r') as f:
        unet_cfg = yaml.safe_load(f)
    cond_channels = unet_cfg["unet"]["cond_channels"]

    # Get embedding dimensions from config
    pov_dim = config["mixer"]["pov_dim"]
    graph_dim = config["mixer"]["graph_dim"]
    
    print(f"Input dimensions - POV: {pov_dim}, Graph: {graph_dim}", flush=True)

    # Determine mixer type (CLI overrides config)
    mixer_type = config["mixer"].get("type", "concat")
    if args.mixer_type is not None:
        mixer_type = args.mixer_type

    print(f"Creating {mixer_type} mixer (out_channels={cond_channels}, target_size={latent_hw})...", flush=True)

    if mixer_type == "concat":
        mixer = ConcatMixer(
            out_channels=cond_channels, 
            target_size=latent_hw,
            pov_channels=pov_dim,
            graph_channels=graph_dim
        ).to(device)
    elif mixer_type == "weighted":
        mixer = WeightedMixer(
            out_channels=cond_channels, 
            target_size=latent_hw,
            pov_channels=pov_dim,
            graph_channels=graph_dim
        ).to(device)
    else:  # learned
        mixer = LearnedWeightedMixer(
            out_channels=cond_channels, 
            target_size=latent_hw,
            pov_channels=pov_dim,
            graph_channels=graph_dim
        ).to(device)

    # Freeze mixer weights (if not learned)
    if mixer_type != "learned":
        for param in mixer.parameters():
            param.requires_grad = False

    # ========================================================================
    # Optimizer and Scheduler
    # ========================================================================
    
    optimizer = Adam(unet.parameters(), lr=config["training"]["learning_rate"])
    scheduler_lr = CosineAnnealingLR(optimizer, T_max=config["training"]["num_epochs"])

    # ========================================================================
    # Fixed samples for visual tracking
    # ========================================================================
    
    num_fixed = min(8, len(val_dataset))
    fixed_indices = random.sample(range(len(val_dataset)), num_fixed)
    fixed_samples = [val_dataset[i] for i in fixed_indices]
    torch.save(fixed_indices, exp_dir / "fixed_indices.pt")

    # ========================================================================
    # Resume from checkpoint
    # ========================================================================
    
    start_epoch = 0
    best_loss = float("inf")
    training_stats = init_training_stats()
    
    latest_checkpoint = exp_dir / 'checkpoints' / 'latest.pt'
    if args.resume and latest_checkpoint.exists():
        start_epoch, best_loss, training_stats = load_checkpoint(
            latest_checkpoint,
            models_dict={'unet': unet, 'mixer': mixer},
            optimizer=optimizer,
            scheduler_lr=scheduler_lr
        )
        start_epoch += 1

    # ========================================================================
    # Training Loop
    # ========================================================================
    
    print(f"\nStarting training from epoch {start_epoch+1} to {config['training']['num_epochs']}")
    print(f"Batch size: {config['training']['batch_size']}, Learning rate: {config['training']['learning_rate']}")
    print(f"Batches per epoch: {len(train_loader)}")
    print("="*60)

    for epoch in range(start_epoch, config["training"]["num_epochs"]):
        # Train and validate
        train_loss = train_epoch_conditioned(unet, scheduler, mixer, train_loader, optimizer, device, epoch)
        val_loss = validate_conditioned(unet, scheduler, mixer, val_loader, device)
        
        # Update learning rate
        scheduler_lr.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # Update training stats
        training_stats = update_training_stats(
            training_stats, epoch, train_loss, val_loss, current_lr
        )

        print(f"Epoch {epoch+1}/{config['training']['num_epochs']} - "
              f"Train: {train_loss:.6f} | Val: {val_loss:.6f} | LR: {current_lr:.6f}", flush=True)
        
        # Save checkpoint
        is_best = val_loss < best_loss
        if is_best:
            best_loss = val_loss
        
        save_periodic = (epoch + 1) % config["training"].get("periodic_checkpoint_every", 10) == 0
        save_checkpoint(
            exp_dir, epoch,
            state_dict={
                'unet': unet.state_dict(),
                'mixer': mixer.state_dict(),
                'opt': optimizer.state_dict(),
                'sched': scheduler_lr.state_dict()
            },
            training_stats=training_stats,
            val_loss=val_loss,
            best_loss=best_loss,
            is_best=is_best,
            save_periodic=save_periodic
        )
        
        # Save training stats
        save_training_stats(exp_dir, training_stats)

        # Generate samples
        if (epoch + 1) % config["training"]["sample_every"] == 0:
            print(f"Generating samples at epoch {epoch+1}...")
            generate_samples_conditioned(
                diffusion_model, mixer, fixed_samples, exp_dir, epoch, device
            )

    print(f"\nTraining complete! Best validation loss: {best_loss:.6f}")
    print(f"Checkpoints saved in: {exp_dir / 'checkpoints'}")
    print(f"Samples saved in: {exp_dir / 'samples'}")


if __name__ == "__main__":
    main()


================================================================================
FILE: training\train_conditioned_diffusion.py
================================================================================

"""
Training script for conditioned latent diffusion model.
Refactored version using modular utilities.
"""

import argparse
import random
from pathlib import Path
import yaml
import torch
from torch.utils.data import DataLoader, random_split
from torch.optim import Adam
from torch.optim.lr_scheduler import CosineAnnealingLR

from modules.unet import UNet
from modules.scheduler import *
from modules.autoencoder import AutoEncoder
from modules.unified_dataset import UnifiedLayoutDataset
from modules.condition_mixer import ConcatMixer, WeightedMixer, LearnedWeightedMixer
from modules.diffusion import LatentDiffusion

# Import utilities
from training_utils import (
    load_experiment_config, save_experiment_config, setup_experiment_dir,
    save_checkpoint, load_checkpoint, save_training_stats, init_training_stats,
    update_training_stats, train_epoch_conditioned, validate_conditioned,
    save_split_files
)
from sampling_utils import generate_samples_conditioned


def collate_fn(batch):
    """Handle batches where some samples have None POV"""
    return {
        'scene_id': [b['scene_id'] for b in batch],
        'room_id': [b['room_id'] for b in batch],
        'pov': torch.stack([b['pov'] for b in batch if b['pov'] is not None]) if any(b['pov'] is not None for b in batch) else None,
        'graph': torch.stack([b['graph'] for b in batch]),
        'layout': torch.stack([b['layout'] for b in batch]),
    }


def main():
    parser = argparse.ArgumentParser(description='Train Conditioned Latent Diffusion Model')
    parser.add_argument("--exp_config", required=True, help="Path to experiment config YAML")
    parser.add_argument("--room_manifest", required=True, help="Path to room manifest")
    parser.add_argument("--scene_manifest", required=True, help="Path to scene manifest")
    parser.add_argument("--pov_type", default=None, help="POV type (e.g., 'rendered', 'semantic')")
    parser.add_argument("--mixer_type", choices=["concat", "weighted", "learned"], 
                        default=None, help="Override mixer type from config")
    parser.add_argument("--resume", action="store_true", help="Resume training")
    args = parser.parse_args()

    # Load and setup experiment
    config = load_experiment_config(args.exp_config)
    exp_dir = setup_experiment_dir(
        config["experiment"]["exp_dir"], 
        config["unet"]["config_path"], 
        args.resume
    )
    
    if not args.resume:
        save_experiment_config(config, exp_dir)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}", flush=True)

    # Load Models
    
    print(f"Loading U-Net from {config['unet']['config_path']}", flush=True)
    unet = UNet.from_config(config["unet"]["config_path"]).to(device)
    
    print(f"Loading {config['scheduler']['type']} with {config['scheduler']['num_steps']} steps", flush=True)
    scheduler_class = globals()[config["scheduler"]["type"]]
    scheduler = scheduler_class(num_steps=config["scheduler"]["num_steps"]).to(device)
    
    print(f"Loading Autoencoder from {config['autoencoder']['config_path']}", flush=True)
    autoencoder = AutoEncoder.from_config(config["autoencoder"]["config_path"]).to(device)
    if config["autoencoder"]["checkpoint_path"]:
        autoencoder.load_state_dict(
            torch.load(config["autoencoder"]["checkpoint_path"], map_location=device)
        )
    autoencoder.eval()

    # Create LatentDiffusion wrapper
    latent_shape = tuple(config["latent_shape"])
    diffusion_model = LatentDiffusion(
        unet=unet,
        scheduler=scheduler,
        autoencoder=autoencoder,
        latent_shape=latent_shape
    ).to(device)

    # Load Dataset
    
    print(f"Loading unified dataset...", flush=True)
    dataset = UnifiedLayoutDataset(
        args.room_manifest, 
        args.scene_manifest, 
        use_embeddings=True,
        pov_type=args.pov_type
    )
            
    train_size = int(0.9 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
    
    print(f"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}", flush=True)
    
    train_loader = DataLoader(
        train_dataset, 
        batch_size=config["training"]["batch_size"], 
        shuffle=True, 
        num_workers=4,
        collate_fn=collate_fn
    )
    val_loader = DataLoader(
        val_dataset, 
        batch_size=config["training"]["batch_size"], 
        shuffle=False, 
        num_workers=4,
        collate_fn=collate_fn
    )

    # Save split info
    train_df = dataset.df.iloc[train_dataset.indices]
    val_df = dataset.df.iloc[val_dataset.indices]
    save_split_files(exp_dir, train_df, val_df)

    # ========================================================================
    # Create Mixer
    # ========================================================================
    
    latent_hw = tuple(config["latent_shape"][-2:])  # (H, W)

    # Get UNet conditioning channels from config
    with open(config["unet"]["config_path"], 'r') as f:
        unet_cfg = yaml.safe_load(f)
    cond_channels = unet_cfg["unet"]["cond_channels"]

    # Get embedding dimensions from config
    pov_dim = config["mixer"]["pov_dim"]
    graph_dim = config["mixer"]["graph_dim"]
    
    print(f"Input dimensions - POV: {pov_dim}, Graph: {graph_dim}", flush=True)

    # Determine mixer type (CLI overrides config)
    mixer_type = config["mixer"].get("type", "concat")
    if args.mixer_type is not None:
        mixer_type = args.mixer_type

    print(f"Creating {mixer_type} mixer (out_channels={cond_channels}, target_size={latent_hw})...", flush=True)

    if mixer_type == "concat":
        mixer = ConcatMixer(
            out_channels=cond_channels, 
            target_size=latent_hw,
            pov_channels=pov_dim,
            graph_channels=graph_dim
        ).to(device)
    elif mixer_type == "weighted":
        mixer = WeightedMixer(
            out_channels=cond_channels, 
            target_size=latent_hw,
            pov_channels=pov_dim,
            graph_channels=graph_dim
        ).to(device)
    else:  # learned
        mixer = LearnedWeightedMixer(
            out_channels=cond_channels, 
            target_size=latent_hw,
            pov_channels=pov_dim,
            graph_channels=graph_dim
        ).to(device)

    # Freeze mixer weights (if not learned)
    if mixer_type != "learned":
        for param in mixer.parameters():
            param.requires_grad = False

    # Optimizer and Scheduler
    
    optimizer = Adam(unet.parameters(), lr=config["training"]["learning_rate"])
    scheduler_lr = CosineAnnealingLR(optimizer, T_max=config["training"]["num_epochs"])

    # Fixed samples for visual tracking
    
    num_fixed = min(8, len(val_dataset))
    fixed_indices = random.sample(range(len(val_dataset)), num_fixed)
    fixed_samples = [val_dataset[i] for i in fixed_indices]
    torch.save(fixed_indices, exp_dir / "fixed_indices.pt")

    # ========================================================================
    # Resume from checkpoint
    # ========================================================================
    
    start_epoch = 0
    best_loss = float("inf")
    training_stats = init_training_stats()
    
    latest_checkpoint = exp_dir / 'checkpoints' / 'latest.pt'
    if args.resume and latest_checkpoint.exists():
        start_epoch, best_loss, training_stats = load_checkpoint(
            latest_checkpoint,
            models_dict={'unet': unet, 'mixer': mixer},
            optimizer=optimizer,
            scheduler_lr=scheduler_lr
        )
        start_epoch += 1

    # ========================================================================
    # Training Loop
    # ========================================================================
    
    print(f"\nStarting training from epoch {start_epoch+1} to {config['training']['num_epochs']}")
    print(f"Batch size: {config['training']['batch_size']}, Learning rate: {config['training']['learning_rate']}")
    print(f"Batches per epoch: {len(train_loader)}")
    print("="*60)

    for epoch in range(start_epoch, config["training"]["num_epochs"]):
        # Train and validate
        train_loss = train_epoch_conditioned(unet, scheduler, mixer, train_loader, optimizer, device, epoch)
        val_loss = validate_conditioned(unet, scheduler, mixer, val_loader, device)
        
        # Update learning rate
        scheduler_lr.step()
        current_lr = optimizer.param_groups[0]['lr']
        
        # Update training stats
        training_stats = update_training_stats(
            training_stats, epoch, train_loss, val_loss, current_lr
        )

        print(f"Epoch {epoch+1}/{config['training']['num_epochs']} - "
              f"Train: {train_loss:.6f} | Val: {val_loss:.6f} | LR: {current_lr:.6f}", flush=True)
        
        # Save checkpoint
        is_best = val_loss < best_loss
        if is_best:
            best_loss = val_loss
        
        save_periodic = (epoch + 1) % config["training"].get("periodic_checkpoint_every", 10) == 0
        save_checkpoint(
            exp_dir, epoch,
            state_dict={
                'unet': unet.state_dict(),
                'mixer': mixer.state_dict(),
                'opt': optimizer.state_dict(),
                'sched': scheduler_lr.state_dict()
            },
            training_stats=training_stats,
            val_loss=val_loss,
            best_loss=best_loss,
            is_best=is_best,
            save_periodic=save_periodic
        )
        
        # Save training stats
        save_training_stats(exp_dir, training_stats)

        # Generate samples
        if (epoch + 1) % config["training"]["sample_every"] == 0:
            print(f"Generating samples at epoch {epoch+1}...")
            generate_samples_conditioned(
                diffusion_model, mixer, fixed_samples, exp_dir, epoch, device
            )

    print(f"\nTraining complete! Best validation loss: {best_loss:.6f}")
    print(f"Checkpoints saved in: {exp_dir / 'checkpoints'}")
    print(f"Samples saved in: {exp_dir / 'samples'}")


if __name__ == "__main__":
    main()


