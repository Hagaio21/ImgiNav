# New Layouts: Variational Autoencoder (VAE) with CLIP Loss for Joint Embedding Space
# Optimized for new layout dataset: 256×256 input → 32×32×4 latents
# Includes CLIP loss to align VAE features with text/POV embeddings
# Architecture: 32×32×4 = 4,096 dims (4x smaller than 64x64 for memory efficiency)
# Encoder: VAE with strong KL divergence regularization
# Loss: MSE=1.0, KLD=0.0001, LatentStd=0.1, CLIP=0.1 (joint embedding space)

experiment:
  name: "new_layouts_VAE_32x32_structural_256_clip"
  phase: "new_layouts_structural_training_with_clip"
  save_path: "/work3/s233249/ImgiNav/experiments/new_layouts/new_layouts_VAE_32x32_structural_256_clip"

dataset:
  # Use manifest with POV and graph embeddings (created by run_embed_controlnet_dataset.sh without VAE)
  # STEP 1: Run: bsub < training/hpc_scripts/run_embed_controlnet_dataset.sh
  # This creates POV and graph embeddings WITHOUT needing VAE checkpoint
  # Output goes to shared embeddings location
  manifest: "/work3/s233249/ImgiNav/experiments/shared_embeddings/manifest_with_embeddings.csv"
  outputs:
    rgb: "layout_path"  # Layout images for VAE training
    text_emb: "graph_embedding_path"  # Graph embeddings for CLIP loss
    pov_emb: "pov_embedding_path"  # POV embeddings for CLIP loss
  filters:
    is_empty: [false]
  return_path: false
  # Add resize transform for 256×256 input
  transform:
    type: Compose
    transforms:
      - type: Resize
        size: [256, 256]
        interpolation: nearest  # Preserve exact RGB values for class matching
      - type: ToTensor
      - type: Normalize
        mean: [0.5, 0.5, 0.5]
        std: [0.5, 0.5, 0.5]  # Normalizes to [-1, 1] range

autoencoder:
  encoder:
    in_channels: 3
    latent_channels: 4  # 32×32×4 latent space
    base_channels: 48  # Capacity 48 (smallest model)
    downsampling_steps: 3  # 256 -> 128 -> 64 -> 32 (32×32 spatial resolution)
    activation: SiLU
    norm_groups: 8
    variational: true  # VAE mode: outputs mu and logvar
  
  decoder:
    latent_channels: 4  # 32×32×4 latent space
    base_channels: 48  # Capacity 48 (smallest model)
    upsampling_steps: 3  # 32 -> 64 -> 128 -> 256 (32×32 spatial resolution)
    activation: SiLU
    norm_groups: 8
    heads:
      - type: RGBHead
        name: rgb
        out_channels: 3
        final_activation: tanh
  
  # CLIP projection layers (attached to model for training)
  clip_projection:
    projection_dim: 256  # Joint embedding space dimension
    text_dim: 384  # Graph embedding dimension (all-MiniLM-L6-v2)
    pov_dim: 512  # POV embedding dimension (ResNet18)
    latent_dim: 384  # Encoder feature dimension: base_channels * 2^downsampling_steps = 48 * 2^3 = 384

training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 32
  num_workers: 8
  shuffle: true
  epochs: 150
  learning_rate: 0.0001
  optimizer: AdamW
  weight_decay: 0.02
  save_interval: 5
  sample_interval: 5
  early_stopping_patience: 10
  early_stopping_min_delta: 0.00005
  early_stopping_restore_best: true
  use_amp: true
  
  loss:
    type: CompositeLoss
    losses:
      - type: MSELoss
        key: rgb
        target: rgb
        weight: 1.0
      - type: KLDLoss
        weight: 0.0001  # Strong KL divergence loss (encourages N(0,1) latent space)
      - type: LatentStandardizationLoss
        key: mu
        weight: 0.1
        mean_penalty_type: l2
        std_penalty_type: l2
        per_channel: true
      - type: CLIPLoss
        key: latent_features  # VAE encoder features (before mu/logvar heads)
        text_key: text_emb
        pov_key: pov_emb
        temperature: 0.07
        weight: 0.1  # Start with small weight, can increase if needed
        projection_dim: 256
        text_dim: 384
        pov_dim: 512
        combine_method: average
        use_model_projections: true  # Use projections attached to Autoencoder model

