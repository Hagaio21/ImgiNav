# Stage 2: Diffusion Viability Fine-tuning
# Loads Stage 1 checkpoint and fine-tunes with semantic losses
# This ensures decoded layouts are semantically viable

experiment:
  name: "diffusion_stage2_base"  # Will be customized per experiment
  phase: "diffusion_stage2"
  save_path: "/work3/s233249/ImgiNav/experiments/diffusion/stage2"

# Stage 1 checkpoint to load (required)
# Update this path to point to your Stage 1 best checkpoint
diffusion:
  stage1_checkpoint: /work3/s233249/ImgiNav/experiments/diffusion/ablation/capacity_unet128_d4/diffusion_ablation_capacity_unet128_d4_checkpoint_best.pt

# Autoencoder config (decoder stays frozen - only UNet is trained)
autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/phase1/phase1_6_AE_normalized/phase1_6_AE_normalized_checkpoint_best.pt
  frozen: true  # Keep decoder frozen - semantic losses guide UNet only

# Dataset: Need RGB and segmentation for semantic losses
# Use entire dataset including augmented samples
dataset:
  manifest: "/work3/s233249/ImgiNav/datasets/augmented/manifest.csv"
  outputs:
    latent: "latent_path"  # Pre-embedded latents
    rgb: "layout_path"  # RGB images for perceptual loss
    segmentation: "layout_path"  # Segmentation maps for semantic loss
  filters:
    is_empty: [false]
    # Use entire dataset including augmented samples
  return_path: false

# UNet config (inherited from Stage 1 checkpoint, but can override)
unet:
  in_channels: 16
  out_channels: 16
  base_channels: 128
  depth: 4
  num_res_blocks: 2
  time_dim: 128
  cond_channels: 0
  fusion_mode: "none"
  norm_groups: 8
  dropout: 0.2

# Scheduler (inherited from Stage 1 checkpoint)
scheduler:
  type: "CosineScheduler"
  num_steps: 500

training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 32  # Smaller batch due to decoding overhead
  num_workers: 8
  shuffle: true
  epochs: 200
  learning_rate: 0.00005  # 3x lower than Stage 1 for fine-tuning
  optimizer: AdamW
  weight_decay: 0.1
  
  # Checkpoint settings
  save_interval: 99999  # Disable periodic checkpoints
  eval_interval: 5  # Evaluate every 5 epochs
  sample_interval: 5  # Generate samples every 5 epochs
  
  # Mixed precision for efficiency
  use_amp: true
  
  # Gradient clipping for stability
  max_grad_norm: 0.5
  
  # Memorization testing - dual mode (light + heavy)
  # Light memorization: frequent checks with fewer samples
  memorization_light_interval: 5  # Light check every 5 epochs
  memorization_light_num_generate: 50  # Generate 50 samples for light check
  memorization_light_num_training: 1000  # Compare against 1000 training samples
  
  # Heavy memorization: thorough checks with full dataset
  memorization_check_interval: 20  # Heavy check every 20 epochs
  memorization_num_generate: 100  # Generate 100 samples to check
  memorization_num_training: 999999  # Check against entire dataset (very large number loads all)
  
  # Weighted sampling to balance distribution
  use_weighted_sampling: true
  column: class_combination  # Column to use for weighting (e.g., "class_combination", "content_category", "room_id")
  weighting_method: inverse_frequency  # Weighting method: "inverse_frequency", "sqrt", "log", "balanced"
  rare_threshold_percentile: 10.0  # Percentile below which classes are considered rare
  min_samples_threshold: 5  # Minimum samples for a class to be included (lower for combinations)
  max_weight: 20  # Cap weights to prevent over-sampling extremely rare classes (prevents memorization)
  use_grouped_weights: false  # Use grouped weights from stats file
  group_rare_classes: false  # Group rare classes into single category
  
  # Loss configuration
  loss:
    type: CompositeLoss
    losses:
      # Noise prediction loss (primary)
      - type: MSELoss
        key: "pred_noise"
        target: "noise"
        weight: 1.0
      
      # Latent-space structural loss (replaces segmentation loss)
      # Encourages spatially coherent latent representations
      - type: LatentStructuralLoss
        key: "pred_noise"
        target: "latent"
        weight: 0.1
        gradient_type: "sobel"  # or "laplacian"
        reduction: "mean"

