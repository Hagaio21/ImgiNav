# Stage 2: Fine-tuning for diffusion_ablation_capacity_unet64_d5
# Loads Stage 1 checkpoint and fine-tunes with semantic losses

experiment:
  name: "diffusion_stage2_unet64_d5"
  phase: "diffusion_stage2"
  save_path: "/work3/s233249/ImgiNav/experiments/diffusion/stage2/stage2_unet64_d5"

# Load Stage 1 checkpoint
diffusion:
  stage1_checkpoint: /work3/s233249/ImgiNav/experiments/diffusion/ablation/capacity_unet64_d5/diffusion_ablation_capacity_unet64_d5_checkpoint_best.pt

# Autoencoder config (decoder stays frozen - only UNet is trained)
autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/phase1/phase1_6_AE_normalized/phase1_6_AE_normalized_checkpoint_best.pt
  frozen: true  # Keep decoder frozen - semantic losses guide UNet only

dataset:
  manifest: "/work3/s233249/ImgiNav/datasets/augmented/manifest.csv"
  outputs:
    latent: "latent_path"
    rgb: "layout_path"
    segmentation: "layout_path"
  filters:
    is_empty: [false]
  return_path: false

unet:
  in_channels: 16
  out_channels: 16
  base_channels: 64
  depth: 5
  num_res_blocks: 2
  time_dim: 128
  cond_channels: 0
  fusion_mode: "none"
  norm_groups: 8
  dropout: 0.2

scheduler:
  type: "CosineScheduler"
  num_steps: 500

training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 36  # Smaller batch due to deeper model + decoding overhead
  num_workers: 8
  shuffle: true
  epochs: 100
  learning_rate: 0.00005
  optimizer: AdamW
  weight_decay: 0.1
  save_interval: 99999
  eval_interval: 5
  sample_interval: 10
  use_amp: true
  max_grad_norm: 0.5
  
  loss:
    noise_loss:
      type: MSELoss
      key: "pred_noise"
      target: "noise"
      weight: 1.0
    semantic_loss:
      type: SemanticLoss
      segmentation_loss:
        type: CrossEntropyLoss
        key: "segmentation"
        target: "segmentation"
        weight: 0.1
      perceptual_loss:
        type: PerceptualLoss
        key: "rgb"
        target: "rgb"
        weight: 0.05

