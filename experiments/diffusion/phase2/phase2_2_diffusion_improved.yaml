# Phase 2.2: Improved Diffusion Model with EMA, SNR Loss, Cosine Scheduler, and Enhanced UNet
# Uses Phase 2.1 autoencoder (64×64×4 latents)
# Improvements:
# - EMA (Exponential Moving Average) for UNet
# - MSELoss (reverted from SNRWeightedNoiseLoss)
# - Increased UNet capacity (base_channels: 64)
# - LinearScheduler (good for simple/geometric images)
# - More attention layers (downs, bottleneck, ups)
# - CosineAnnealingLR learning rate scheduler

experiment:
  name: phase2_2_diffusion_improved
  phase: phase2_2_diffusion
  save_path: /work3/s233249/ImgiNav/experiments/phase2/phase2_2_diffusion_improved
  epochs_target: 500

dataset:
  manifest: /work3/s233249/ImgiNav/experiments/phase2/phase2_2_diffusion_improved/embeddings/manifest_with_latents.csv
  outputs:
    latent: latent_path
  filters:
    is_empty:
    - false
    whiteness_ratio__lt: 0.95
  return_path: false

autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/phase2/phase2_1_VAE_64x64_structural_256/phase2_1_VAE_64x64_structural_256_checkpoint_best.pt
  frozen: true

unet:
  type: UnetWithAttention
  in_channels: 4
  out_channels: 4
  base_channels: 64  # Increased capacity (as requested)
  depth: 3  # Reduced from 4 to save memory
  num_res_blocks: 1  # Reduced from 2 to save memory
  time_dim: 256
  cond_channels: 0
  fusion_mode: none
  cond_dim: 256
  num_cond_classes: 2
  norm_groups: 8
  dropout: 0.1
  use_attention: true
  attention_heads: null
  attention_at:
  - bottleneck # Only bottleneck attention to fit in smaller GPU (15.77 GiB)
  # Removed downs and ups attention due to memory constraints
  use_ema: true  # Enable EMA for UNet
  ema_decay: 0.9999  # EMA decay rate

scheduler:
  type: LinearScheduler  # Linear scheduler for simple/geometric images
  num_steps: 1000

training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 8  # Very small batch size to accommodate EMA (which doubles UNet memory)
  num_workers: 8
  shuffle: true
  epochs: 500
  learning_rate: 0.0002  # Increased learning rate
  optimizer: AdamW
  weight_decay: 0.05
  save_interval: 99999
  eval_interval: 5
  sample_interval: 5
  use_amp: true  # Enable mixed precision to reduce memory usage (~50% reduction)
  use_weighted_sampling: true
  weight_column: type
  max_grad_norm: 1.0  # Gradient clipping enabled
  early_stopping_patience: 20
  early_stopping_min_delta: 1.0e-05
  use_non_uniform_sampling: false
  cfg_dropout_rate: 0.1
  guidance_scale: 3.0  # Lower for simple/geometric images (7.5 was too high)
  scheduler:
    type: cosine  # Changed from linear to cosine (CosineAnnealingLR)
  loss:
    type: CompositeLoss
    losses:
    - type: MSELoss
      key: pred_noise
      target: noise
      weight: 1.0

