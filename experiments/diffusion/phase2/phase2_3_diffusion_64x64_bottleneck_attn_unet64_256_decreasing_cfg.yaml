# Phase 2.3: Diffusion Model with Decreasing CFG Dropout Schedule (New Layouts)
# Uses new layouts VAE (64×64×4 latents)
# Strategy: Start with high CFG dropout (unconditional) and gradually decrease to conditional
# This allows the model to learn unconditional generation first, then fine-tune with conditioning
# CFG dropout schedule: 1.0 → 0.1 (linear decrease over training)

experiment:
  name: new_layouts_phase2_3_diffusion_64x64_bottleneck_attn_unet64_256_decreasing_cfg
  phase: new_layouts_phase2_3_diffusion
  save_path: /work3/s233249/ImgiNav/experiments/new_layouts/new_layouts_phase2_3_diffusion_64x64_bottleneck_attn_unet64_256_decreasing_cfg
  epochs_target: 500

dataset:
  manifest: /work3/s233249/ImgiNav/experiments/new_layouts/new_layouts_diffusion_64x64_bottleneck_attn_unet64_256_unconditional/embeddings/manifest_with_latents.csv
  outputs:
    latent: latent_path
  filters:
    is_empty:
    - false
  return_path: false

autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/new_layouts/new_layouts_VAE_64x64_structural_256/new_layouts_VAE_64x64_structural_256_checkpoint_best.pt
  frozen: true
latent_clamp_min: -6.0
latent_clamp_max: 6.0

diffusion:
  # Load from unconditional checkpoint (Stage 1) - fine-tune with decreasing CFG
  stage1_checkpoint: /work3/s233249/ImgiNav/experiments/new_layouts/new_layouts_diffusion_64x64_bottleneck_attn_unet64_256_unconditional/checkpoints/new_layouts_diffusion_64x64_bottleneck_attn_unet64_256_unconditional_checkpoint_best.pt

unet:
  type: UnetWithAttention
  in_channels: 4
  out_channels: 4
  base_channels: 64
  depth: 4
  num_res_blocks: 2
  time_dim: 256
  cond_channels: 0
  fusion_mode: none
  cond_dim: 256
  num_cond_classes: 2
  norm_groups: 8
  dropout: 0.1
  use_attention: true
  attention_heads: null
  attention_at:
  - bottleneck
  use_ema: false  # Disable EMA for simpler training

scheduler:
  type: LinearScheduler
  num_steps: 1000

training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 96
  num_workers: 8
  shuffle: true
  epochs: 500
  learning_rate: 0.0002
  optimizer: AdamW
  weight_decay: 0.05
  save_interval: 99999
  eval_interval: 5
  sample_interval: 5
  use_amp: false
  use_weighted_sampling: true
  weight_column: type
  max_grad_norm: 1.0
  early_stopping_patience: 20
  early_stopping_min_delta: 1.0e-05
  use_non_uniform_sampling: false
  # Decreasing CFG dropout schedule: start at 1.0 (fully unconditional), end at 0.1 (mostly conditional)
  # Changes every 10 epochs, plateaus at 0.1 after 200 epochs
  cfg_dropout_rate:
    start: 1.0  # Start with 100% dropout (fully unconditional training)
    end: 0.1    # End with 10% dropout (90% conditional, 10% unconditional)
    schedule: linear  # Linear decrease over epochs
    step_size: 10  # Change every 10 epochs (rate stays constant within each 10-epoch block)
    plateau_epoch: 200  # After this epoch, stay at end value (0.1)
  guidance_scale: 3.0  # Lower for simple/geometric images (7.5 was too high)
  scheduler:
    type: linear
  loss:
    type: CompositeLoss
    losses:
    - type: MSELoss
      key: pred_noise
      target: noise
      weight: 1.0

