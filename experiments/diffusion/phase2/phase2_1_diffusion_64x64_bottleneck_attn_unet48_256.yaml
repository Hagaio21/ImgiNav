experiment:
  name: phase2_1_diffusion_64x64_bottleneck_attn_unet48_256
  phase: phase2_1_diffusion
  save_path: /work3/s233249/ImgiNav/experiments/phase2/phase2_1_diffusion_64x64_bottleneck_attn_unet48_256
  epochs_target: 500
dataset:
  manifest: /work3/s233249/ImgiNav/experiments/phase2/phase2_1_diffusion_64x64_bottleneck_attn_unet48_256/embeddings/manifest_with_latents.csv
  outputs:
    latent: latent_path
  filters:
    is_empty:
    - false
    whiteness_ratio__lt: 0.95  # Exclude images with â‰¥95% white pixels
  return_path: false
autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/phase2/phase2_1_VAE_64x64_structural_256/phase2_1_VAE_64x64_structural_256_checkpoint_best.pt
  frozen: true
unet:
  type: UnetWithAttention
  in_channels: 4
  out_channels: 4
  base_channels: 48
  depth: 4  # Increased from 3 for deeper model
  num_res_blocks: 2
  time_dim: 256  # Increased from 128 for richer temporal encoding
  cond_channels: 0
  fusion_mode: none
  cond_dim: 256  # Increased from 128 for stronger room/scene conditioning
  num_cond_classes: 2
  norm_groups: 8
  dropout: 0.1  # Reduced from 0.2 for smaller model capacity
  use_attention: true
  attention_heads: null
  attention_at:
  - bottleneck
scheduler:
  type: LinearScheduler
  num_steps: 1000  # 1000 noise steps with linear schedule for stable training
training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 128
  num_workers: 8
  shuffle: true
  epochs: 500
  learning_rate: 0.00015
  optimizer: AdamW
  weight_decay: 0.05  # Reduced from 0.15 to reduce over-regularization
  save_interval: 99999
  eval_interval: 5
  sample_interval: 5
  use_amp: false
  use_weighted_sampling: true  # Enable weighted sampling for balanced room/scene distribution
  weight_column: type  # Sample based on room/scene type
  max_grad_norm: 1.0  # Increased from 0.5 to allow stronger gradients
  early_stopping_patience: 20
  early_stopping_min_delta: 1.0e-05
  use_non_uniform_sampling: false
  cfg_dropout_rate: 0.05  # Reduced from 0.1 for better early learning
  guidance_scale: 2.0
  scheduler:
    type: linear
  loss:
    type: CompositeLoss
    losses:
    - type: MSELoss
      key: pred_noise
      target: noise
      weight: 1.0
