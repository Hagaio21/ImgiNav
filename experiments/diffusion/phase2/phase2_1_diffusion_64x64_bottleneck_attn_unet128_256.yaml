# Phase 2.1: Diffusion with 256×256 Input (64×64×4 latents)
# Uses Phase 2.1 autoencoder (64×64×4 latents from 256×256 images)
# Large UNet (base_channels: 128) with attention only in bottleneck for global context
# Optimized for small datasets

experiment:
  name: phase2_1_diffusion_64x64_bottleneck_attn_unet128_256
  phase: phase2_1_diffusion
  save_path: /work3/s233249/ImgiNav/experiments/phase2/phase2_1_diffusion_64x64_bottleneck_attn_unet128_256
  epochs_target: 500  # Match training epochs for proper LR decay

dataset:
  # Manifest path will be automatically updated by train_pipeline_phase2.py
  # after embedding completes (will point to manifest_with_latents.csv in experiment directory)
  manifest: /work3/s233249/ImgiNav/datasets/layouts.csv  # Will be updated to embedded manifest
  outputs:
    latent: latent_path  # Use pre-embedded latents (will be set after embedding)
    type: type  # Room/scene conditioning: "room" -> 0, "scene" -> 1
  filters:
    is_empty: [false]
  return_path: false

autoencoder:
  # Checkpoint path will be automatically updated by train_pipeline_phase2.py
  # after autoencoder training completes
  checkpoint: null  # Will be set by pipeline script
  frozen: true

unet:
  type: UnetWithAttention  # Use attention-enabled UNet
  in_channels: 4  # Matches 64×64×4 latent space
  out_channels: 4  # Matches 64×64×4 latent space
  base_channels: 128  # Large model
  depth: 4  # Increased from 3 for deeper model
  num_res_blocks: 2
  time_dim: 256  # Increased from 128 for richer temporal encoding
  cond_channels: 0
  fusion_mode: none
  cond_dim: 256  # Increased from 128 for stronger room/scene conditioning
  num_cond_classes: 2  # ROOM=0, SCENE=1
  norm_groups: 8
  dropout: 0.2  # Increased from 0.1 to prevent memorization with predictable augmentations
  use_attention: true  # Enable attention
  attention_heads: null  # Auto (channels // 32)
  attention_at: ["bottleneck"]  # Attention only in bottleneck for global context without significant computational cost

scheduler:
  type: LinearScheduler
  num_steps: 500  # 500 noise steps with linear schedule for stable training

training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 64  # Smaller batch size due to larger model (but still larger than 512×512 version)
  num_workers: 8
  shuffle: true
  epochs: 500
  learning_rate: 0.00015  # Learning rate for linear scheduler
  optimizer: AdamW
  weight_decay: 0.15  # Increased from 0.1 for stronger regularization
  save_interval: 99999
  eval_interval: 5
  sample_interval: 5
  use_amp: false
  use_weighted_sampling: true  # Enable weighted sampling for balanced room/scene distribution
  weight_column: type  # Sample based on room/scene type
  max_grad_norm: 1.0  # Increased from 0.5 to allow stronger gradients
  early_stopping_patience: 20  # Stop if no improvement for 20 epochs
  early_stopping_min_delta: 0.00001  # Much smaller delta for diffusion (losses ~0.1, improvements ~0.00001)
  use_non_uniform_sampling: false  # Use uniform timestep sampling (all timesteps have equal probability)
  cfg_dropout_rate: 0.1  # Classifier-Free Guidance: randomly drop condition 10% of the time during training
  guidance_scale: 2.0  # CFG guidance scale for sampling (1.0 = no CFG, higher = stronger conditioning)
  
  scheduler:
    type: linear
  loss:
    type: CompositeLoss
    losses:
      - type: SNRWeightedNoiseLoss
        key: pred_noise
        target: noise
        weight: 1.0

