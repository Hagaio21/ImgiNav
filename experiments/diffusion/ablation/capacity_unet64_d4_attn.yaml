experiment:
  name: diffusion_ablation_capacity_unet64_d4_attn
  phase: diffusion_ablation
  save_path: /work3/s233249/ImgiNav/experiments/diffusion/ablation/capacity_unet64_d4_attn
  epochs_target: 500  # Match training epochs for proper LR decay
dataset:
  manifest: /work3/s233249/ImgiNav/datasets/augmented/manifest.csv  # Augmented dataset manifest with original + augmented images and latents
  outputs:
    latent: latent_path  # Use pre-embedded latents for faster training
  filters:
    is_empty:
    - false
    is_augmented: true  # Only use real (non-augmented) samples
  return_path: false
autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/phase1/phase1_6_AE_normalized/phase1_6_AE_normalized_checkpoint_best.pt
  frozen: true
unet:
  type: UnetWithAttention  # Use attention-enabled UNet
  in_channels: 16
  out_channels: 16
  base_channels: 64
  depth: 4
  num_res_blocks: 2
  time_dim: 128
  cond_channels: 0
  fusion_mode: none
  norm_groups: 8
  dropout: 0.2  # Increased from 0.1 to prevent memorization with predictable augmentations
  use_attention: true  # Enable attention
  attention_heads: null  # Auto (channels // 32)
  attention_at: ["bottleneck", "downs", "ups"]  # Apply attention everywhere
scheduler:
  type: CosineScheduler
  num_steps: 500  # Reduced from 1000 for faster training and sampling
training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 64  # Increased for gradient stability
  num_workers: 8
  shuffle: true
  epochs: 500
  learning_rate: 0.00015  # Slightly reduced for cosine scheduler stability (was 0.0002)
  optimizer: AdamW
  weight_decay: 0.1  # Increased from 0.05 for stronger regularization to prevent memorization
  save_interval: 99999
  eval_interval: 5
  sample_interval: 10
  use_amp: true
  
  # Weighted sampling to balance distribution
  use_weighted_sampling: true
  column: class_combination  # Column to use for weighting (e.g., "class_combination", "content_category", "room_id")
  weighting_method: inverse_frequency  # Weighting method: "inverse_frequency", "sqrt", "log", "balanced"
  rare_threshold_percentile: 10.0  # Percentile below which classes are considered rare
  min_samples_threshold: 5  # Minimum samples for a class to be included (lower for combinations)
  max_weight: 20  # Cap weights to prevent over-sampling extremely rare classes (prevents memorization)
  use_grouped_weights: false  # Use grouped weights from stats file
  group_rare_classes: true  # Group rare classes into single category (prevents all rare classes from hitting max_weight cap)
  
  # Gradient clipping for stability (prevents network collapse, especially with cosine schedulers)
  max_grad_norm: 0.5  # More aggressive clipping to prevent network collapse (was 1.0)
  
  scheduler:
    type: cosine
  loss:
    type: MSELoss
    key: pred_noise
    target: noise
    weight: 1.0

