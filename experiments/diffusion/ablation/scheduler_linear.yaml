experiment:
  name: diffusion_ablation_scheduler_linear
  phase: diffusion_ablation
  save_path: /work3/s233249/ImgiNav/experiments/diffusion/ablation/scheduler_linear
  epochs_target: 1000
dataset:
  manifest: /work3/s233249/ImgiNav/datasets/layouts_latents.csv  # Manifest with latent_path column (created by preembed_latents.py)
  outputs:
    latent: latent_path  # Use pre-embedded latents for faster training
  filters:
    is_empty:
    - false
  return_path: false
autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/phase1/phase1_5_AE_final/phase1_5_AE_final_checkpoint_best.pt
  frozen: true
unet:
  in_channels: 16
  out_channels: 16
  base_channels: 128
  depth: 4
  num_res_blocks: 2
  time_dim: 128
  cond_channels: 0
  fusion_mode: none
  norm_groups: 8
scheduler:
  type: LinearScheduler
  num_steps: 1000
training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 32
  num_workers: 8
  shuffle: true
  epochs: 500
  learning_rate: 0.0001
  optimizer: AdamW
  weight_decay: 0.01
  save_interval: 99999
  eval_interval: 5
  sample_interval: 10
  use_amp: true
  
  # Weighted sampling to balance room_id distribution
  use_weighted_sampling: true
  
  # Gradient clipping for stability (prevents network collapse, especially with cosine schedulers)
  max_grad_norm: 1.0
  
  scheduler:
    type: cosine
  loss:
    type: MSELoss
    key: pred_noise
    target: noise
    weight: 1.0
