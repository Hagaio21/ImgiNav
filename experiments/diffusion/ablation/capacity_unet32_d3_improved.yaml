experiment:
  name: diffusion_ablation_capacity_unet32_d3_improved
  phase: diffusion_ablation
  save_path: /work3/s233249/ImgiNav/experiments/diffusion/ablation/capacity_unet32_d3_improved
  epochs_target: 400

dataset:
  manifest: /work3/s233249/ImgiNav/datasets/augmented/manifest.csv
  outputs:
    latent: latent_path
  filters:
    is_empty: [false]
  return_path: false

autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/phase1/phase1_6_AE_normalized/phase1_6_AE_normalized_checkpoint_best.pt
  frozen: true

unet:
  in_channels: 16
  out_channels: 16
  base_channels: 32  # Small capacity to prevent memorization
  depth: 3  # Reduced depth
  num_res_blocks: 1  # Reduced blocks
  time_dim: 128
  cond_channels: 0
  fusion_mode: none
  norm_groups: 8
  dropout: 0.35  # Increased dropout for low diversity dataset

scheduler:
  type: CosineScheduler
  num_steps: 1000  # Increased steps for better quality

training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 128  # Larger batch for smallest model
  num_workers: 8
  shuffle: true
  epochs: 400
  learning_rate: 0.00008  # Lower LR for stability
  optimizer: AdamW
  weight_decay: 0.2  # Stronger regularization
  save_interval: 99999
  eval_interval: 3  # More frequent evaluation
  sample_interval: 10
  use_amp: true
  use_weighted_sampling: false
  max_grad_norm: 0.5
  early_stopping_patience: 20  # Stop if no improvement for 20 epochs
  early_stopping_min_delta: 0.00001  # Much smaller delta for diffusion (losses ~0.1, improvements ~0.00001)
  use_non_uniform_sampling: true  # Favor high-noise timesteps for better generalization
  
  scheduler:
    type: cosine
  
  loss:
    type: CompositeLoss
    losses:
      - type: SNRWeightedNoiseLoss
        key: pred_noise
        target: noise
        weight: 1.0
      - type: LatentStructuralLoss
        key: pred_noise
        target: latent
        gradient_type: sobel
        weight: 0.3  # Increased structural loss for better spatial quality

