# Base template for diffusion ablation experiments
# This config will be customized for each ablation

experiment:
  name: "diffusion_ablation_base"  # Will be replaced per experiment
  phase: "diffusion_ablation"
  save_path: "/work3/s233249/ImgiNav/experiments/diffusion/ablation"
  epochs_target: 1000  # Target epochs for learning rate scheduler

dataset:
  manifest: "/work3/s233249/ImgiNav/datasets/augmented/manifest.csv"  # Augmented dataset manifest with original + augmented images and latents
  outputs:
    latent: "latent_path"  # Use pre-embedded latents for faster training
  filters:
    is_empty: [false]
  return_path: false

# Autoencoder checkpoint (decoder will be extracted from it)
# Only decoder needed since we're using pre-embedded latents
# Phase 1.6: Normalized autoencoder with standardized latents ~N(0,1)
autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/phase1/phase1_6_AE_normalized/phase1_6_AE_normalized_checkpoint_best.pt
  frozen: true  # Freeze decoder during diffusion training

unet:
  in_channels: 16  # Matches latent_channels from Phase 1.6
  out_channels: 16  # Matches latent_channels from Phase 1.6
  base_channels: 128  # Will vary: 64, 128, 256
  depth: 4  # Will vary: 3, 4, 5
  num_res_blocks: 2
  time_dim: 128
  cond_channels: 0  # Unconditional for now
  fusion_mode: "none"
  norm_groups: 8
  dropout: 0.2  # Increased from 0.1 to prevent memorization with predictable augmentations

scheduler:
  type: "CosineScheduler"  # Will vary: LinearScheduler, CosineScheduler, QuadraticScheduler
  num_steps: 500  # Reduced from 1000 for faster training and sampling

training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 64  # Increased for pre-embedded latents (32Ã— smaller than images) - larger batch for gradient stability
  num_workers: 8
  shuffle: true
  epochs: 500  # Long training
  learning_rate: 0.00015  # Slightly reduced for cosine scheduler stability (was 0.0002)
  optimizer: AdamW
  weight_decay: 0.1  # Increased from 0.05 for stronger regularization to prevent memorization
  
  # Checkpoint settings - only best and latest
  save_interval: 99999  # Very large number to disable periodic checkpoints
  eval_interval: 5  # Evaluate every 5 epochs
  sample_interval: 10  # Generate samples every 10 epochs
  memorization_check_interval: 5  # Check memorization every 5 epochs
  memorization_num_generate: 100  # Number of samples to generate for check
  memorization_num_training: 1000  # Number of training samples to compare against
  
  # Mixed precision for efficiency
  use_amp: true
  
  # Weighted sampling disabled: uniform sampling for first 20k steps to prevent red-blob collapse
  use_weighted_sampling: false
  
  # Gradient clipping for stability (prevents network collapse, especially with cosine schedulers)
  max_grad_norm: 0.5  # More aggressive clipping to prevent network collapse (was 1.0)
  
  # Loss (simple MSE on noise prediction)
  # Note: Latents are standardized ~N(0,1) from Phase 1.6, so standard N(0,1) noise is used
  loss:
    type: MSELoss
    key: "pred_noise"
    target: "noise"
    weight: 1.0

