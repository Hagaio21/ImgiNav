# Base template for diffusion ablation experiments
# This config will be customized for each ablation

experiment:
  name: "diffusion_ablation_base"  # Will be replaced per experiment
  phase: "diffusion_ablation"
  save_path: "/work3/s233249/ImgiNav/experiments/diffusion/ablation"
  epochs_target: 1000  # Target epochs for learning rate scheduler

dataset:
  manifest: "/work3/s233249/ImgiNav/datasets/layouts_latents.csv"  # Manifest with latent_path column (created by preembed_latents.py)
  outputs:
    latent: "latent_path"  # Use pre-embedded latents for faster training
  filters:
    is_empty: [false]
  return_path: false

# Autoencoder checkpoint (decoder will be extracted from it)
# Only decoder needed since we're using pre-embedded latents
autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/phase1/phase1_5_AE_final/phase1_5_AE_final_checkpoint_best.pt
  frozen: true  # Freeze decoder during diffusion training

unet:
  in_channels: 16  # Matches latent_channels from Phase 1.5
  out_channels: 16  # Matches latent_channels from Phase 1.5
  base_channels: 128  # Will vary: 64, 128, 256
  depth: 4  # Will vary: 3, 4, 5
  num_res_blocks: 2
  time_dim: 128
  cond_channels: 0  # Unconditional for now
  fusion_mode: "none"
  norm_groups: 8

scheduler:
  type: "CosineScheduler"  # Will vary: LinearScheduler, CosineScheduler, QuadraticScheduler
  num_steps: 1000  # Will vary: 500, 1000, 2000

training:
  seed: 42
  train_split: 0.8
  split_seed: 42
  batch_size: 32  # Increased for pre-embedded latents (32Ã— smaller than images)
  num_workers: 8
  shuffle: true
  epochs: 500  # Long training
  learning_rate: 0.0001
  optimizer: AdamW
  weight_decay: 0.01
  
  # Checkpoint settings - only best and latest
  save_interval: 99999  # Very large number to disable periodic checkpoints
  eval_interval: 5  # Evaluate every 5 epochs
  sample_interval: 10  # Generate samples every 10 epochs
  
  # Mixed precision for efficiency
  use_amp: true
  
  # Weighted sampling to balance room_id distribution
  use_weighted_sampling: true
  
  # Gradient clipping for stability (prevents network collapse, especially with cosine schedulers)
  max_grad_norm: 1.0  # Set to null/omit to disable gradient clipping
  
  # Loss (simple MSE on noise prediction)
  loss:
    type: MSELoss
    key: "pred_noise"
    target: "noise"
    weight: 1.0

