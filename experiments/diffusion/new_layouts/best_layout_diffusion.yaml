experiment:
  name: best_layout_diffusion
  phase: diffusion_training
  save_path: /work3/s233249/ImgiNav/experiments/new_layouts/best_layout_diffusion
  epochs_target: 300

dataset:
  # Point this to your latent manifest
  manifest: /work3/s233249/ImgiNav/experiments/new_layouts/new_layouts_diffusion_64x64_bottleneck_attn_unet48_256_unconditional/embeddings/manifest_with_latents.csv
  outputs:
    latent: latent_path
  filters:
    is_empty:
    - false
  return_path: false

autoencoder:
  checkpoint: /work3/s233249/ImgiNav/experiments/new_layouts/new_layouts_VAE_64x64_structural_256/new_layouts_VAE_64x64_structural_256_checkpoint_best.pt
  frozen: true

# CRITICAL: Fixes the "Neon Blob" issue.
latent_clamp_min: -6.0
latent_clamp_max: 6.0

# Use 5.5 based on typical KL-VAE statistics (You must add the support in diffusion.py for this to work)
scale_factor: 5.5 

unet:
  type: UnetWithAttention
  in_channels: 4
  out_channels: 4
  
  # ARCHITECTURE OPTIMIZATION
  base_channels: 96       # Increased capacity
  depth: 3                # Reduced depth (Bottleneck at 8x8 resolution instead of 4x4)
  num_res_blocks: 2
  
  time_dim: 256
  norm_groups: 32         # Good for 96 channels
  dropout: 0.1
  
  # ATTENTION STRATEGY
  use_attention: true
  attention_heads: 8      
  attention_at:
    - bottleneck          # Applies attention at 8x8
    - downs               # Applies attention at 16x16 and 32x32
    - ups                 # Applies attention at 16x16 and 32x32
  
  # DiffusionModel reads this key to enable/disable EMA wrapper
  use_ema: false

scheduler:
  type: LinearScheduler   # Uses your existing class from scheduler.py
  num_steps: 1000         # Your code hardcodes betas 1e-4 to 0.02 for this

training:
  seed: 42
  train_split: 0.8
  batch_size: 64          # Try 64. If OOM, reduce to 48.
  num_workers: 8
  shuffle: true
  epochs: 300
  learning_rate: 0.0001   # Lower LR for stability
  optimizer: AdamW
  weight_decay: 0.01
  
  save_interval: 20
  eval_interval: 10
  sample_interval: 10
  
  use_amp: true
  max_grad_norm: 0.5
  
  scheduler:
    type: cosine          # This refers to the Learning Rate scheduler, not Noise scheduler

  loss:
    type: MSELoss
    key: pred_noise
    target: noise
    weight: 1.0

