# ControlNet training config for fine-tuned Stable Diffusion UNet
# Uses POV and graph embeddings for conditioning

experiment:
  name: "sd_finetuned_controlnet"
  phase: "controlnet_sd_finetuned"
  save_path: "/work3/s233249/ImgiNav/experiments/controlnet/sd_finetuned"

# Fine-tuned UNet (will be loaded via --finetuned_unet_path argument)
# Path: outputs/baseline_sd_finetuned_full/checkpoint-best/unet

# Scheduler
scheduler:
  model_id: "runwayml/stable-diffusion-v1-5"  # Base SD model for scheduler config

# Dataset: ControlNet requires layout latents + POV + graph embeddings
dataset:
  manifest: "/work3/s233249/ImgiNav/datasets/controlnet_training_manifest.csv"
  outputs:
    latent: "layout_embedding_path"      # Column with layout latent paths (.pt files)
    text_emb: "graph_embedding_path"      # Column with graph embedding paths (.pt files)
    pov_emb: "pov_embedding_path"        # Column with POV embedding paths (.pt files)
  filters:
    is_empty: [false]
  return_path: false

# ControlNet adapter config
# The adapter converts POV and graph embeddings to control features
controlnet:
  adapter:
    type: "SimpleAdapter"  # Options: "SimpleAdapter", "MLPAdapter", "DeepAdapter"
    text_dim: 384  # Graph embedding dimension (all-MiniLM-L6-v2 outputs 384)
    pov_dim: 512   # POV embedding dimension (ResNet18 outputs 512)
    # Note: base_channels and depth are estimated from SD UNet (typically 320, 4)
  fusion:
    type: scaled_add
    init_scale: 0.5  # Initialize scale to 0.5 instead of 1.0 to give control features more initial influence

debug_control_features: true  # Enable debug logging for control feature magnitudes during training

# Training config
training:
  seed: 42
  batch_size: 16  # Smaller batch size for SD UNet (larger model)
  num_workers: 4
  shuffle: true
  epochs: 100
  learning_rate: 0.0001
  optimizer: AdamW
  weight_decay: 0.01
  
  # Checkpoint settings
  save_interval: 10  # Save every 10 epochs
  eval_interval: 5   # Evaluate every 5 epochs
  sample_interval: 10  # Generate samples every 10 epochs
  
  # Mixed precision for efficiency
  use_amp: true
  
  # Gradient clipping for stability
  max_grad_norm: 1.0

