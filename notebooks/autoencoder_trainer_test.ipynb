{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2142918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training autoencoder for 5 epochs on cpu\n",
      "[Epoch 1] Step 1 | Loss: 0.093619\n",
      "[Epoch 1] Step 2 | Loss: 0.092660\n",
      "[Sample] Saved: ae_test_outputs\\samples\\sample_step_2.png\n",
      "[Epoch 1] Step 3 | Loss: 0.092034\n",
      "[Epoch 1] Step 4 | Loss: 0.092135\n",
      "[Sample] Saved: ae_test_outputs\\samples\\sample_step_4.png\n",
      "[Epoch 1] Average training loss: 0.092612\n",
      "[Epoch 1] Validation loss: 0.083098\n",
      "[Checkpoint] Saved: ae_test_outputs/checkpoints\\ae_epoch_1.pt\n",
      "[Plot] Updated: ae_test_outputs\\train_val_curve.png\n",
      "[Metrics] Updated: ae_test_outputs\\metrics.json\n",
      "[Epoch 2] Step 5 | Loss: 0.091514\n",
      "[Epoch 2] Step 6 | Loss: 0.090474\n",
      "[Sample] Saved: ae_test_outputs\\samples\\sample_step_6.png\n",
      "[Epoch 2] Step 7 | Loss: 0.089356\n",
      "[Epoch 2] Step 8 | Loss: 0.089698\n",
      "[Sample] Saved: ae_test_outputs\\samples\\sample_step_8.png\n",
      "[Epoch 2] Average training loss: 0.090261\n",
      "[Epoch 2] Validation loss: 0.083367\n",
      "[Checkpoint] Saved: ae_test_outputs/checkpoints\\ae_epoch_2.pt\n",
      "[Plot] Updated: ae_test_outputs\\train_val_curve.png\n",
      "[Metrics] Updated: ae_test_outputs\\metrics.json\n",
      "[Epoch 3] Step 9 | Loss: 0.089081\n",
      "[Epoch 3] Step 10 | Loss: 0.088665\n",
      "[Sample] Saved: ae_test_outputs\\samples\\sample_step_10.png\n",
      "[Epoch 3] Step 11 | Loss: 0.088447\n",
      "[Epoch 3] Step 12 | Loss: 0.088243\n",
      "[Sample] Saved: ae_test_outputs\\samples\\sample_step_12.png\n",
      "[Epoch 3] Average training loss: 0.088609\n",
      "[Epoch 3] Validation loss: 0.083095\n",
      "[Checkpoint] Saved: ae_test_outputs/checkpoints\\ae_epoch_3.pt\n",
      "[Plot] Updated: ae_test_outputs\\train_val_curve.png\n",
      "[Metrics] Updated: ae_test_outputs\\metrics.json\n",
      "[Epoch 4] Step 13 | Loss: 0.087619\n",
      "[Epoch 4] Step 14 | Loss: 0.087184\n",
      "[Sample] Saved: ae_test_outputs\\samples\\sample_step_14.png\n",
      "[Epoch 4] Step 15 | Loss: 0.087829\n",
      "[Epoch 4] Step 16 | Loss: 0.086813\n",
      "[Sample] Saved: ae_test_outputs\\samples\\sample_step_16.png\n",
      "[Epoch 4] Average training loss: 0.087361\n",
      "[Epoch 4] Validation loss: 0.083298\n",
      "[Checkpoint] Saved: ae_test_outputs/checkpoints\\ae_epoch_4.pt\n",
      "[Plot] Updated: ae_test_outputs\\train_val_curve.png\n",
      "[Metrics] Updated: ae_test_outputs\\metrics.json\n",
      "[Epoch 5] Step 17 | Loss: 0.086785\n",
      "[Epoch 5] Step 18 | Loss: 0.086902\n",
      "[Sample] Saved: ae_test_outputs\\samples\\sample_step_18.png\n",
      "[Epoch 5] Step 19 | Loss: 0.086308\n",
      "[Epoch 5] Step 20 | Loss: 0.086288\n",
      "[Sample] Saved: ae_test_outputs\\samples\\sample_step_20.png\n",
      "[Epoch 5] Average training loss: 0.086571\n",
      "[Epoch 5] Validation loss: 0.083432\n",
      "[Checkpoint] Saved: ae_test_outputs/checkpoints\\ae_epoch_5.pt\n",
      "[Plot] Updated: ae_test_outputs\\train_val_curve.png\n",
      "[Metrics] Updated: ae_test_outputs\\metrics.json\n",
      "Autoencoder training complete.\n",
      "\n",
      "Autoencoder training complete.\n",
      "Artifacts saved in: c:\\Users\\Hagai.LAPTOP-QAG9263N\\Desktop\\Thesis\\repositories\\ImagiNav\\notebooks\\ae_test_outputs\n",
      "Metric log entries: 25\n",
      "Example metric entry: {'epoch': 1, 'step': 1, 'train_loss': 0.09361857175827026}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1. Setup (run once)\n",
    "# ============================================================\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys, os\n",
    "sys.path.append(r\"C:\\Users\\Hagai.LAPTOP-QAG9263N\\Desktop\\Thesis\\repositories\\ImagiNav\")\n",
    "\n",
    "from modules.autoencoder import AutoEncoder\n",
    "from training.autoencoder_trainer import AutoEncoderTrainer\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. Dummy dataset\n",
    "# ============================================================\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, length=32, shape=(3, 64, 64)):\n",
    "        self.length = length\n",
    "        self.shape = shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # return tuple to preserve batch dimension\n",
    "        return (torch.rand(self.shape),)  # random data for testing\n",
    "\n",
    "\n",
    "train_loader = DataLoader(DummyDataset(length=16), batch_size=4)\n",
    "val_loader = DataLoader(DummyDataset(length=8), batch_size=4)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. Instantiate components\n",
    "# ============================================================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "autoencoder = AutoEncoder.from_shape(\n",
    "    in_channels=3,\n",
    "    out_channels=3,\n",
    "    base_channels=16,\n",
    "    latent_channels=3,\n",
    "    image_size=64,\n",
    "    latent_base=16,\n",
    "    norm=\"batch\",\n",
    "    act=\"relu\"\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "trainer = AutoEncoderTrainer(\n",
    "    autoencoder=autoencoder,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=5,\n",
    "    log_interval=1,       # log every step\n",
    "    sample_interval=2,    # save reconstructions\n",
    "    eval_interval=1,      # validate every epoch\n",
    "    output_dir=\"ae_test_outputs\",\n",
    "    ckpt_dir=\"ae_test_outputs/checkpoints\",\n",
    ")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. Run one short training cycle\n",
    "# ============================================================\n",
    "trainer.fit(train_loader, val_loader)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. Inspect results\n",
    "# ============================================================\n",
    "print(\"\\nAutoencoder training complete.\")\n",
    "print(\"Artifacts saved in:\", os.path.abspath(trainer.output_dir))\n",
    "print(\"Metric log entries:\", len(getattr(trainer, 'metric_log', [])))\n",
    "\n",
    "if hasattr(trainer, \"metric_log\") and trainer.metric_log:\n",
    "    print(\"Example metric entry:\", trainer.metric_log[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImgiNav",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
