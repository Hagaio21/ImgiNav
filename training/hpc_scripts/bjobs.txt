
Job <26695258[6]>, Job Name <diffusion_sweep[6]>, User <s233249>, Project <defa
                          ult>, Status <RUN>, Queue <gpul40s>, Command <#!/bin/
                          bash;#BSUB -J diffusion_sweep[1-6];#BSUB -o /work3/s2
                          33249/ImgiNav/ImgiNav/training/hpc_scripts/logs/diffu
                          sion_sweep.%J.%I.out;#BSUB -e /work3/s233249/ImgiNav/
                          ImgiNav/training/hpc_scripts/logs/diffusion_sweep.%J.
                          %I.err;#BSUB -n 8;#BSUB -R "rusage[mem=24000]";#BSUB 
                          -gpu "num=1";#BSUB -W 24:00;#BSUB -q gpul40s; set -eu
                          o pipefail; # =======================================
                          ======================================;# PATHS;# ====
                          =====================================================
                          ====================;BASE_DIR="/work3/s233249/ImgiNav
                          /ImgiNav";PYTHON_SCRIPT="${BASE_DIR}/training/train_d
                          iffusion.py";CONFIG_DIR="${BASE_DIR}/config/architect
                          ure/diffusion"; # Ordered list of YAMLs for unconditi
                          oned diffusion sweep;CONFIG_FILES=(;  "${CONFIG_DIR}/
                          E1_linear_64.yaml";  "${CONFIG_DIR}/E2_cosine_64.yaml
                          ";  "${CONFIG_DIR}/E3_quadratic_64.yaml";  "${CONFIG_
                          DIR}/E4_linear_128.yaml";  "${CONFIG_DIR}/E5_cosine_1
                          28.yaml";  "${CONFIG_DIR}/E6_quadratic_128.yaml";); #
                           Pick config for this array index;CONFIG_FILE="${CONF
                          IG_FILES[$((LSB_JOBINDEX-1))]}"; # ==================
                          =====================================================
                          ======;# MODULES;# ==================================
                          ===========================================;module lo
                          ad cuda/11.8;module load cudnn/v8.6.0.163-prod-cuda-1
                          1.X;export MKL_INTERFACE_LAYER=LP64; # ==============
                          =====================================================
                          ==========;# CONDA ENV;# ============================
                          =================================================;if 
                          [ -f "$HOME/miniconda3/etc/profile.d/conda.sh" ]; the
                          n;  source "$HOME/miniconda3/etc/profile.d/conda.sh";
                            conda activate imginav || {;    echo "Failed to act
                          ivate conda environment 'imginav'" >&2;    conda acti
                          vate scenefactor || {;      echo "Failed to activate 
                          any conda environment" >&2;      exit 1;    };  };fi;
                           # ==================================================
                          ===========================;# RUN;# =================
                          =====================================================
                          =======;echo "=======================================
                          ===";echo "Array job index: ${LSB_JOBINDEX}";echo "Ru
                          nning Diffusion experiment";echo "Config: ${CONFIG_FI
                          LE}";echo "Start: $(date)";echo "====================
                          ======================"; python "${PYTHON_SCRIPT}" --
                          config "${CONFIG_FILE}"; echo "======================
                          ====================";echo "Experiment COMPLETE";echo
                           "End: $(date)";echo "===============================
                          ===========">, Share group charged </s233249>, Esub <
                          dcc>
Tue Oct 28 15:56:11 2025: Submitted from host <gbarlogin1>, CWD </work3/s233249
                          /ImgiNav/ImgiNav/training/hpc_scripts>, Output File <
                          /work3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/l
                          ogs/diffusion_sweep.26695258.6.out>, Error File </wor
                          k3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/
                          diffusion_sweep.26695258.6.err>, 8 Task(s), Requested
                           Resources <rusage[mem=24000] span[hosts=1]>, Request
                          ed GPU <num=1>;
Tue Oct 28 15:58:31 2025: [6] started 8 Task(s) on Host(s) <8*n-62-13-18>, Allo
                          cated 8 Slot(s) on Host(s) <8*n-62-13-18>, Execution 
                          Home </zhome/62/5/203350>, Execution CWD </work3/s233
                          249/ImgiNav/ImgiNav/training/hpc_scripts>;
 RUNLIMIT                
 1440.0 min

 MEMLIMIT
   23.4 G 

 MEMORY USAGE:
 MEM Efficiency: 0.00%

 CPU USAGE:
 CPU PEAK: 0.00 ;  CPU PEAK DURATION: 0 second(s)
 CPU AVERAGE EFFICIENCY: 0.00% ;  CPU PEAK EFFICIENCY: 0.00%

 PENDING TIME DETAILS:
 Eligible pending time (seconds):	140
 Ineligible pending time (seconds):	0

 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

 EXTERNAL MESSAGES:
 MSG_ID FROM       POST_TIME      MESSAGE                             ATTACHMENT 
 0      s233249    Oct 28 15:58   n-62-13-18:gpus=1;                      N     

 RESOURCE REQUIREMENT DETAILS:
 Job-level: rusage[mem=24000] span[hosts=1]
 App-level: -
 Queue-level: same[type:model] affinity[core] cu[type=enclosure:maxcus=1]
 Combined: select[(ngpus>0) && (type == local)] order[-slots:-maxslots] rusage[
                          mem=24000.00:ngpus_physical=1.00] span[hosts=1] same[
                          type:model] cu[pref=config:maxcus=1:type=enclosure] a
                          ffinity[core(1)*1]
 Effective: select[((ngpus>0)) && (type == local)] order[-slots:-maxslots] rusa
                          ge[mem=24000.00,ngpus_physical=1.00] span[hosts=1] sa
                          me[type:model] cu[type=enclosure:maxcus=1:pref=config
                          ] affinity[core(1)*1] 

 GPU REQUIREMENT DETAILS:
 Combined: num=1:mode=exclusive_process:mps=no:j_exclusive=yes:aff=no:gvendor=n
                          vidia
 Effective: num=1:mode=exclusive_process:mps=no:j_exclusive=yes:aff=no:gvendor=
                          nvidia

