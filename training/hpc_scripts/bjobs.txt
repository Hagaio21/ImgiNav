
Job <26695129[1]>, Job Name <diffusion_sweep[1]>, User <s233249>, Project <defa
                          ult>, Status <PEND>, Queue <gpul40s>, Command <#!/bin
                          /bash;#BSUB -J diffusion_sweep[1-6];#BSUB -o /work3/s
                          233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/diff
                          usion_sweep.%J.%I.out;#BSUB -e /work3/s233249/ImgiNav
                          /ImgiNav/training/hpc_scripts/logs/diffusion_sweep.%J
                          .%I.err;#BSUB -n 8;#BSUB -R "rusage[mem=48000]";#BSUB
                           -gpu "num=1";#BSUB -W 24:00;#BSUB -q gpul40s; set -e
                          uo pipefail; # ======================================
                          =======================================;# PATHS;# ===
                          =====================================================
                          =====================;BASE_DIR="/work3/s233249/ImgiNa
                          v/ImgiNav";PYTHON_SCRIPT="${BASE_DIR}/training/train_
                          diffusion.py";CONFIG_DIR="${BASE_DIR}/config/architec
                          ture/diffusion"; # Ordered list of YAMLs for uncondit
                          ioned diffusion sweep;CONFIG_FILES=(;  "${CONFIG_DIR}
                          /E1_linear_64.yaml";  "${CONFIG_DIR}/E2_cosine_64.yam
                          l";  "${CONFIG_DIR}/E3_quadratic_64.yaml";  "${CONFIG
                          _DIR}/E4_linear_128.yaml";  "${CONFIG_DIR}/E5_cosine_
                          128.yaml";  "${CONFIG_DIR}/E6_quadratic_128.yaml";); 
                          # Pick config for this array index;CONFIG_FILE="${CON
                          FIG_FILES[$((LSB_JOBINDEX-1))]}"; # =================
                          =====================================================
                          =======;# MODULES;# =================================
                          ============================================;module l
                          oad cuda/11.8;module load cudnn/v8.6.0.163-prod-cuda-
                          11.X;export MKL_INTERFACE_LAYER=LP64; # =============
                          =====================================================
                          ===========;# CONDA ENV;# ===========================
                          ==================================================;if
                           [ -f "$HOME/miniconda3/etc/profile.d/conda.sh" ]; th
                          en;  source "$HOME/miniconda3/etc/profile.d/conda.sh"
                          ;  conda activate imginav || {;    echo "Failed to ac
                          tivate conda environment 'imginav'" >&2;    conda act
                          ivate scenefactor || {;      echo "Failed to activate
                           any conda environment" >&2;      exit 1;    };  };fi
                          ; # =================================================
                          ============================;# RUN;# ================
                          =====================================================
                          ========;echo "======================================
                          ====";echo "Array job index: ${LSB_JOBINDEX}";echo "R
                          unning Diffusion experiment";echo "Config: ${CONFIG_F
                          ILE}";echo "Start: $(date)";echo "===================
                          ======================="; python "${PYTHON_SCRIPT}" -
                          -config "${CONFIG_FILE}"; echo "=====================
                          =====================";echo "Experiment COMPLETE";ech
                          o "End: $(date)";echo "==============================
                          ============">, Share group charged </s233249>, Esub 
                          <dcc>
Tue Oct 28 15:41:37 2025: Submitted from host <gbarlogin1>, CWD </work3/s233249
                          /ImgiNav/ImgiNav/training/hpc_scripts>, Output File <
                          /work3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/l
                          ogs/diffusion_sweep.26695129.1.out>, Error File </wor
                          k3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/
                          diffusion_sweep.26695129.1.err>, 8 Task(s), Requested
                           Resources <rusage[mem=48000] span[hosts=1]>, Request
                          ed GPU <num=1>;
Tue Oct 28 15:41:37 2025: Reserved <8> job slots on host(s) <8*n-62-13-14> for 
                          <8> tasks;
 PENDING REASONS:
 Job's requirements for reserving resource (ngpus_physical) not satisfied: 3 ho
                          sts;

 RUNLIMIT                
 1440.0 min

 MEMLIMIT
   46.8 G 

 PENDING TIME DETAILS:
 Eligible pending time (seconds):	47
 Ineligible pending time (seconds):	0

 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

 RESOURCE REQUIREMENT DETAILS:
 Job-level: rusage[mem=48000] span[hosts=1]
 App-level: -
 Queue-level: same[type:model] affinity[core] cu[type=enclosure:maxcus=1]
 Combined: select[(ngpus>0) && (type == local)] order[-slots:-maxslots] rusage[
                          mem=48000.00:ngpus_physical=1.00] span[hosts=1] same[
                          type:model] cu[pref=config:maxcus=1:type=enclosure] a
                          ffinity[core(1)*1]
 Effective: -

 GPU REQUIREMENT DETAILS:
 Combined: num=1:mode=exclusive_process:mps=no:j_exclusive=yes:aff=no:gvendor=n
                          vidia
 Effective: -
------------------------------------------------------------------------------

Job <26695129[2]>, Job Name <diffusion_sweep[2]>, User <s233249>, Project <defa
                          ult>, Status <PEND>, Queue <gpul40s>, Command <#!/bin
                          /bash;#BSUB -J diffusion_sweep[1-6];#BSUB -o /work3/s
                          233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/diff
                          usion_sweep.%J.%I.out;#BSUB -e /work3/s233249/ImgiNav
                          /ImgiNav/training/hpc_scripts/logs/diffusion_sweep.%J
                          .%I.err;#BSUB -n 8;#BSUB -R "rusage[mem=48000]";#BSUB
                           -gpu "num=1";#BSUB -W 24:00;#BSUB -q gpul40s; set -e
                          uo pipefail; # ======================================
                          =======================================;# PATHS;# ===
                          =====================================================
                          =====================;BASE_DIR="/work3/s233249/ImgiNa
                          v/ImgiNav";PYTHON_SCRIPT="${BASE_DIR}/training/train_
                          diffusion.py";CONFIG_DIR="${BASE_DIR}/config/architec
                          ture/diffusion"; # Ordered list of YAMLs for uncondit
                          ioned diffusion sweep;CONFIG_FILES=(;  "${CONFIG_DIR}
                          /E1_linear_64.yaml";  "${CONFIG_DIR}/E2_cosine_64.yam
                          l";  "${CONFIG_DIR}/E3_quadratic_64.yaml";  "${CONFIG
                          _DIR}/E4_linear_128.yaml";  "${CONFIG_DIR}/E5_cosine_
                          128.yaml";  "${CONFIG_DIR}/E6_quadratic_128.yaml";); 
                          # Pick config for this array index;CONFIG_FILE="${CON
                          FIG_FILES[$((LSB_JOBINDEX-1))]}"; # =================
                          =====================================================
                          =======;# MODULES;# =================================
                          ============================================;module l
                          oad cuda/11.8;module load cudnn/v8.6.0.163-prod-cuda-
                          11.X;export MKL_INTERFACE_LAYER=LP64; # =============
                          =====================================================
                          ===========;# CONDA ENV;# ===========================
                          ==================================================;if
                           [ -f "$HOME/miniconda3/etc/profile.d/conda.sh" ]; th
                          en;  source "$HOME/miniconda3/etc/profile.d/conda.sh"
                          ;  conda activate imginav || {;    echo "Failed to ac
                          tivate conda environment 'imginav'" >&2;    conda act
                          ivate scenefactor || {;      echo "Failed to activate
                           any conda environment" >&2;      exit 1;    };  };fi
                          ; # =================================================
                          ============================;# RUN;# ================
                          =====================================================
                          ========;echo "======================================
                          ====";echo "Array job index: ${LSB_JOBINDEX}";echo "R
                          unning Diffusion experiment";echo "Config: ${CONFIG_F
                          ILE}";echo "Start: $(date)";echo "===================
                          ======================="; python "${PYTHON_SCRIPT}" -
                          -config "${CONFIG_FILE}"; echo "=====================
                          =====================";echo "Experiment COMPLETE";ech
                          o "End: $(date)";echo "==============================
                          ============">, Share group charged </s233249>, Esub 
                          <dcc>
Tue Oct 28 15:41:37 2025: Submitted from host <gbarlogin1>, CWD </work3/s233249
                          /ImgiNav/ImgiNav/training/hpc_scripts>, Output File <
                          /work3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/l
                          ogs/diffusion_sweep.26695129.2.out>, Error File </wor
                          k3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/
                          diffusion_sweep.26695129.2.err>, 8 Task(s), Requested
                           Resources <rusage[mem=48000] span[hosts=1]>, Request
                          ed GPU <num=1>;
Tue Oct 28 15:41:37 2025: Reserved <8> job slots on host(s) <8*n-62-13-14> for 
                          <8> tasks;
 PENDING REASONS:
 Job's requirements for reserving resource (ngpus_physical) not satisfied: 3 ho
                          sts;

 RUNLIMIT                
 1440.0 min

 MEMLIMIT
   46.8 G 

 PENDING TIME DETAILS:
 Eligible pending time (seconds):	47
 Ineligible pending time (seconds):	0

 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

 RESOURCE REQUIREMENT DETAILS:
 Job-level: rusage[mem=48000] span[hosts=1]
 App-level: -
 Queue-level: same[type:model] affinity[core] cu[type=enclosure:maxcus=1]
 Combined: select[(ngpus>0) && (type == local)] order[-slots:-maxslots] rusage[
                          mem=48000.00:ngpus_physical=1.00] span[hosts=1] same[
                          type:model] cu[pref=config:maxcus=1:type=enclosure] a
                          ffinity[core(1)*1]
 Effective: -

 GPU REQUIREMENT DETAILS:
 Combined: num=1:mode=exclusive_process:mps=no:j_exclusive=yes:aff=no:gvendor=n
                          vidia
 Effective: -
------------------------------------------------------------------------------

Job <26695129[3]>, Job Name <diffusion_sweep[3]>, User <s233249>, Project <defa
                          ult>, Status <PEND>, Queue <gpul40s>, Command <#!/bin
                          /bash;#BSUB -J diffusion_sweep[1-6];#BSUB -o /work3/s
                          233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/diff
                          usion_sweep.%J.%I.out;#BSUB -e /work3/s233249/ImgiNav
                          /ImgiNav/training/hpc_scripts/logs/diffusion_sweep.%J
                          .%I.err;#BSUB -n 8;#BSUB -R "rusage[mem=48000]";#BSUB
                           -gpu "num=1";#BSUB -W 24:00;#BSUB -q gpul40s; set -e
                          uo pipefail; # ======================================
                          =======================================;# PATHS;# ===
                          =====================================================
                          =====================;BASE_DIR="/work3/s233249/ImgiNa
                          v/ImgiNav";PYTHON_SCRIPT="${BASE_DIR}/training/train_
                          diffusion.py";CONFIG_DIR="${BASE_DIR}/config/architec
                          ture/diffusion"; # Ordered list of YAMLs for uncondit
                          ioned diffusion sweep;CONFIG_FILES=(;  "${CONFIG_DIR}
                          /E1_linear_64.yaml";  "${CONFIG_DIR}/E2_cosine_64.yam
                          l";  "${CONFIG_DIR}/E3_quadratic_64.yaml";  "${CONFIG
                          _DIR}/E4_linear_128.yaml";  "${CONFIG_DIR}/E5_cosine_
                          128.yaml";  "${CONFIG_DIR}/E6_quadratic_128.yaml";); 
                          # Pick config for this array index;CONFIG_FILE="${CON
                          FIG_FILES[$((LSB_JOBINDEX-1))]}"; # =================
                          =====================================================
                          =======;# MODULES;# =================================
                          ============================================;module l
                          oad cuda/11.8;module load cudnn/v8.6.0.163-prod-cuda-
                          11.X;export MKL_INTERFACE_LAYER=LP64; # =============
                          =====================================================
                          ===========;# CONDA ENV;# ===========================
                          ==================================================;if
                           [ -f "$HOME/miniconda3/etc/profile.d/conda.sh" ]; th
                          en;  source "$HOME/miniconda3/etc/profile.d/conda.sh"
                          ;  conda activate imginav || {;    echo "Failed to ac
                          tivate conda environment 'imginav'" >&2;    conda act
                          ivate scenefactor || {;      echo "Failed to activate
                           any conda environment" >&2;      exit 1;    };  };fi
                          ; # =================================================
                          ============================;# RUN;# ================
                          =====================================================
                          ========;echo "======================================
                          ====";echo "Array job index: ${LSB_JOBINDEX}";echo "R
                          unning Diffusion experiment";echo "Config: ${CONFIG_F
                          ILE}";echo "Start: $(date)";echo "===================
                          ======================="; python "${PYTHON_SCRIPT}" -
                          -config "${CONFIG_FILE}"; echo "=====================
                          =====================";echo "Experiment COMPLETE";ech
                          o "End: $(date)";echo "==============================
                          ============">, Share group charged </s233249>, Esub 
                          <dcc>
Tue Oct 28 15:41:37 2025: Submitted from host <gbarlogin1>, CWD </work3/s233249
                          /ImgiNav/ImgiNav/training/hpc_scripts>, Output File <
                          /work3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/l
                          ogs/diffusion_sweep.26695129.3.out>, Error File </wor
                          k3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/
                          diffusion_sweep.26695129.3.err>, 8 Task(s), Requested
                           Resources <rusage[mem=48000] span[hosts=1]>, Request
                          ed GPU <num=1>;
Tue Oct 28 15:41:37 2025: Reserved <8> job slots on host(s) <8*n-62-13-14> for 
                          <8> tasks;
 PENDING REASONS:
 Job's requirements for reserving resource (ngpus_physical) not satisfied: 3 ho
                          sts;

 RUNLIMIT                
 1440.0 min

 MEMLIMIT
   46.8 G 

 PENDING TIME DETAILS:
 Eligible pending time (seconds):	47
 Ineligible pending time (seconds):	0

 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

 RESOURCE REQUIREMENT DETAILS:
 Job-level: rusage[mem=48000] span[hosts=1]
 App-level: -
 Queue-level: same[type:model] affinity[core] cu[type=enclosure:maxcus=1]
 Combined: select[(ngpus>0) && (type == local)] order[-slots:-maxslots] rusage[
                          mem=48000.00:ngpus_physical=1.00] span[hosts=1] same[
                          type:model] cu[pref=config:maxcus=1:type=enclosure] a
                          ffinity[core(1)*1]
 Effective: -

 GPU REQUIREMENT DETAILS:
 Combined: num=1:mode=exclusive_process:mps=no:j_exclusive=yes:aff=no:gvendor=n
                          vidia
 Effective: -
------------------------------------------------------------------------------

Job <26695129[4]>, Job Name <diffusion_sweep[4]>, User <s233249>, Project <defa
                          ult>, Status <PEND>, Queue <gpul40s>, Command <#!/bin
                          /bash;#BSUB -J diffusion_sweep[1-6];#BSUB -o /work3/s
                          233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/diff
                          usion_sweep.%J.%I.out;#BSUB -e /work3/s233249/ImgiNav
                          /ImgiNav/training/hpc_scripts/logs/diffusion_sweep.%J
                          .%I.err;#BSUB -n 8;#BSUB -R "rusage[mem=48000]";#BSUB
                           -gpu "num=1";#BSUB -W 24:00;#BSUB -q gpul40s; set -e
                          uo pipefail; # ======================================
                          =======================================;# PATHS;# ===
                          =====================================================
                          =====================;BASE_DIR="/work3/s233249/ImgiNa
                          v/ImgiNav";PYTHON_SCRIPT="${BASE_DIR}/training/train_
                          diffusion.py";CONFIG_DIR="${BASE_DIR}/config/architec
                          ture/diffusion"; # Ordered list of YAMLs for uncondit
                          ioned diffusion sweep;CONFIG_FILES=(;  "${CONFIG_DIR}
                          /E1_linear_64.yaml";  "${CONFIG_DIR}/E2_cosine_64.yam
                          l";  "${CONFIG_DIR}/E3_quadratic_64.yaml";  "${CONFIG
                          _DIR}/E4_linear_128.yaml";  "${CONFIG_DIR}/E5_cosine_
                          128.yaml";  "${CONFIG_DIR}/E6_quadratic_128.yaml";); 
                          # Pick config for this array index;CONFIG_FILE="${CON
                          FIG_FILES[$((LSB_JOBINDEX-1))]}"; # =================
                          =====================================================
                          =======;# MODULES;# =================================
                          ============================================;module l
                          oad cuda/11.8;module load cudnn/v8.6.0.163-prod-cuda-
                          11.X;export MKL_INTERFACE_LAYER=LP64; # =============
                          =====================================================
                          ===========;# CONDA ENV;# ===========================
                          ==================================================;if
                           [ -f "$HOME/miniconda3/etc/profile.d/conda.sh" ]; th
                          en;  source "$HOME/miniconda3/etc/profile.d/conda.sh"
                          ;  conda activate imginav || {;    echo "Failed to ac
                          tivate conda environment 'imginav'" >&2;    conda act
                          ivate scenefactor || {;      echo "Failed to activate
                           any conda environment" >&2;      exit 1;    };  };fi
                          ; # =================================================
                          ============================;# RUN;# ================
                          =====================================================
                          ========;echo "======================================
                          ====";echo "Array job index: ${LSB_JOBINDEX}";echo "R
                          unning Diffusion experiment";echo "Config: ${CONFIG_F
                          ILE}";echo "Start: $(date)";echo "===================
                          ======================="; python "${PYTHON_SCRIPT}" -
                          -config "${CONFIG_FILE}"; echo "=====================
                          =====================";echo "Experiment COMPLETE";ech
                          o "End: $(date)";echo "==============================
                          ============">, Share group charged </s233249>, Esub 
                          <dcc>
Tue Oct 28 15:41:37 2025: Submitted from host <gbarlogin1>, CWD </work3/s233249
                          /ImgiNav/ImgiNav/training/hpc_scripts>, Output File <
                          /work3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/l
                          ogs/diffusion_sweep.26695129.4.out>, Error File </wor
                          k3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/
                          diffusion_sweep.26695129.4.err>, 8 Task(s), Requested
                           Resources <rusage[mem=48000] span[hosts=1]>, Request
                          ed GPU <num=1>;
Tue Oct 28 15:41:37 2025: Reserved <8> job slots on host(s) <8*n-62-13-14> for 
                          <8> tasks;
 PENDING REASONS:
 Job's requirements for reserving resource (ngpus_physical) not satisfied: 3 ho
                          sts;

 RUNLIMIT                
 1440.0 min

 MEMLIMIT
   46.8 G 

 PENDING TIME DETAILS:
 Eligible pending time (seconds):	47
 Ineligible pending time (seconds):	0

 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

 RESOURCE REQUIREMENT DETAILS:
 Job-level: rusage[mem=48000] span[hosts=1]
 App-level: -
 Queue-level: same[type:model] affinity[core] cu[type=enclosure:maxcus=1]
 Combined: select[(ngpus>0) && (type == local)] order[-slots:-maxslots] rusage[
                          mem=48000.00:ngpus_physical=1.00] span[hosts=1] same[
                          type:model] cu[pref=config:maxcus=1:type=enclosure] a
                          ffinity[core(1)*1]
 Effective: -

 GPU REQUIREMENT DETAILS:
 Combined: num=1:mode=exclusive_process:mps=no:j_exclusive=yes:aff=no:gvendor=n
                          vidia
 Effective: -
------------------------------------------------------------------------------

Job <26695129[5]>, Job Name <diffusion_sweep[5]>, User <s233249>, Project <defa
                          ult>, Status <PEND>, Queue <gpul40s>, Command <#!/bin
                          /bash;#BSUB -J diffusion_sweep[1-6];#BSUB -o /work3/s
                          233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/diff
                          usion_sweep.%J.%I.out;#BSUB -e /work3/s233249/ImgiNav
                          /ImgiNav/training/hpc_scripts/logs/diffusion_sweep.%J
                          .%I.err;#BSUB -n 8;#BSUB -R "rusage[mem=48000]";#BSUB
                           -gpu "num=1";#BSUB -W 24:00;#BSUB -q gpul40s; set -e
                          uo pipefail; # ======================================
                          =======================================;# PATHS;# ===
                          =====================================================
                          =====================;BASE_DIR="/work3/s233249/ImgiNa
                          v/ImgiNav";PYTHON_SCRIPT="${BASE_DIR}/training/train_
                          diffusion.py";CONFIG_DIR="${BASE_DIR}/config/architec
                          ture/diffusion"; # Ordered list of YAMLs for uncondit
                          ioned diffusion sweep;CONFIG_FILES=(;  "${CONFIG_DIR}
                          /E1_linear_64.yaml";  "${CONFIG_DIR}/E2_cosine_64.yam
                          l";  "${CONFIG_DIR}/E3_quadratic_64.yaml";  "${CONFIG
                          _DIR}/E4_linear_128.yaml";  "${CONFIG_DIR}/E5_cosine_
                          128.yaml";  "${CONFIG_DIR}/E6_quadratic_128.yaml";); 
                          # Pick config for this array index;CONFIG_FILE="${CON
                          FIG_FILES[$((LSB_JOBINDEX-1))]}"; # =================
                          =====================================================
                          =======;# MODULES;# =================================
                          ============================================;module l
                          oad cuda/11.8;module load cudnn/v8.6.0.163-prod-cuda-
                          11.X;export MKL_INTERFACE_LAYER=LP64; # =============
                          =====================================================
                          ===========;# CONDA ENV;# ===========================
                          ==================================================;if
                           [ -f "$HOME/miniconda3/etc/profile.d/conda.sh" ]; th
                          en;  source "$HOME/miniconda3/etc/profile.d/conda.sh"
                          ;  conda activate imginav || {;    echo "Failed to ac
                          tivate conda environment 'imginav'" >&2;    conda act
                          ivate scenefactor || {;      echo "Failed to activate
                           any conda environment" >&2;      exit 1;    };  };fi
                          ; # =================================================
                          ============================;# RUN;# ================
                          =====================================================
                          ========;echo "======================================
                          ====";echo "Array job index: ${LSB_JOBINDEX}";echo "R
                          unning Diffusion experiment";echo "Config: ${CONFIG_F
                          ILE}";echo "Start: $(date)";echo "===================
                          ======================="; python "${PYTHON_SCRIPT}" -
                          -config "${CONFIG_FILE}"; echo "=====================
                          =====================";echo "Experiment COMPLETE";ech
                          o "End: $(date)";echo "==============================
                          ============">, Share group charged </s233249>, Esub 
                          <dcc>
Tue Oct 28 15:41:37 2025: Submitted from host <gbarlogin1>, CWD </work3/s233249
                          /ImgiNav/ImgiNav/training/hpc_scripts>, Output File <
                          /work3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/l
                          ogs/diffusion_sweep.26695129.5.out>, Error File </wor
                          k3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/
                          diffusion_sweep.26695129.5.err>, 8 Task(s), Requested
                           Resources <rusage[mem=48000] span[hosts=1]>, Request
                          ed GPU <num=1>;
Tue Oct 28 15:41:37 2025: Reserved <2> job slots on host(s) <2*n-62-13-16> for 
                          <2> tasks;
Tue Oct 28 16:02:40 2025: Job will start no sooner than indicated time stamp;
 PENDING REASONS:
 Job's requirements for reserving resource (ngpus_physical) not satisfied: 3 ho
                          sts;

 RUNLIMIT                
 1440.0 min

 MEMLIMIT
   46.8 G 

 PENDING TIME DETAILS:
 Eligible pending time (seconds):	47
 Ineligible pending time (seconds):	0

 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

 RESOURCE REQUIREMENT DETAILS:
 Job-level: rusage[mem=48000] span[hosts=1]
 App-level: -
 Queue-level: same[type:model] affinity[core] cu[type=enclosure:maxcus=1]
 Combined: select[(ngpus>0) && (type == local)] order[-slots:-maxslots] rusage[
                          mem=48000.00:ngpus_physical=1.00] span[hosts=1] same[
                          type:model] cu[pref=config:maxcus=1:type=enclosure] a
                          ffinity[core(1)*1]
 Effective: -

 GPU REQUIREMENT DETAILS:
 Combined: num=1:mode=exclusive_process:mps=no:j_exclusive=yes:aff=no:gvendor=n
                          vidia
 Effective: -
------------------------------------------------------------------------------

Job <26695129[6]>, Job Name <diffusion_sweep[6]>, User <s233249>, Project <defa
                          ult>, Status <PEND>, Queue <gpul40s>, Command <#!/bin
                          /bash;#BSUB -J diffusion_sweep[1-6];#BSUB -o /work3/s
                          233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/diff
                          usion_sweep.%J.%I.out;#BSUB -e /work3/s233249/ImgiNav
                          /ImgiNav/training/hpc_scripts/logs/diffusion_sweep.%J
                          .%I.err;#BSUB -n 8;#BSUB -R "rusage[mem=48000]";#BSUB
                           -gpu "num=1";#BSUB -W 24:00;#BSUB -q gpul40s; set -e
                          uo pipefail; # ======================================
                          =======================================;# PATHS;# ===
                          =====================================================
                          =====================;BASE_DIR="/work3/s233249/ImgiNa
                          v/ImgiNav";PYTHON_SCRIPT="${BASE_DIR}/training/train_
                          diffusion.py";CONFIG_DIR="${BASE_DIR}/config/architec
                          ture/diffusion"; # Ordered list of YAMLs for uncondit
                          ioned diffusion sweep;CONFIG_FILES=(;  "${CONFIG_DIR}
                          /E1_linear_64.yaml";  "${CONFIG_DIR}/E2_cosine_64.yam
                          l";  "${CONFIG_DIR}/E3_quadratic_64.yaml";  "${CONFIG
                          _DIR}/E4_linear_128.yaml";  "${CONFIG_DIR}/E5_cosine_
                          128.yaml";  "${CONFIG_DIR}/E6_quadratic_128.yaml";); 
                          # Pick config for this array index;CONFIG_FILE="${CON
                          FIG_FILES[$((LSB_JOBINDEX-1))]}"; # =================
                          =====================================================
                          =======;# MODULES;# =================================
                          ============================================;module l
                          oad cuda/11.8;module load cudnn/v8.6.0.163-prod-cuda-
                          11.X;export MKL_INTERFACE_LAYER=LP64; # =============
                          =====================================================
                          ===========;# CONDA ENV;# ===========================
                          ==================================================;if
                           [ -f "$HOME/miniconda3/etc/profile.d/conda.sh" ]; th
                          en;  source "$HOME/miniconda3/etc/profile.d/conda.sh"
                          ;  conda activate imginav || {;    echo "Failed to ac
                          tivate conda environment 'imginav'" >&2;    conda act
                          ivate scenefactor || {;      echo "Failed to activate
                           any conda environment" >&2;      exit 1;    };  };fi
                          ; # =================================================
                          ============================;# RUN;# ================
                          =====================================================
                          ========;echo "======================================
                          ====";echo "Array job index: ${LSB_JOBINDEX}";echo "R
                          unning Diffusion experiment";echo "Config: ${CONFIG_F
                          ILE}";echo "Start: $(date)";echo "===================
                          ======================="; python "${PYTHON_SCRIPT}" -
                          -config "${CONFIG_FILE}"; echo "=====================
                          =====================";echo "Experiment COMPLETE";ech
                          o "End: $(date)";echo "==============================
                          ============">, Share group charged </s233249>, Esub 
                          <dcc>
Tue Oct 28 15:41:37 2025: Submitted from host <gbarlogin1>, CWD </work3/s233249
                          /ImgiNav/ImgiNav/training/hpc_scripts>, Output File <
                          /work3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/l
                          ogs/diffusion_sweep.26695129.6.out>, Error File </wor
                          k3/s233249/ImgiNav/ImgiNav/training/hpc_scripts/logs/
                          diffusion_sweep.26695129.6.err>, 8 Task(s), Requested
                           Resources <rusage[mem=48000] span[hosts=1]>, Request
                          ed GPU <num=1>;
Tue Oct 28 15:41:37 2025: Reserved <2> job slots on host(s) <2*n-62-13-16> for 
                          <2> tasks;
Tue Oct 28 16:02:40 2025: Job will start no sooner than indicated time stamp;
 PENDING REASONS:
 Job's requirements for reserving resource (ngpus_physical) not satisfied: 3 ho
                          sts;

 RUNLIMIT                
 1440.0 min

 MEMLIMIT
   46.8 G 

 PENDING TIME DETAILS:
 Eligible pending time (seconds):	47
 Ineligible pending time (seconds):	0

 SCHEDULING PARAMETERS:
           r15s   r1m  r15m   ut      pg    io   ls    it    tmp    swp    mem
 loadSched   -     -     -     -       -     -    -     -     -      -      -  
 loadStop    -     -     -     -       -     -    -     -     -      -      -  

 RESOURCE REQUIREMENT DETAILS:
 Job-level: rusage[mem=48000] span[hosts=1]
 App-level: -
 Queue-level: same[type:model] affinity[core] cu[type=enclosure:maxcus=1]
 Combined: select[(ngpus>0) && (type == local)] order[-slots:-maxslots] rusage[
                          mem=48000.00:ngpus_physical=1.00] span[hosts=1] same[
                          type:model] cu[pref=config:maxcus=1:type=enclosure] a
                          ffinity[core(1)*1]
 Effective: -

 GPU REQUIREMENT DETAILS:
 Combined: num=1:mode=exclusive_process:mps=no:j_exclusive=yes:aff=no:gvendor=n
                          vidia
 Effective: -

