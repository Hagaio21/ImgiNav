# Conditioned Diffusion Experiment Configuration
# This file contains all parameters for conditional training with POV + Graph

experiment:
  exp_dir: /work3/s233249/ImgiNav/experiments/conditioned_diffusion_001
  name: "Conditioned Diffusion - POV + Graph - Run 001"
  description: "Training conditional diffusion with room POV and scene graphs"

# U-Net Configuration
unet:
  config_path: /work3/s233249/ImgiNav/ImgiNav/config/base_unet_medium_cond.yml

# Noise Scheduler
scheduler:
  type: CosineScheduler  # LinearScheduler, CosineScheduler, SquaredCosineScheduler
  num_steps: 500

# Autoencoder (for generating samples during training)
autoencoder:
  config_path: /work3/s233249/ImgiNav/experiments/ae_configs/config_diff_4ch_64x64_vanilla.yml
  checkpoint_path: /work3/s233249/ImgiNav/experiments/autoencoder_final_64x64x4_vanila/20251004-082051_ae_final/best.pt

# Dataset - NOTE: Not used in conditioned version, uses manifests from CLI args instead
dataset:
  manifest_path: null  # Placeholder - room_manifest and scene_manifest passed via CLI

# Latent shape (C, H, W)
latent_shape: [4, 64, 64]

# Training Parameters
training:
  batch_size: 64
  num_epochs: 200
  learning_rate: 0.00001
  sample_every: 5      # Generate samples every N epochs
  num_samples: 8       # Number of samples to generate (fixed for comparison)
  periodic_checkpoint_every: 10  # Save lightweight checkpoint every N epochs

# Usage:
# python train_conditioned_diffusion.py \
#   --exp_config experiment_config_conditioned.yaml \
#   --room_manifest /path/to/room_manifest.csv \
#   --scene_manifest /path/to/scene_manifest.csv \
#   --mixer_type concat  # or 'weighted' or 'learned'

# To resume:
# python train_conditioned_diffusion.py \
#   --exp_config experiment_config_conditioned.yaml \
#   --room_manifest /path/to/room_manifest.csv \
#   --scene_manifest /path/to/scene_manifest.csv \
#   --mixer_type concat \
#   --resume

# Experiment directory structure:
#   checkpoints/           (latest.pt, best.pt, epoch_*.pt)
#   samples/               (epoch_XXXX_samples.png - top=generated, bottom=target)
#   configs/               (copy of unet config)
#   logs/                  (training_stats.json, training_curves.png)
#   experiment_config.yaml (copy of this file)
#   trained_on.txt         (train split manifest)
#   evaluated_on.txt       (validation split manifest)
#   fixed_indices.pt       (indices of fixed validation samples)

# Notes:
# - The conditioned version requires TWO manifests (room + scene)
# - Room manifest must have: SCENE_ID, ROOM_ID, POV_TYPE, POV_PATH, POV_EMBEDDING_PATH,
#                            ROOM_GRAPH_PATH, ROOM_GRAPH_EMBEDDING_PATH, 
#                            ROOM_LAYOUT_PATH, ROOM_LAYOUT_EMBEDDING_PATH
# - Scene manifest must have: SCENE_ID, GRAPH_PATH, GRAPH_EMBEDDING_PATH,
#                             LAYOUT_PATH, LAYOUT_EMBEDDING_PATH
# - Both room and scene samples are unified into a single dataset
# - Room samples have POV conditioning, scene samples use graph only