dataset:
  manifest_path: "/work3/s233249/ImgiNav/datasets/training_manifest.csv"
  sample_type: "room"  # 'room', 'scene', or 'both'
  pov_type: "seg"  # 'seg', 'tex', or null (to use both)
  taxonomy_path: "/work3/s233249/ImgiNav/ImgiNav/config/taxonomy.json"
  batch_size: 64
  num_workers: 8
  seed: 1234

model:
  latent_base: 64
  mixer: "mlp"                      # "linear" or "mlp"
  mixer_hidden_dim: 16384
  
  embedders:
    pov: "resnet18"
    graph: "sentence-transformers/all-MiniLM-L6-v2"
  
  autoencoder:
    config: "/work3/s233249/ImgiNav/experiments/VAE/VAE_512_64x64x4/autoencoder_config.yaml"
    ckpt: "/work3/s233249/ImgiNav/experiments/VAE/VAE_512_64x64x4/checkpoints/ae_epoch_40.pt"
  
  diffusion:
    scheduler: "cosine"             # "linear" or "cosine"
    num_steps: 500
    latent_channels: 4
    unet_config: "/work3/s233249/ImgiNav/ImgiNav/config/architecture/unets/medium_c.yml"

training:
  epochs: 10
  lr: 0.0001
  weight_decay: 0.01
  grad_clip: 1.0
  mixed_precision: true
  ema_decay: 0.999
  
  # Classifier-Free Guidance (CFG)
  cfg_scale: 7.5                    # Guidance scale for sampling (1.0 = disabled, 7.5 = standard)
  
  # Conditional Dropout for CFG Training
  # During training, randomly drop conditions to learn unconditional distribution
  cond_dropout_pov: 0.1             # 10% chance to drop POV only
  cond_dropout_graph: 0.1           # 10% chance to drop graph only  
  cond_dropout_both: 0.1            # 10% chance to drop BOTH (unconditional training)
  
  # Ablation Mode
  use_modalities: "both"        # "both" | "pov_only" | "graph_only" | "none"
  
  # Logging & Evaluation
  log_interval: 500
  eval_interval: 500
  sample_interval: 500
  eval_sample_num: 8
  
  # Output Paths
  experiment_dir: "/work3/s233249/ImgiNav/experiments/Pipeline"