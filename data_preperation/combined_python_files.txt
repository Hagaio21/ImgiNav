================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\build_graphs.py
================================================================================
#!/usr/bin/env python3
"""
build_graphs.py

Constructs scene-level and room-level graphs from parquet point cloud data.

Inputs:
  - semantic_maps.json (must be located in dataset root or parent folder)
  - <scene_id>/<scene_id>_scene_info.json
  - <scene_id>/rooms/<room_id>/<scene_id>_<room_id>.parquet
  - <scene_id>/rooms/<room_id>/<scene_id>_<room_id>_meta.json

Outputs:
  - <scene_id>/<scene_id>_graph.json
  - <scene_id>/rooms/<room_id>/<scene_id>_<room_id>_graph.json
"""

import argparse, sys, json, re, csv
from pathlib import Path
from typing import Optional, Dict, List
import numpy as np
import pandas as pd

# ----------------- helpers -----------------

def find_semantic_maps_json(start: Path) -> Optional[Path]:
    for p in [start, *start.parents]:
        cand = p / "semantic_maps.json"
        if cand.exists():
            return cand
    return None

def load_semantic_maps(maps_path: Path) -> Dict:
    return json.loads(maps_path.read_text(encoding="utf-8"))

def center_and_bbox(df: pd.DataFrame):
    coords = df[["x","y","z"]].to_numpy(dtype=np.float64)
    if coords.shape[0] == 0:
        return [0,0,0], [[0,0,0],[0,0,0]]
    center = coords.mean(axis=0).tolist()
    mins = coords.min(axis=0).tolist()
    maxs = coords.max(axis=0).tolist()
    return center, [mins, maxs]

def bbox_overlap(b1, b2, tol=0.0):
    """Check if 2 axis-aligned bounding boxes overlap with tolerance."""
    for i in range(3):
        if b1[1][i] < b2[0][i] - tol: return False
        if b2[1][i] < b1[0][i] - tol: return False
    return True

def xy_overlap(b1, b2, tol=0.0):
    """Check XY overlap only (ignore Z)."""
    for i in [0,1]:
        if b1[1][i] < b2[0][i] - tol: return False
        if b2[1][i] < b1[0][i] - tol: return False
    return True

def objects_from_parquet(parquet_path: Path, maps: Dict) -> List[Dict]:
    df = pd.read_parquet(parquet_path)
    objs = []
    for oid, g in df.groupby("label_id"):
        center, bbox = center_and_bbox(g)
        label = None
        for k,v in maps.get("label2id", {}).items():
            if v == oid:
                label = k
                break
        objs.append({
            "object_id": f"{parquet_path.stem}_{oid}",
            "label_id": int(oid),
            "label": label or "unknown",
            "center": center,
            "bbox": bbox
        })
    return objs

def build_room_graph(scene_id: str, room_id: str, room_dir: Path, maps: Dict,
                     near_thresh: float, overlap_tol: float, above_gap: float) -> Dict:
    parquet_files = list(room_dir.glob(f"{scene_id}_{room_id}.parquet"))
    if not parquet_files:
        return {}
    parquet_path = parquet_files[0]
    objects = objects_from_parquet(parquet_path, maps)

    edges = []
    for i, a in enumerate(objects):
        for j, b in enumerate(objects):
            if j <= i: continue
            ca, cb = np.array(a["center"]), np.array(b["center"])
            ba, bb = a["bbox"], b["bbox"]

            # near
            d = np.linalg.norm(ca - cb)
            if d < near_thresh:
                edges.append({
                    "obj_a": a["object_id"],
                    "obj_b": b["object_id"],
                    "relation": "near",
                    "distance": float(d)
                })

            # overlap
            if bbox_overlap(ba, bb, tol=overlap_tol):
                edges.append({
                    "obj_a": a["object_id"],
                    "obj_b": b["object_id"],
                    "relation": "overlap"
                })

            # vertical relations
            if xy_overlap(ba, bb, tol=overlap_tol):
                if ba[0][2] >= bb[1][2] - overlap_tol:
                    dz = ba[0][2] - bb[1][2]
                    if abs(dz) <= above_gap:
                        edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "on_top_of"})
                    elif dz > above_gap:
                        edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "above"})
                if bb[0][2] >= ba[1][2] - overlap_tol:
                    dz = bb[0][2] - ba[1][2]
                    if abs(dz) <= above_gap:
                        edges.append({"obj_a": b["object_id"], "obj_b": a["object_id"], "relation": "on_top_of"})
                    elif dz > above_gap:
                        edges.append({"obj_a": b["object_id"], "obj_b": a["object_id"], "relation": "above"})

            # directional (simple global x/y comparison)
            if abs(ca[0]-cb[0]) > abs(ca[1]-cb[1]):
                if ca[0] < cb[0]:
                    edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "left_of"})
                else:
                    edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "right_of"})
            else:
                if ca[1] < cb[1]:
                    edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "front_of"})
                else:
                    edges.append({"obj_a": a["object_id"], "obj_b": b["object_id"], "relation": "behind"})

    graph = {
        "scene_id": scene_id,
        "room_id": room_id,
        "objects": objects,
        "edges": edges
    }
    outp = room_dir / f"{scene_id}_{room_id}_graph.json"
    outp.write_text(json.dumps(graph, indent=2), encoding="utf-8")
    print(f"  â†³ wrote room graph: {outp}")
    return graph

def build_scene_graph(scene_id: str, scene_dir: Path, maps: Dict,
                      adjacent_thresh: float, overlap_tol: float) -> Dict:
    room_graphs = []
    rooms_root = scene_dir / "rooms"
    if not rooms_root.exists():
        return {}

    for room_id_dir in sorted(rooms_root.iterdir()):
        if not room_id_dir.is_dir():
            continue
        room_id = room_id_dir.name
        rg_path = room_id_dir / f"{scene_id}_{room_id}_graph.json"
        if not rg_path.exists():
            continue
        try:
            if rg_path.stat().st_size == 0:
                print(f"[warn] empty room graph file skipped: {rg_path}")
                continue
            rg = json.loads(rg_path.read_text(encoding="utf-8"))
        except Exception as e:
            print(f"[warn] failed to load room graph {rg_path}: {e}")
            continue

        if rg and rg.get("objects"):
            all_centers = np.array([o["center"] for o in rg["objects"]])
            center = all_centers.mean(axis=0).tolist()
            # compute room bbox
            mins = np.min([o["bbox"][0] for o in rg["objects"]], axis=0).tolist()
            maxs = np.max([o["bbox"][1] for o in rg["objects"]], axis=0).tolist()
            room_graphs.append({
                "room_id": room_id,
                "center": center,
                "bbox": [mins, maxs]
            })

    edges = []
    for i, a in enumerate(room_graphs):
        for j, b in enumerate(room_graphs):
            if j <= i: continue
            ca, cb = np.array(a["center"]), np.array(b["center"])
            d = np.linalg.norm(ca - cb)

            # adjacent
            if d < adjacent_thresh:
                edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "adjacent", "distance": float(d)})

            # overlap / connected
            if bbox_overlap(a["bbox"], b["bbox"], tol=overlap_tol):
                edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "overlap"})
            else:
                # check if touching (faces within tol)
                touching = any(abs(a["bbox"][1][k]-b["bbox"][0][k]) <= overlap_tol or
                               abs(b["bbox"][1][k]-a["bbox"][0][k]) <= overlap_tol for k in range(3))
                if touching:
                    edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "connected"})

            # directional
            if abs(ca[0]-cb[0]) > abs(ca[1]-cb[1]):
                if ca[0] < cb[0]:
                    edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "left_of"})
                else:
                    edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "right_of"})
            else:
                if ca[1] < cb[1]:
                    edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "front_of"})
                else:
                    edges.append({"room_a": a["room_id"], "room_b": b["room_id"], "relation": "behind"})

    graph = {
        "scene_id": scene_id,
        "rooms": room_graphs,
        "edges": edges
    }
    outp = scene_dir / f"{scene_id}_graph.json"
    outp.write_text(json.dumps(graph, indent=2), encoding="utf-8")
    print(f"âœ” wrote scene graph: {outp}")
    return graph

# ----------------- main -----------------

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_dir", required=True, help="Root folder containing scenes/<scene_id>")
    ap.add_argument("--manifest", type=str,
        help="Optional manifest CSV listing files to process (overrides auto discovery)")

    # thresholds
    ap.add_argument("--near-thresh", type=float, default=2.0)
    ap.add_argument("--adjacent-thresh", type=float, default=5.0)
    ap.add_argument("--overlap-tol", type=float, default=0.1)
    ap.add_argument("--above-gap", type=float, default=0.5)

    args = ap.parse_args()

    in_dir = Path(args.in_dir)
    maps_path = find_semantic_maps_json(in_dir)
    if maps_path is None:
        print("semantic_maps.json not found.", file=sys.stderr)
        sys.exit(2)
    maps = load_semantic_maps(maps_path)

    scenes = []
    if args.manifest:
        with open(args.manifest, newline='', encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if "scene_id" in row and row["scene_id"]:
                    scenes.append(row["scene_id"])
    else:
        for p in in_dir.rglob("*_scene_info.json"):
            scenes.append(p.stem.replace("_scene_info",""))

    if not scenes:
        print("No scenes found.", file=sys.stderr)
        sys.exit(2)

    for sid in scenes:
        scene_dir = in_dir / sid
        if not scene_dir.exists():
            continue
        print(f"Processing scene {sid} ...")

        # build room graphs
        rooms_root = scene_dir / "rooms"
        if rooms_root.exists():
            for room_id_dir in sorted(rooms_root.iterdir()):
                if not room_id_dir.is_dir():
                    continue
                build_room_graph(
                    sid, room_id_dir.name, room_id_dir, maps,
                    near_thresh=args.near_thresh,
                    overlap_tol=args.overlap_tol,
                    above_gap=args.above_gap
                )

        # build scene graph
        build_scene_graph(
            sid, scene_dir, maps,
            adjacent_thresh=args.adjacent_thresh,
            overlap_tol=args.overlap_tol
        )

if __name__ == "__main__":
    main()




================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\collect_all.py
================================================================================
#!/usr/bin/env python3
"""
collect_all.py

Creates a manifest for conditional diffusion training:
- Each row represents one training sample
- Links POV â†’ Graph Text â†’ Layout
- Filters out empty rooms based on pov_manifest
- Includes BOTH seg and tex POVs with pov_type column
"""

import argparse
import json
import csv
from pathlib import Path
from tqdm import tqdm


def load_empty_rooms(pov_manifest_path: str):
    """
    Load set of empty rooms from POV manifest.
    Returns set of (scene_id, room_id) tuples that are empty.
    """
    empty_rooms = set()
    
    with open(pov_manifest_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            if row['is_empty'] == '1':
                empty_rooms.add((row['scene_id'], row['room_id']))
    
    return empty_rooms


def create_manifest(data_root: str, pov_manifest: str, output_manifest: str):
    """
    Create training manifest with columns:
    - sample_id: unique identifier
    - scene_id: scene UUID
    - room_id: room ID (empty for scene-level)
    - sample_type: 'room' or 'scene'
    - pov_type: 'seg' or 'tex' (empty for scene-level)
    - pov_image: path to POV image (empty for scene-level)
    - pov_embedding: path to POV embedding (empty for scene-level)
    - graph_text: path to graph text file
    - graph_embedding: path to graph embedding (if exists)
    - layout_image: path to layout image
    - layout_embedding: path to layout embedding (if exists)
    """
    
    data_root = Path(data_root)
    
    # Load empty rooms to filter out
    print(f"Loading empty rooms from {pov_manifest}")
    empty_rooms = load_empty_rooms(pov_manifest)
    print(f"Found {len(empty_rooms)} empty room(s) to skip")
    
    samples = []
    skipped_empty = 0
    
    # Find all scene directories
    scene_dirs = [d for d in data_root.iterdir() if d.is_dir()]
    
    print(f"Found {len(scene_dirs)} scenes")
    
    for scene_dir in tqdm(scene_dirs, desc="Processing scenes"):
        scene_id = scene_dir.name
        
        # ================================================================
        # SCENE-LEVEL SAMPLES (no POVs)
        # ================================================================
        scene_graph_json = scene_dir / f"{scene_id}_scene_graph.json"
        scene_graph_txt = scene_dir / f"{scene_id}_scene_graph.txt"
        scene_graph_emb = scene_dir / f"{scene_id}_scene_graph.pt"
        scene_layout_img = scene_dir / "layouts" / f"{scene_id}_scene_layout.png"
        scene_layout_emb = scene_dir / "layouts" / f"{scene_id}_layout_emb.pt"
        
        if scene_graph_txt.exists() and scene_layout_img.exists():
            samples.append({
                'sample_id': f"{scene_id}_scene",
                'scene_id': scene_id,
                'room_id': '',
                'sample_type': 'scene',
                'pov_type': '',
                'pov_image': '',
                'pov_embedding': '',
                'graph_text': str(scene_graph_txt),
                'graph_embedding': str(scene_graph_emb) if scene_graph_emb.exists() else '',
                'layout_image': str(scene_layout_img),
                'layout_embedding': str(scene_layout_emb) if scene_layout_emb.exists() else '',
            })
        
        # ================================================================
        # ROOM-LEVEL SAMPLES (with POVs - BOTH seg and tex)
        # ================================================================
        rooms_dir = scene_dir / "rooms"
        if not rooms_dir.exists():
            continue
        
        for room_dir in rooms_dir.iterdir():
            if not room_dir.is_dir():
                continue
            
            room_id = room_dir.name
            
            # Skip empty rooms
            if (scene_id, room_id) in empty_rooms:
                skipped_empty += 1
                continue
            
            # Room graph and layout
            room_graph_json = room_dir / "layouts" / f"{scene_id}_{room_id}_graph.json"
            room_graph_txt = room_dir / "layouts" / f"{scene_id}_{room_id}_graph.txt"
            room_layout_img = room_dir / "layouts" / f"{scene_id}_{room_id}_room_seg_layout.png"
            
            if not room_graph_txt.exists() or not room_layout_img.exists():
                continue
            
            # Process BOTH POV types: seg and tex
            for pov_type in ['seg', 'tex']:
                pov_dir = room_dir / "povs" / pov_type
                
                if not pov_dir.exists():
                    continue
                
                # Get all POV variants (v01, v02, etc.) for this type
                pov_images = sorted(pov_dir.glob(f"{scene_id}_{room_id}_v*_pov_{pov_type}.png"))
                
                for pov_img in pov_images:
                    # Extract viewpoint ID (v01, v02, etc.)
                    viewpoint = pov_img.stem.split('_')[-3]  # e.g., 'v01'
                    
                    pov_emb = pov_img.with_suffix('.pt')
                    
                    sample_id = f"{scene_id}_{room_id}_{pov_type}_{viewpoint}"
                    
                    samples.append({
                        'sample_id': sample_id,
                        'scene_id': scene_id,
                        'room_id': room_id,
                        'sample_type': 'room',
                        'pov_type': pov_type,
                        'pov_image': str(pov_img),
                        'pov_embedding': str(pov_emb) if pov_emb.exists() else '',
                        'graph_text': str(room_graph_txt),
                        'graph_embedding': '',
                        'layout_image': str(room_layout_img),
                        'layout_embedding': '',
                    })
    
    # Write manifest
    output_path = Path(output_manifest)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    fieldnames = [
        'sample_id',
        'scene_id', 
        'room_id',
        'sample_type',
        'pov_type',
        'pov_image',
        'pov_embedding',
        'graph_text',
        'graph_embedding',
        'layout_image',
        'layout_embedding'
    ]
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(samples)
    
    # Print statistics
    room_samples_seg = sum(1 for s in samples if s['sample_type'] == 'room' and s['pov_type'] == 'seg')
    room_samples_tex = sum(1 for s in samples if s['sample_type'] == 'room' and s['pov_type'] == 'tex')
    scene_samples = sum(1 for s in samples if s['sample_type'] == 'scene')
    
    print(f"\nâœ“ Created manifest with {len(samples)} total samples")
    print(f"  - Room-level samples (seg POVs): {room_samples_seg}")
    print(f"  - Room-level samples (tex POVs): {room_samples_tex}")
    print(f"  - Scene-level samples: {scene_samples}")
    print(f"  - Skipped empty rooms: {skipped_empty}")
    print(f"âœ“ Output: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Create training manifest for conditional diffusion pipeline"
    )
    parser.add_argument(
        "--data-root",
        required=True,
        help="Root directory containing scene folders"
    )
    parser.add_argument(
        "--pov-manifest",
        required=True,
        help="Path to POV manifest CSV with is_empty column"
    )
    parser.add_argument(
        "--output",
        required=True,
        help="Output path for training_manifest.csv"
    )
    
    args = parser.parse_args()
    
    create_manifest(
        data_root=args.data_root,
        pov_manifest=args.pov_manifest,
        output_manifest=args.output
    )


if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\collect_dataset.py
================================================================================
import os
import csv
import argparse

def load_empty_map(layouts_csv):
    """Load room emptiness info from layouts.csv into dict[(scene_id, room_id)] = is_empty"""
    empty_map = {}
    if not os.path.isfile(layouts_csv):
        return empty_map
    with open(layouts_csv, newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            sid = row.get("scene_id", "")
            rid = row.get("room_id", "")
            try:
                empty = int(row.get("is_empty", "0"))
            except ValueError:
                empty = 0
            empty_map[(sid, rid)] = empty
    return empty_map


def collect_povs(root, out_csv, empty_map):
    rows = []
    for scene_id in os.listdir(root):
        scene_dir = os.path.join(root, scene_id)
        if not os.path.isdir(scene_dir):
            continue
        rooms_dir = os.path.join(scene_dir, "rooms")
        if not os.path.isdir(rooms_dir):
            continue

        for room_id in os.listdir(rooms_dir):
            povs_dir = os.path.join(rooms_dir, room_id, "povs")
            if not os.path.isdir(povs_dir):
                continue
            for pov_type in ("seg", "tex"):
                tdir = os.path.join(povs_dir, pov_type)
                if not os.path.isdir(tdir):
                    continue
                for f in os.listdir(tdir):
                    if not f.endswith(".png"):
                        continue
                    path = os.path.join(tdir, f)
                    is_empty = empty_map.get((scene_id, room_id), 0)
                    rows.append([scene_id, room_id, pov_type, path, is_empty])

    with open(out_csv, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["scene_id", "room_id", "type", "pov_path", "is_empty"])
        writer.writerows(rows)


def collect_data(root, out_csv):
    rows = []

    for scene_id in os.listdir(root):
        scene_dir = os.path.join(root, scene_id)
        if not os.path.isdir(scene_dir):
            continue

        # --- scene-level files ---
        for f in os.listdir(scene_dir):
            path = os.path.join(scene_dir, f)
            if not os.path.isfile(path):
                continue

            if f.endswith("_scene_info.json"):
                cat = "scene_info"
            elif f.endswith("_sem_pointcloud.parquet"):
                cat = "scene_parquet"
            elif f.endswith("_scene_layout.png"):
                cat = "scene_layout"
            else:
                cat = "other"
            rows.append([scene_id, "", cat, path])

        # --- rooms ---
        rooms_dir = os.path.join(scene_dir, "rooms")
        if not os.path.isdir(rooms_dir):
            continue

        for room_id in os.listdir(rooms_dir):
            room_dir = os.path.join(rooms_dir, room_id)
            if not os.path.isdir(room_dir):
                continue

            for f in os.listdir(room_dir):
                path = os.path.join(room_dir, f)
                if not os.path.isfile(path):
                    continue
                if f.endswith(".parquet"):
                    cat = "room_parquet"
                elif f.endswith("_meta.json"):
                    cat = "room_meta"
                else:
                    cat = "other"
                rows.append([scene_id, room_id, cat, path])

            # --- room layouts ---
            layouts_dir = os.path.join(room_dir, "layouts")
            if os.path.isdir(layouts_dir):
                for f in os.listdir(layouts_dir):
                    path = os.path.join(layouts_dir, f)
                    if f.endswith("_room_seg_layout.png"):
                        cat = "room_layout_seg"
                    else:
                        cat = "other"
                    rows.append([scene_id, room_id, cat, path])

            # --- povs ---
            povs_dir = os.path.join(room_dir, "povs")
            if os.path.isdir(povs_dir):
                for f in os.listdir(povs_dir):
                    path = os.path.join(povs_dir, f)
                    if f.endswith("_pov_meta.json"):
                        cat = "pov_meta"
                    elif f.endswith("_minimap.png"):
                        cat = "pov_minimap"
                    else:
                        cat = "other"
                    if os.path.isfile(path):
                        rows.append([scene_id, room_id, cat, path])

                for pov_type in ("seg", "tex"):
                    tdir = os.path.join(povs_dir, pov_type)
                    if not os.path.isdir(tdir):
                        continue
                    for f in os.listdir(tdir):
                        path = os.path.join(tdir, f)
                        if not f.endswith(".png"):
                            cat = "other"
                        else:
                            cat = "pov_seg" if pov_type == "seg" else "pov_tex"
                        rows.append([scene_id, room_id, cat, path])

    # --- write CSV ---
    with open(out_csv, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["scene_id", "room_id", "category", "file_path"])
        writer.writerows(rows)


def main():
    parser = argparse.ArgumentParser(description="Collect POVs and full dataset manifest.")
    parser.add_argument("--root", required=True, help="Dataset root directory")
    parser.add_argument("--out", required=True, help="Output directory for CSVs")
    parser.add_argument("--layouts", required=True, help="Path to layouts.csv (for is_empty info)")
    args = parser.parse_args()

    os.makedirs(args.out, exist_ok=True)

    empty_map = load_empty_map(args.layouts)

    povs_csv = os.path.join(args.out, "povs.csv")
    data_csv = os.path.join(args.out, "data.csv")

    collect_povs(args.root, povs_csv, empty_map)
    collect_data(args.root, data_csv)


if __name__ == "__main__":
    main()




================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\collect_graphs.py
================================================================================
#!/usr/bin/env python3
"""
collect_graph_manifest.py

Scans filesystem for existing graph JSON files and creates manifest.
"""

import argparse
import csv
from pathlib import Path


def collect_graphs(root_dir: Path):
    """Scan filesystem for graph JSON files that exist."""
    graphs = []
    
    print("Scanning for graph files...")
    
    for graph_path in root_dir.rglob("*_graph.json"):
        filename = graph_path.stem
        
        if filename.endswith("_scene_graph"):
            scene_id = filename.replace("_scene_graph", "")
            graph_type = "scene"
            room_id = "scene"
        else:
            parts = filename.replace("_graph", "").split("_")
            if len(parts) >= 2:
                room_id = parts[-1]
                scene_id = "_".join(parts[:-1])
                graph_type = "room"
            else:
                continue
        
        if graph_type == "scene":
            layout_filename = f"{scene_id}_scene_layout.png"
        else:
            layout_filename = f"{scene_id}_{room_id}_room_seg_layout.png"
        
        layout_path = graph_path.parent / layout_filename
        
        graphs.append({
            'scene_id': scene_id,
            'type': graph_type,
            'room_id': room_id,
            'layout_path': str(layout_path) if layout_path.exists() else '',
            'graph_path': str(graph_path),
            'is_empty': 'false'
        })
    
    return graphs


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--root", required=True)
    parser.add_argument("--output", required=True)
    args = parser.parse_args()
    
    root_dir = Path(args.root)
    output_path = Path(args.output)
    
    if not root_dir.exists():
        print(f"[error] Directory not found: {root_dir}")
        return
    
    graphs = collect_graphs(root_dir)
    
    if not graphs:
        print("[error] No graphs found")
        return
    
    scene_count = sum(1 for g in graphs if g['type'] == 'scene')
    room_count = sum(1 for g in graphs if g['type'] == 'room')
    
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['scene_id', 'type', 'room_id', 'layout_path', 'graph_path', 'is_empty']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(graphs)
    
    print(f"\nTotal graphs: {len(graphs)}")
    print(f"Scene graphs: {scene_count}")
    print(f"Room graphs:  {room_count}")
    print(f"\nManifest: {output_path}")


if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\create_graph_text.py
================================================================================
#!/usr/bin/env python3
"""
create_graph_text.py

Reads graphs.csv manifest, converts graphs to text using graph2text,
and saves each as a .txt file.
"""

import argparse
import csv
import json
from pathlib import Path
from tqdm import tqdm


def load_taxonomy(taxonomy_path: str):
    """Load taxonomy mapping from taxonomy.json."""
    t = json.loads(Path(taxonomy_path).read_text(encoding="utf-8"))
    return {int(k): v for k, v in t.get("id2room", {}).items()}


def articleize(label: str) -> str:
    """Add 'the', 'a', or 'an' before a label depending on plurality."""
    clean = label.strip().replace("_", " ")
    lower = clean.lower()

    # heuristic plural detection
    if lower.endswith(("s", "x", "z", "ch", "sh")) and not lower.endswith(("ss", "us")):
        article = "a"
    else:
        # singular
        vowels = "aeiou"
        article = "an" if lower[0] in vowels else "the"
    return f"{article} {clean}"


def graph2text(graph_path: str, taxonomy: dict, max_edges: int = 10_000):
    """
    Converts either a 3D-FRONT room graph or scene graph JSON to text.
    Uses taxonomy to decode room_id when available.
    Removes underscores and adds articles ('the', 'a', 'an').
    """
    path = Path(graph_path)
    
    if not path.exists():
        return ""
    
    g = json.loads(path.read_text(encoding="utf-8"))

    nodes = g.get("nodes", [])
    edges = g.get("edges", [])
    if not edges:
        return ""

    is_scene_graph = "room_a" in edges[0] or "room_b" in edges[0]

    # build node label map
    id_to_label = {}
    for n in nodes:
        if is_scene_graph:
            rid = n.get("room_id")
            raw_label = taxonomy.get(rid, n.get("room_type", str(rid)))
        else:
            raw_label = n.get("label", n.get("id"))
        id_to_label[n["id"]] = articleize(raw_label)

    sentences = []
    seen = set()

    for e in edges[:max_edges]:
        a = e.get("room_a") if is_scene_graph else e.get("obj_a")
        b = e.get("room_b") if is_scene_graph else e.get("obj_b")
        if not a or not b:
            continue

        label_a = id_to_label.get(a)
        label_b = id_to_label.get(b)
        if not label_a or not label_b:
            continue

        key = tuple(sorted([label_a, label_b]))
        if key in seen:
            continue
        seen.add(key)

        dist = e.get("distance_relation")
        direc = e.get("direction_relation")

        if dist and direc:
            sentence = f"{label_a} is {dist} and {direc} {label_b}."
        elif dist:
            sentence = f"{label_a} is {dist} {label_b}."
        elif direc:
            sentence = f"{label_a} is {direc} {label_b}."
        else:
            sentence = f"{label_a} relates to {label_b}."

        sentences.append(sentence)

    text = " ".join(sentences)
    return text.replace("_", " ")


def process_graphs(manifest_path: str, taxonomy_path: str):
    """
    Process all graphs in manifest: convert to text and save as .txt files.
    
    Args:
        manifest_path: Path to graphs.csv
        taxonomy_path: Path to taxonomy.json
    """
    # Load taxonomy
    print(f"Loading taxonomy from {taxonomy_path}")
    taxonomy = load_taxonomy(taxonomy_path)
    
    # Read manifest
    print(f"Reading manifest: {manifest_path}")
    manifest_path = Path(manifest_path)
    
    with open(manifest_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    print(f"Found {len(rows)} graphs to process")
    
    # Process each graph
    skipped = 0
    
    for row in tqdm(rows, desc="Creating text files"):
        graph_path = row['graph_path']
        
        try:
            text = graph2text(graph_path, taxonomy)
            
            if not text:
                print(f"Warning: Empty text for {graph_path}")
                skipped += 1
                continue
            
            # Save as .txt file
            txt_path = Path(graph_path).with_suffix('.txt')
            txt_path.write_text(text, encoding='utf-8')
            
        except Exception as e:
            print(f"Error processing {graph_path}: {e}")
            skipped += 1
    
    print(f"\nâœ“ Processed {len(rows) - skipped}/{len(rows)} graphs successfully")
    print(f"âœ“ Skipped {skipped} graphs")


def main():
    parser = argparse.ArgumentParser(
        description="Convert graph JSON files to text files"
    )
    parser.add_argument(
        "--manifest",
        required=True,
        help="Path to graphs.csv manifest"
    )
    parser.add_argument(
        "--taxonomy",
        required=True,
        help="Path to taxonomy.json"
    )
    
    args = parser.parse_args()
    
    process_graphs(
        manifest_path=args.manifest,
        taxonomy_path=args.taxonomy
    )


if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\generate_palette.py
================================================================================
#!/usr/bin/env python3
import json
from pathlib import Path
import colorsys

def assign_colors(label2id: dict) -> dict:
    """Assign deterministic colors for each label id."""
    ids = sorted(int(v) for v in label2id.values())
    palette = {}
    phi = 0.61803398875  # golden ratio
    for i, lid in enumerate(ids):
        h = (lid * phi) % 1.0
        r, g, b = colorsys.hsv_to_rgb(h, 0.65, 0.95)
        palette[str(lid)] = [int(r*255), int(g*255), int(b*255)]
    return palette

def main(json_path: Path):
    data = json.loads(json_path.read_text(encoding="utf-8"))
    if "id2color" in data:
        print("id2color already exists, skipping.")
        return
    palette = assign_colors(data["label2id"])
    data["id2color"] = palette
    json_path.write_text(json.dumps(data, indent=2), encoding="utf-8")
    print(f"âœ” Added id2color to {json_path}")

if __name__ == "__main__":
    root = Path("/work3/s233249/ImgiNav/datasets/scenes/semantic_maps.json")  # adjust path
    main(root)




================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\layout_collection.py
================================================================================
#!/usr/bin/env python3
import argparse
import csv
from pathlib import Path
from PIL import Image
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing


def analyze_layout(img_path: Path, white_vals, gray_vals) -> dict:
    """Analyze one layout file and return row dict."""
    stem = img_path.stem
    parts = stem.split("_")
    scene_id = parts[0]

    if "_scene_" in stem:
        layout_type = "scene"
        room_id = "scene"
    else:
        layout_type = "room"
        room_id = parts[1]

    im = Image.open(img_path).convert("RGB")
    colors = {tuple(rgb) for count, rgb in im.getcolors(maxcolors=1_000_000)}

    is_empty = colors.issubset(white_vals | gray_vals) and len(colors) <= 2

    return {
        "scene_id": scene_id,
        "type": layout_type,
        "room_id": room_id,
        "layout_path": str(img_path.resolve()),
        "is_empty": str(is_empty).lower(),
    }


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--root", required=True, help="Root folder containing layout PNGs")
    ap.add_argument("--out", default="layouts.csv", help="Output CSV path")
    ap.add_argument("--workers", type=int, default=multiprocessing.cpu_count(),
                    help="Number of parallel workers (default: all cores)")
    args = ap.parse_args()

    root = Path(args.root)

    # Adjust once you confirm actual RGB values
    white_vals = {(240, 240, 240), (255, 255, 255)}
    gray_vals = {(200, 200, 200), (211, 211, 211)}

    # Explicit search patterns
    print("[INFO] Scanning for scene layouts...", flush=True)
    scene_files = list(root.rglob("*/layouts/*_scene_layout.png"))
    print(f"[INFO] Found {len(scene_files)} scene layouts", flush=True)

    print("[INFO] Scanning for room layouts...", flush=True)
    room_files = list(root.rglob("*/rooms/*/layouts/*_room_*_layout.png"))
    print(f"[INFO] Found {len(room_files)} room layouts", flush=True)

    img_files = scene_files + room_files
    total = len(img_files)
    print(f"[INFO] Total files to process: {total}", flush=True)
    print(f"[INFO] Using {args.workers} workers", flush=True)

    rows = []
    completed = 0
    with ProcessPoolExecutor(max_workers=args.workers) as ex:
        future_to_file = {ex.submit(analyze_layout, p, white_vals, gray_vals): p for p in img_files}
        for future in as_completed(future_to_file):
            row = future.result()
            rows.append(row)
            completed += 1
            if completed % 500 == 0 or completed == total:
                print(f"[PROGRESS] {completed}/{total} files ({100*completed/total:.1f}%)", flush=True)

    with open(args.out, "w", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=["scene_id", "type", "room_id", "layout_path", "is_empty"])
        writer.writeheader()
        writer.writerows(rows)

    print(f"[INFO] Wrote {len(rows)} rows to {args.out}", flush=True)


if __name__ == "__main__":
    main()




================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\semantic_scene_visualization.py
================================================================================
#!/usr/bin/env python3
"""
Visualize 3D-FRONT scenes with isometric plots: textured and semantic side-by-side.
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from mpl_toolkits.mplot3d import Axes3D
from matplotlib.colors import ListedColormap

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

try:
    from utils.semantic_utils import Taxonomy
except ImportError:
    # Try absolute import if relative doesn't work
    import importlib.util
    spec = importlib.util.spec_from_file_location("semantic_utils", Path(__file__).parent / "utils" / "semantic_utils.py")
    semantic_utils = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(semantic_utils)
    Taxonomy = semantic_utils.Taxonomy


def load_point_cloud(scene_dir: Path, scene_id: str, format: str = "parquet"):
    """Load point cloud from parquet or csv."""
    if format == "parquet":
        file_path = scene_dir / f"{scene_id}_sem_pointcloud.parquet"
        return pd.read_parquet(file_path)
    else:
        file_path = scene_dir / f"{scene_id}_sem_pointcloud.csv"
        return pd.read_csv(file_path)


def create_category_colormap(taxonomy: Taxonomy, categories: List[str]) -> Dict[str, np.ndarray]:
    """Create a consistent color mapping for categories."""
    unique_cats = sorted(set(categories))
    
    # Use a colorblind-friendly palette
    base_colors = plt.cm.tab20c(np.linspace(0, 1, 20))
    extended_colors = plt.cm.Set3(np.linspace(0, 1, 12))
    all_colors = np.vstack([base_colors, extended_colors])
    
    color_map = {}
    for i, cat in enumerate(unique_cats):
        color_map[cat] = all_colors[i % len(all_colors)][:3]  # RGB only
    
    # Special colors for architectural elements
    if 'floor' in color_map:
        color_map['floor'] = np.array([0.8, 0.8, 0.8])
    if 'wall' in color_map:
        color_map['wall'] = np.array([0.9, 0.9, 0.85])
    
    return color_map


def setup_isometric_view(ax: Axes3D, bounds: Dict):
    """Configure axis for isometric view."""
    # Set equal aspect ratio
    x_range = bounds['max'][0] - bounds['min'][0]
    y_range = bounds['max'][1] - bounds['min'][1]
    z_range = bounds['max'][2] - bounds['min'][2]
    
    max_range = max(x_range, y_range, z_range)
    mid_x = (bounds['max'][0] + bounds['min'][0]) / 2
    mid_y = (bounds['max'][1] + bounds['min'][1]) / 2
    mid_z = (bounds['max'][2] + bounds['min'][2]) / 2
    
    ax.set_xlim(mid_x - max_range/2, mid_x + max_range/2)
    ax.set_ylim(mid_y - max_range/2, mid_y + max_range/2)
    ax.set_zlim(mid_z - max_range/2, mid_z + max_range/2)
    
    # Isometric viewing angle (35.264Â° elevation, 45Â° azimuth)
    ax.view_init(elev=25, azim=45)
    
    # Clean up axes
    ax.set_xlabel('X')
    ax.set_ylabel('Y')
    ax.set_zlabel('Z')
    ax.grid(True, alpha=0.3)


def downsample_points(df: pd.DataFrame, max_points: int = 50000) -> pd.DataFrame:
    """Downsample point cloud for faster rendering."""
    if len(df) <= max_points:
        return df
    
    indices = np.random.choice(len(df), max_points, replace=False)
    return df.iloc[indices]


def plot_textured_view(ax: Axes3D, df: pd.DataFrame, bounds: Dict, point_size: float = 5.0):
    """Plot textured (RGB) view."""
    # Normalize RGB values
    colors = np.column_stack([df['r'], df['g'], df['b']]) / 255.0
    
    ax.scatter(df['x'], df['y'], df['z'], 
               c=colors, 
               s=point_size, 
               alpha=0.8,
               edgecolors='none')
    
    setup_isometric_view(ax, bounds)
    ax.set_title('Textured View', fontsize=14, fontweight='bold')


def plot_semantic_view(ax: Axes3D, df: pd.DataFrame, bounds: Dict, 
                       taxonomy: Taxonomy, point_size: float = 1.0):
    """Plot semantic (category-colored) view."""
    # Create color mapping
    color_map = create_category_colormap(taxonomy, df['category'].tolist())
    
    # Assign colors to each point
    colors = np.array([color_map[cat] for cat in df['category']])
    
    ax.scatter(df['x'], df['y'], df['z'], 
               c=colors, 
               s=point_size, 
               alpha=0.8,
               edgecolors='none')
    
    setup_isometric_view(ax, bounds)
    ax.set_title('Semantic View', fontsize=14, fontweight='bold')
    
    # Add legend for top categories
    unique_cats = df['category'].value_counts().head(10).index.tolist()
    legend_elements = [plt.Line2D([0], [0], marker='o', color='w', 
                                   markerfacecolor=color_map[cat], 
                                   markersize=8, label=cat)
                       for cat in unique_cats if cat in color_map]
    
    ax.legend(handles=legend_elements, loc='upper left', 
              bbox_to_anchor=(1.05, 1), fontsize=8)


def visualize_scene(scene_dir: Path, scene_id: str, taxonomy: Taxonomy,
                   output_path: Path = None, max_points: int = 50000,
                   point_size: float = 1.0, format: str = "parquet",
                   figsize: Tuple[int, int] = (16, 8), dpi: int = 150):
    """Create side-by-side isometric visualization."""
    
    # Load data
    print(f"Loading scene {scene_id}...")
    df = load_point_cloud(scene_dir, scene_id, format)
    
    # Load scene info for bounds
    scene_info_path = scene_dir / f"{scene_id}_scene_info.json"
    with open(scene_info_path, 'r') as f:
        scene_info = json.load(f)
    
    # Downsample if needed
    if len(df) > max_points:
        print(f"Downsampling from {len(df)} to {max_points} points...")
        df = downsample_points(df, max_points)
    
    # Create figure with two subplots
    fig = plt.figure(figsize=figsize, dpi=dpi)
    
    # Textured view (left)
    ax1 = fig.add_subplot(121, projection='3d')
    plot_textured_view(ax1, df, scene_info['bounds'], point_size)
    
    # Semantic view (right)
    ax2 = fig.add_subplot(122, projection='3d')
    plot_semantic_view(ax2, df, scene_info['bounds'], taxonomy, point_size)
    
    # Add main title
    fig.suptitle(f'Scene: {scene_id}', fontsize=16, fontweight='bold', y=0.98)
    
    plt.tight_layout()
    
    # Save or show
    if output_path:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_path, bbox_inches='tight', dpi=dpi)
        print(f"Saved visualization to {output_path}")
    else:
        plt.show()
    
    plt.close()


def main():
    parser = argparse.ArgumentParser(
        description="Generate isometric visualizations of 3D-FRONT scenes"
    )
    parser.add_argument("--scene_dir", type=str, required=True,
                       help="Directory containing scene point clouds and metadata")
    parser.add_argument("--scene_id", type=str, required=True,
                       help="Scene ID to visualize")
    parser.add_argument("--taxonomy", type=str, required=True,
                       help="Path to taxonomy file")
    parser.add_argument("--output", type=str, default=None,
                       help="Output image path (if not specified, displays interactively)")
    parser.add_argument("--format", type=str, default="parquet",
                       choices=["parquet", "csv"],
                       help="Point cloud format")
    parser.add_argument("--max_points", type=int, default=50000,
                       help="Maximum points to render (for performance)")
    parser.add_argument("--point_size", type=float, default=1.0,
                       help="Size of points in plot")
    parser.add_argument("--figsize", type=int, nargs=2, default=[16, 8],
                       help="Figure size (width height)")
    parser.add_argument("--dpi", type=int, default=150,
                       help="Output resolution")
    
    args = parser.parse_args()
    
    # Load taxonomy
    taxonomy = Taxonomy(Path(args.taxonomy))
    
    # Visualize scene
    output_path = Path(args.output) if args.output else None
    
    visualize_scene(
        scene_dir=Path(args.scene_dir),
        scene_id=args.scene_id,
        taxonomy=taxonomy,
        output_path=output_path,
        max_points=args.max_points,
        point_size=args.point_size,
        format=args.format,
        figsize=tuple(args.figsize),
        dpi=args.dpi
    )


if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\stage1_build_scenes.py
================================================================================
#!/usr/bin/env python3
"""
Stage 1: Build 3D-FRONT scenes into point clouds + metadata.
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Dict, List

import numpy as np
import pandas as pd
import trimesh
from scipy.spatial.transform import Rotation

# --- imports ---
from utils.semantic_utils import Taxonomy
from utils.utils import (
    gather_paths_from_sources, infer_ids_from_path,
    load_config_with_profile, create_progress_tracker,
    safe_mkdir, write_json
)

# --- Global Taxonomy Object ---
TAXONOMY: Taxonomy = None
ARGS = None

# ---------------------------------------------------------------------
# Utility Functions
# ---------------------------------------------------------------------
def export_outputs(scene_id, output_dir, textured_scene, meshes_info, point_cloud, args, taxonomy):
    """Export all requested formats and scene metadata."""

    # ----- Save GLB -----
    if args.save_glb and textured_scene:
        textured_scene.export(output_dir / f"{scene_id}_textured.glb", file_type="glb")

    # ----- Save OBJ -----
    if args.save_obj and meshes_info:
        with open(output_dir / f"{scene_id}.obj", 'w') as f:
            f.write("# Scene OBJ\n")
            vertex_offset = 1
            for info in meshes_info:
                mesh = info['mesh']
                for v in mesh.vertices:
                    f.write(f"v {v[0]:.6f} {v[1]:.6f} {v[2]:.6f}\n")
                for face in mesh.faces:
                    f.write(f"f {face[0]+vertex_offset} {face[1]+vertex_offset} {face[2]+vertex_offset}\n")
                vertex_offset += len(mesh.vertices)

    # ----- Scene Metadata -----
    if meshes_info:
        all_vertices = np.vstack([info['mesh'].vertices for info in meshes_info])
        scene_info = {
            "bounds": {
                "min": all_vertices.min(axis=0).tolist(),
                "max": all_vertices.max(axis=0).tolist()
            },
            "size": (all_vertices.max(axis=0) - all_vertices.min(axis=0)).tolist(),
            "up_normal": [0.0, 0.0, 1.0]
        }
        write_json(scene_info, output_dir / f"{scene_id}_scene_info.json")

    # ----- Point Cloud Exports -----
    if point_cloud is not None and point_cloud.size > 0:
        xyz = np.column_stack([point_cloud["x"], point_cloud["y"], point_cloud["z"]])
        rgb = np.column_stack([point_cloud["r"], point_cloud["g"], point_cloud["b"]])

        titles = point_cloud["title"].tolist()
        labels = point_cloud["label"].tolist()
        categories = point_cloud["category"].tolist()
        supers = point_cloud["super"].tolist()
        rooms = point_cloud["room_type"].tolist()

        df_data = {
            "x": xyz[:, 0], "y": xyz[:, 1], "z": xyz[:, 2],
            "r": rgb[:, 0], "g": rgb[:, 1], "b": rgb[:, 2],
            "title": titles,
            "label": labels,
            "category": categories,
            "super": supers,
            "room_type": rooms,
            "title_id": point_cloud["title_id"].tolist(),
            "label_id": point_cloud["label_id"].tolist(),
            "category_id": point_cloud["category_id"].tolist(),
            "super_id": point_cloud["super_id"].tolist(),
            "room_id": point_cloud["room_id"].tolist(),
        }


        if args.save_parquet:
            pd.DataFrame(df_data).to_parquet(
                output_dir / f"{scene_id}_sem_pointcloud.parquet", index=False
            )

        if args.save_csv:
            pd.DataFrame(df_data).to_csv(
                output_dir / f"{scene_id}_sem_pointcloud.csv", index=False
            )


def get_point_colors(mesh, points, face_indices):
    """Sample colors from mesh using UV/vertex colors."""
    N = len(points)
    if N == 0 or mesh is None or mesh.is_empty:
        return np.zeros((0, 3), dtype=np.uint8)

    vis = getattr(mesh, "visual", None)

    # Try UV texture sampling
    try:
        uv = getattr(vis, "uv", None) if vis else None
        mat = getattr(vis, "material", None) if vis else None
        img = getattr(mat, "image", None) if mat else None
        if uv is not None and img is not None:
            tris = mesh.triangles[face_indices]
            bary = trimesh.triangles.points_to_barycentric(tris, points)
            faces = mesh.faces[face_indices]
            tri_uv = uv[faces]
            uv_pts = (bary[:, :, None] * tri_uv).sum(axis=1)

            img_np = np.asarray(img.convert("RGB"))
            H, W = img_np.shape[:2]
            u = np.clip(uv_pts[:, 0], 0.0, 1.0) * (W - 1)
            v = (1.0 - np.clip(uv_pts[:, 1], 0.0, 1.0)) * (H - 1)
            ui = np.clip(np.round(u).astype(np.int64), 0, W - 1)
            vi = np.clip(np.round(v).astype(np.int64), 0, H - 1)
            return img_np[vi, ui, :].astype(np.uint8)
    except Exception:
        pass

    # Try vertex colors
    try:
        vcols = getattr(vis, "vertex_colors", None) if vis else None
        if vcols is not None and len(vcols) == len(mesh.vertices):
            tris = mesh.triangles[face_indices]
            bary = trimesh.triangles.points_to_barycentric(tris, points)
            faces = mesh.faces[face_indices]
            tri_vc = vcols[faces][:, :, :3]
            cols = (bary[:, :, None] * tri_vc.astype(np.float32)).sum(axis=1)
            return np.clip(np.round(cols), 0, 255).astype(np.uint8)
    except Exception:
        pass

    # Fallback: flat gray
    return np.full((N, 3), 128, dtype=np.uint8)

def load_mesh(model_dir: Path, jid: str):
    """Load mesh from OBJ or GLB."""
    obj_path = model_dir / jid / "raw_model.obj"
    glb_path = model_dir / jid / "raw_model.glb"

    if obj_path.exists():
        resolver = trimesh.visual.resolvers.FilePathResolver(obj_path.parent)
        return trimesh.load(str(obj_path), force="mesh", process=False,
                            maintain_order=True, resolver=resolver)
    elif glb_path.exists():
        return trimesh.load(str(glb_path), force="mesh", process=False,
                            maintain_order=True)
    else:
        raise FileNotFoundError(f"Model not found: {obj_path} or {glb_path}")

def create_arch_mesh(arch: Dict):
    """Create mesh from architectural JSON data."""
    vertices = np.array(arch["xyz"], dtype=np.float64).reshape(-1, 3)
    faces = np.array(arch["faces"], dtype=np.int64).reshape(-1, 3)
    mesh = trimesh.Trimesh(vertices=vertices, faces=faces, process=True)
    mesh.visual.vertex_colors = np.array([200, 200, 200, 255], dtype=np.uint8)
    return mesh

def build_transform(child: Dict):
    """Build transform matrix from child node."""
    pos = np.array(child.get("pos", [0, 0, 0]), dtype=np.float64)
    rot = np.array(child.get("rot", [0, 0, 0, 1]), dtype=np.float64)
    scl = np.array(child.get("scale", [1, 1, 1]), dtype=np.float64)

    T = np.eye(4, dtype=np.float64)
    T[:3, 3] = pos
    Rm = Rotation.from_quat(rot).as_matrix()
    Sm = np.diag(scl)
    T[:3, :3] = Rm @ Sm
    return T

def sample_points(scene_objects):
    """Sample points from scene objects and attach taxonomy info."""
    areas = [max(0.0, obj['mesh'].area) for obj in scene_objects]
    total_area = sum(a for a in areas if a > 0.0) or 1.0

    all_data = []
    for obj, area in zip(scene_objects, areas):
        if area <= 0.0:
            continue

        if ARGS.ppsm > 0.0:
            n_pts = int(round(area * ARGS.ppsm))
        else:
            n_pts = int(round((area / total_area) * ARGS.total_points))

        n_pts = max(ARGS.min_pts_per_mesh, n_pts)
        if ARGS.max_pts_per_mesh > 0:
            n_pts = min(n_pts, ARGS.max_pts_per_mesh)

        if n_pts <= 0:
            continue

        # ---- sample points ----
        pts_local, face_indices = trimesh.sample.sample_surface(obj['mesh'], n_pts)
        colors = get_point_colors(obj['mesh'], pts_local, face_indices)
        pts_world = trimesh.transform_points(pts_local, obj['transform'])

        title = obj['label']
        room_type = obj.get('room_type', 'UnknownRoom')

        # category
        category_id = TAXONOMY.translate(title, output="id")
        category = TAXONOMY.translate(category_id, output="name") if category_id else "UnknownCategory"

        # super (explicit name + id)
        super_cat = TAXONOMY.get_sup(title, output="name")
        super_id  = TAXONOMY.get_sup(title, output="id")


        # title id
        title_id = TAXONOMY.translate(title, output="id")

        # room
        room_id = TAXONOMY.translate(room_type, output="id")


        all_data.append({
            'xyz': pts_world.astype(np.float32),
            'rgb': colors.astype(np.uint8),
            'title': [title] * n_pts,
            'label': [title] * n_pts,
            'category': [category] * n_pts,
            'super': [super_cat] * n_pts,
            'room_type': [room_type] * n_pts,
            'title_id': [title_id] * n_pts,
            'label_id': [title_id] * n_pts,
            'category_id': [category_id or 0] * n_pts,
            'super_id': [super_id] * n_pts,
            'room_id': [room_id] * n_pts,
        })

    if not all_data:
        return np.array([])

    # ---- combine into structured array ----
    xyz = np.vstack([d['xyz'] for d in all_data])
    rgb = np.vstack([d['rgb'] for d in all_data])

    def flat(key): return sum([d[key] for d in all_data], [])

    dtype = [
        ('x','f4'), ('y','f4'), ('z','f4'),
        ('r','u1'), ('g','u1'), ('b','u1'),
        ('title','U100'), ('label','U100'),
        ('category','U80'), ('super','U80'),
        ('room_type','U50'),
        ('title_id','i4'), ('label_id','i4'),
        ('category_id','i4'), ('super_id','i4'),
        ('room_id','i4'),
    ]

    N = xyz.shape[0]
    structured = np.empty(N, dtype=dtype)
    structured['x'], structured['y'], structured['z'] = xyz.T
    structured['r'], structured['g'], structured['b'] = rgb.T
    structured['title'] = flat('title')
    structured['label'] = flat('label')
    structured['category'] = flat('category')
    structured['super'] = flat('super')
    structured['room_type'] = flat('room_type')
    structured['title_id'] = flat('title_id')
    structured['label_id'] = flat('label_id')
    structured['category_id'] = flat('category_id')
    structured['super_id'] = flat('super_id')
    structured['room_id'] = flat('room_id')

    return structured


# ---------------------------------------------------------------------
# Main Processing
# ---------------------------------------------------------------------
def process_scene(scene_data, model_dir, model_info_map):
    """Process scene into objects and point cloud."""
    failed_models = {}
    scene_objects = []
    furniture_map = {f['uid']: f for f in scene_data.get('furniture', [])}
    arch_map = {m['uid']: m for m in scene_data.get('mesh', [])}

    for room in scene_data.get("scene", {}).get("room", []):
        room_type = room.get("type", "UnknownRoom")  # <-- capture room type once

        for child_index, child in enumerate(room.get("children", [])):
            ref_id = child.get("ref")
            if not ref_id:
                continue

            try:
                mesh, label, model_item = None, "unknown", None

                if ref_id in furniture_map:
                    item_info = furniture_map[ref_id]
                    jid = item_info.get('jid')
                    if not jid:
                        raise ValueError("Missing 'jid'")

                    mesh = load_mesh(model_dir, jid)
                    label = (model_info_map.get(jid, {}).get('category')
                             or item_info.get('title') or "unknown")
                    model_item = item_info

                elif ref_id in arch_map:
                    arch = arch_map[ref_id]
                    if "Ceiling" in arch.get("type", ""):
                        continue
                    mesh = create_arch_mesh(arch)
                    label = 'floor' if 'Floor' in arch.get("type", "") else 'wall'
                else:
                    failed_models[ref_id] = "Reference not found"
                    continue

                if mesh is None or mesh.is_empty:
                    raise ValueError("Empty mesh")

                transform = build_transform(child)

                # ---- Save room type alongside the object ----
                scene_objects.append({
                    "mesh": mesh,
                    "transform": transform,
                    "label": label,
                    "room_type": room_type,            # << attach parent room type
                    "node_name": f"{ref_id}_{child_index}",
                    "model_item": model_item
                })

            except Exception as e:
                failed_models[ref_id] = str(e)

    if not scene_objects:
        return None, None, None, failed_models

    # Build scene and sample points
    textured_scene = trimesh.Scene()
    meshes_world = []
    for obj in scene_objects:
        textured_scene.add_geometry(obj['mesh'], node_name=obj['node_name'],
                                    transform=obj['transform'])
        mesh_copy = obj['mesh'].copy()
        mesh_copy.apply_transform(obj['transform'])
        meshes_world.append({'mesh': mesh_copy, 'label': obj['label']})

    point_cloud = sample_points(scene_objects)
    return textured_scene, meshes_world, point_cloud, failed_models

def process_one_scene(
    scene_path: Path, model_dir: Path, model_info_file: Path, out_root: Path,
    args: argparse.Namespace
) -> bool:
    """Process one scene into meshes, point cloud, and metadata."""
    scene_id = infer_ids_from_path(scene_path)
    if isinstance(scene_id, tuple):
        scene_id = scene_id[0]
    scene_id = str(scene_id)

    out_dir = out_root / scene_id if args.per_scene_subdir else out_root
    if args.per_scene_subdir:
        safe_mkdir(out_dir)

    # Load scene JSON
    with open(scene_path, "r", encoding="utf-8") as f:
        scene = json.load(f)

    # Load model_info.json
    with open(model_info_file, "r", encoding="utf-8") as f:
        model_info_map = {m["model_id"]: m for m in json.load(f)}

    # Build config dict
    config = {
        "ppsm": args.ppsm,
        "total_points": args.total_points,
        "min_pts": args.min_pts_per_mesh,
        "max_pts": args.max_pts_per_mesh,
    }

    # Process scene: returns (trimesh.Scene, meshes_world, structured_pointcloud, failed_models)
    textured_scene, meshes_info, point_cloud, failed = process_scene(
        scene, model_dir, model_info_map)

    if point_cloud is None or point_cloud.size == 0:
        print(f"[WARN] No points sampled for scene {scene_id}")
        return False

    # Delegate all saving to export_outputs
    export_outputs(scene_id, out_dir, textured_scene, meshes_info, point_cloud, args, TAXONOMY)

    return True


# ---------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------

def main():
    global TAXONOMY
    global ARGS
    ap = argparse.ArgumentParser()
    ap.add_argument("--scenes", nargs="+")
    ap.add_argument("--scene_list")
    ap.add_argument("--scene_file")
    ap.add_argument("--out_dir", required=True)
    ap.add_argument("--model_dir", required=True)
    ap.add_argument("--model_info", required=True)
    ap.add_argument("--taxonomy", required=True)
    ap.add_argument("--num_points", type=int, default=2048)
    ap.add_argument("--total_points", type=int, default=500000)
    ap.add_argument("--ppsm", type=float, default=0.0)
    ap.add_argument("--min_pts_per_mesh", type=int, default=100)
    ap.add_argument("--max_pts_per_mesh", type=int, default=0)
    ap.add_argument("--save_glb", action="store_true")
    ap.add_argument("--save_obj", action="store_true")
    ap.add_argument("--save_parquet", action="store_true")
    ap.add_argument("--save_csv", action="store_true")
    ap.add_argument("--per_scene_subdir", action="store_true", default=True)
    ap.add_argument(
        "--limit",
        type=int,
        default=None,
        help="Maximum number of scenes to process (default: all)"
    )
    args = ap.parse_args()
    ARGS = args
    TAXONOMY = Taxonomy(Path(args.taxonomy))

    scene_paths = gather_paths_from_sources(args.scene_file, args.scenes, args.scene_list)
    if not scene_paths:
        print("No scenes found")
        return

    if args.limit is not None:
        scene_paths = scene_paths[:args.limit]

    out_root = Path(args.out_dir)
    safe_mkdir(out_root)

    progress = create_progress_tracker(len(scene_paths), "scenes")
    success_count = 0
    for i, scene_path in enumerate(scene_paths, 1):
        try:
            success = process_one_scene(scene_path, Path(args.model_dir),
                                        Path(args.model_info), out_root, args)
            if success:
                success_count += 1
            progress(i, scene_path.name, success)
        except Exception as e:
            progress(i, f"failed {scene_path.name}: {e}", False)

    print(f"\nSuccessfully processed {success_count}/{len(scene_paths)} scenes")



if __name__ == "__main__":
    main()




================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\stage2_split2rooms.py
================================================================================
#!/usr/bin/env python3
"""
stage2_split2rooms.py (refactored)

Create partitioned Parquet dataset by room from scene-level semantic point clouds,
and optionally compute per-room floor-aligned frames.
"""

import argparse
import sys
import numpy as np
import pandas as pd
import json
from pathlib import Path
from typing import Optional, Tuple, List
from utils.utils import (
    discover_files, infer_scene_id, find_semantic_maps_json, 
    get_floor_label_ids, safe_mkdir, write_json, 
    create_progress_tracker
)
from utils.semantic_utils import Taxonomy
TAXONOMY: Taxonomy = None


# ---------- Frame Computation ----------

def pca_plane(points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Return (origin, unit normal) using PCA on candidate floor points."""
    origin = points.mean(axis=0)
    X = points - origin
    if X.shape[0] < 3:
        n = np.array([0, 0, 1.0], dtype=np.float64)
        return origin.astype(np.float64), n
    
    C = np.cov(X.T)
    w, V = np.linalg.eigh(C)  # ascending eigenvalues
    n = V[:, 0]  # smallest eigenvalue = normal to plane
    n = n / (np.linalg.norm(n) + 1e-12)
    return origin.astype(np.float64), n

def orient_normal_upward(normal: np.ndarray, all_xyz: np.ndarray, origin: np.ndarray) -> np.ndarray:
    """Choose normal sign so most points have positive height along +normal."""
    heights = (all_xyz - origin) @ normal
    if (heights > 0).sum() < (heights < 0).sum():
        normal = -normal
    return normal / (np.linalg.norm(normal) + 1e-12)

def build_orthonormal_frame(origin: np.ndarray, normal: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """Build coherent UVN frame: v=proj(+Y), u=nÃ—v, re-orthonormalize."""
    Y = np.array([0, 1, 0], dtype=np.float64)
    X = np.array([1, 0, 0], dtype=np.float64)
    
    # Project Y onto plane perpendicular to normal
    v = Y - (Y @ normal) * normal
    if np.linalg.norm(v) < 1e-9:
        # Y is parallel to normal, use X instead
        v = X - (X @ normal) * normal
    v = v / (np.linalg.norm(v) + 1e-12)
    
    # Complete right-handed frame
    u = np.cross(normal, v)
    u = u / (np.linalg.norm(u) + 1e-12)
    v = np.cross(u, normal)
    v = v / (np.linalg.norm(v) + 1e-12)
    
    return u, v, normal

def world_to_local(xyz: np.ndarray, origin: np.ndarray, u: np.ndarray, v: np.ndarray, n: np.ndarray) -> np.ndarray:
    """Transform world coordinates to local UVH frame."""
    R = np.stack([u, v, n], axis=1)  # world -> local transformation
    return (xyz - origin) @ R

def compute_room_frame(parquet_path: Path, floor_label_ids=None, height_band=(0.05, 0.50)) -> dict:
    """Compute floor-aligned coordinate frame for room."""
    try:
        import pandas as pd
    except ImportError:
        raise RuntimeError("pandas required for room frame computation")
    
    # Read point cloud
    df = pd.read_parquet(parquet_path)
    required_cols = {"x", "y", "z"}
    if not required_cols.issubset(df.columns):
        raise RuntimeError(f"Missing columns {required_cols} in {parquet_path}")
    
    xyz = df[["x", "y", "z"]].to_numpy(dtype=np.float64, copy=False)
    
    # Extract floor points if label info available
    floor_pts = np.empty((0, 3), dtype=np.float64)
    if ("label_id" in df.columns) and (floor_label_ids is not None):
        mask = np.isin(df["label_id"].to_numpy(), np.array(floor_label_ids, dtype=df["label_id"].dtype))
        floor_pts = xyz[mask]
    
    # Fallback to low-Z points if insufficient floor labels
    if floor_pts.shape[0] < 50:
        z = xyz[:, 2]
        z_cutoff = np.quantile(z, 0.02)
        candidates = xyz[z <= z_cutoff + 1e-6]
        
        if floor_pts.shape[0] == 0 and "label_id" in df.columns and floor_label_ids is not None:
            pass  # Keep empty to make error obvious
        else:
            floor_pts = candidates
    
    if floor_pts.shape[0] < 3:
        raise RuntimeError(f"Too few floor points to compute plane (check floor_label_ids) point num = {floor_pts.shape[0]}")
    
    # Compute floor plane
    origin, normal = pca_plane(floor_pts)
    normal = orient_normal_upward(normal, xyz, origin)
    u, v, n = build_orthonormal_frame(origin, normal)
    
    # Transform all points to local coordinates
    uvh = world_to_local(xyz, origin, u, v, n)
    umin, umax = float(uvh[:, 0].min()), float(uvh[:, 0].max())
    vmin, vmax = float(uvh[:, 1].min()), float(uvh[:, 1].max())
    
    # Auto-orient: longer axis becomes forward (+v)
    yaw_auto = 0.0 if (vmax - vmin) >= (umax - umin) else 90.0
    
    return {
        "origin_world": origin.tolist(),
        "u_world": u.tolist(),
        "v_world": v.tolist(),
        "n_world": n.tolist(),
        "uv_bounds": [umin, umax, vmin, vmax],
        "yaw_auto": float(yaw_auto),
        "map_band_m": [float(height_band[0]), float(height_band[1])]
    }

# ---------- Meta Writing ----------

def write_room_meta_files(root: Path, layout: str, floor_label_ids=None, height_band=(0.05, 0.50)):
    """Write meta.json files for all rooms based on layout type."""
    if layout == "inplace":
        pattern = "*/rooms/*/*.parquet"
    else:
        pattern = "part-*.parquet"
    
    parquet_files = list(root.rglob(pattern))
    progress = create_progress_tracker(len(parquet_files), "room frames")
    
    for i, parquet_path in enumerate(parquet_files, 1):
        try:
            meta = compute_room_frame(parquet_path, floor_label_ids, height_band)
            
            if layout == "inplace":
                # New layout: <scene>_<room>_meta.json
                stem = parquet_path.stem
                meta_path = parquet_path.parent / f"{stem}_meta.json"
            else:
                # Old layout: meta.json
                meta_path = parquet_path.parent / "meta.json"
            
            write_json(meta, meta_path)
            progress(i, f"frame: {meta_path}", True)
            
        except Exception as e:
            progress(i, f"[warn] frame failed for {parquet_path}: {e}", False)

# ---------- Main Processing ----------

def split_scene_to_rooms(input_files: List[Path], output_config: dict, columns: List[str] = None) -> int:
    """Split scene-level parquets into room-level parquets."""
    processed_count = 0
    progress = create_progress_tracker(len(input_files), "scene splits")
    
    # Determine processing engine
    engine = output_config.get("engine", "auto")
    if engine in ("auto", "dataset"):
        try:
            import pyarrow as pa
            import pyarrow.parquet as pq
            engine = "dataset"
        except ImportError:
            if output_config.get("engine") == "dataset":
                raise RuntimeError("pyarrow not available; use --engine pandas or install pyarrow")
            engine = "pandas"

    for i, input_path in enumerate(input_files, 1):
        try:
            scene_id = infer_scene_id(input_path)
            scene_dir = input_path.parent
            
            if engine == "dataset":
                import pyarrow.parquet as pq
                table = pq.read_table(input_path)
                
                if "room_id" not in table.column_names:
                    progress(i, f"skip {input_path.name}: missing room_id", False)
                    continue
                
                # Apply column filtering
                if columns:
                    keep_cols = [c for c in columns if c in table.column_names]
                    for required in ("scene_id", "room_id"):
                        if required not in keep_cols and required in table.column_names:
                            keep_cols.append(required)
                    table = table.select(keep_cols)
                
                df = table.to_pandas()
            else:
                # Pandas fallback
                df = pd.read_parquet(input_path)
                if "room_id" not in df.columns:
                    progress(i, f"skip {input_path.name}: missing room_id", False)
                    continue
                
                # Apply column filtering
                if columns:
                    keep_cols = [c for c in columns if c in df.columns]
                    for required in ("scene_id", "room_id"):
                        if required not in keep_cols and required in df.columns:
                            keep_cols.append(required)
                    df = df[keep_cols]
            
            # Ensure scene_id column exists
            if "scene_id" not in df.columns:
                df["scene_id"] = scene_id
            
            # Split by room and save
            for (sc_id, room_id), group in df.groupby(["scene_id", "room_id"]):
                if output_config["inplace"]:
                    # New layout: scenes/<scene>/rooms/<rid>/<scene>_<rid>.parquet
                    room_dir = scene_dir / "rooms" / str(int(room_id))
                    safe_mkdir(room_dir)
                    output_path = room_dir / f"{scene_id}_{room_id}.parquet"
                else:
                    # Old layout: dataset/scene_id=<>/rooms/room_id=<>/part-*.parquet
                    room_dir = (output_config["output_dir"] / f"scene_id={sc_id}" / 
                               "rooms" / f"room_id={int(room_id)}")
                    safe_mkdir(room_dir)
                    output_path = room_dir / "part-00000.parquet"
                
                group.to_parquet(output_path, index=False)
            
            processed_count += 1
            progress(i, f"split {scene_id}", True)
            
        except Exception as e:
            progress(i, f"failed {input_path.name}: {e}", False)
    
    return processed_count



def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_dir", required=True, help="Root folder with scene parquet files")
    ap.add_argument("--glob", default="*_sem_pointcloud.parquet", help="Glob for input files")
    ap.add_argument("--out_root", help="(Default mode) Root folder for partitioned dataset")
    ap.add_argument("--dataset_name", default="room_dataset", help="(Default mode) Dataset folder name")
    ap.add_argument("--columns", nargs="*", default=[], help="Optional column restriction")
    ap.add_argument("--engine", choices=["auto", "dataset", "pandas"], default="auto")
    ap.add_argument("--inplace", action="store_true", help="Write inside each scene directory instead of separate dataset")

    # Frame computation
    ap.add_argument("--compute-frames", action="store_true", help="Compute per-room floor frames")
    ap.add_argument("--floor-label-ids", type=int, nargs="*", help="Override floor label IDs manually")
    ap.add_argument("--taxonomy", required=True, help="Path to taxonomy.json")
    ap.add_argument("--map-band", type=float, nargs=2, default=[0.05, 0.50],
                    help="Height band [min max] above floor")
    ap.add_argument("--manifest", help="Optional manifest CSV listing files to process")

    args = ap.parse_args()
    global TAXONOMY
    TAXONOMY = Taxonomy(Path(args.taxonomy))

    in_dir = Path(args.in_dir)

    # Validate output configuration
    if not args.inplace and not args.out_root:
        print("ERROR: --out_root required unless using --inplace", file=sys.stderr)
        sys.exit(2)

    # Set up output configuration
    output_config = {
        "inplace": args.inplace,
        "engine": args.engine
    }

    if not args.inplace:
        output_config["output_dir"] = Path(args.out_root) / args.dataset_name
        safe_mkdir(output_config["output_dir"])

    # Discover input files
    manifest_path = Path(args.manifest) if args.manifest else None
    input_files = discover_files(in_dir, args.glob, manifest_path, "scene_parquet")

    if not input_files:
        print("No input files found", file=sys.stderr)
        sys.exit(2)

    print(f"Found {len(input_files)} input files")

    # Process splits
    processed = split_scene_to_rooms(input_files, output_config, args.columns)
    print(f"Processed {processed} scene files")

    # Compute frames if requested
    if args.compute_frames:
        if args.floor_label_ids:
            floor_ids = tuple(args.floor_label_ids)
            print(f"Using provided floor label IDs: {list(floor_ids)}")
        else:
            floor_ids = TAXONOMY.get_floor_ids()
            print(f"Using floor label IDs from taxonomy {args.taxonomy}: {list(floor_ids)}")

        # --- Write per-room frames ---
        print("Computing per-room floor frames...")
        layout = "inplace" if args.inplace else "default"
        search_root = in_dir if args.inplace else output_config["output_dir"]

        write_room_meta_files(
            search_root, layout,
            floor_label_ids=floor_ids,
            height_band=tuple(args.map_band)
        )
        print("Done computing frames")


if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\stage3_create_room_scenes_layouts.py
================================================================================
#!/usr/bin/env python3
"""
stage3_create_room_scenes_layouts.py (refactored)

Generate segmented top-down layout images per room and scene.
"""

import argparse
import csv
import time
from pathlib import Path
from typing import List, Tuple

import numpy as np
import pandas as pd
from PIL import Image
import json

from utils.utils import (
    discover_files, load_room_meta, extract_frame_from_meta, 
    find_semantic_maps_json, load_global_palette, create_progress_tracker, safe_mkdir
)
from utils.semantic_utils import Taxonomy
from utils.geometry_utils import (
    world_to_local_coords

)

TAXONOMY = None
# ---------- Rendering Helpers ----------

def draw_point(canvas: np.ndarray, x: int, y: int, color: np.ndarray, size: int = 1):
    """Draw a square point on canvas with given size."""
    half = size // 2
    x0, x1 = max(x - half, 0), min(x + half, canvas.shape[1] - 1)
    y0, y1 = max(y - half, 0), min(y + half, canvas.shape[0] - 1)
    canvas[y0:y1 + 1, x0:x1 + 1] = color

def points_to_image_coords(u_vals: np.ndarray, v_vals: np.ndarray, 
                          uv_bounds: Tuple[float, float, float, float],
                          resolution: int, margin: int = 10) -> Tuple[np.ndarray, np.ndarray]:
    """Convert UV coordinates to image pixel coordinates."""
    umin, umax, vmin, vmax = uv_bounds
    span = max(umax - umin, vmax - vmin, 1e-6)
    scale = (resolution - 2 * margin) / span
    
    u_pix = (u_vals - umin) * scale + margin
    v_pix = (v_vals - vmin) * scale + margin
    
    # Flip V for image coordinates (origin at top-left)
    x_img = np.clip(np.round(u_pix).astype(np.int32), 0, resolution - 1)
    y_img = np.clip(np.round((resolution - 1) - v_pix).astype(np.int32), 0, resolution - 1)
    
    return x_img, y_img

def load_taxonomy(taxonomy_path):
    """Load taxonomy JSON once and prepare lookups."""
    with open(taxonomy_path, "r", encoding="utf-8") as f:
        taxonomy = json.load(f)

    category2color = {}
    super2color = {}

    # Structural categories (direct mapping)
    if "structural" in taxonomy:
        for cat, info in taxonomy["structural"].items():
            if "color" in info:
                category2color[cat.lower()] = tuple(info["color"])

    # Furniture categories (nested under super)
    if "furniture" in taxonomy:
        for super_name, super_info in taxonomy["furniture"].items():
            if "color" in super_info:
                super2color[super_name.lower()] = tuple(super_info["color"])
            if "categories" in super_info:
                for cat, info in super_info["categories"].items():
                    if "color" in info:
                        category2color[cat.lower()] = tuple(info["color"])

    return category2color, super2color


# ---------- Room Layout Generation ----------


def create_room_layout(
    parquet_path: Path,
    output_path: Path,
    color_mode: str = "category",
    resolution: int = 512,
    margin: int = 10,
    height_min: float = None,
    height_max: float = None,
    point_size: int = 1,
):
    """Generate segmented layout image for a single room."""
    # Load room metadata
    meta = load_room_meta(parquet_path.parent)
    if meta is None:
        raise RuntimeError(f"No metadata found for {parquet_path}")

    origin, u, v, n, uv_bounds, _, map_band = extract_frame_from_meta(meta)

    # Resolve height filtering bounds
    if height_min is None or height_max is None:
        if map_band and len(map_band) == 2:
            if height_min is None:
                height_min = float(map_band[0])
            if height_max is None:
                height_max = float(map_band[1])

    if height_min is None:
        height_min = 0.0
    if height_max is None:
        height_max = 2.5
    if height_max <= height_min:
        raise ValueError(f"height_max ({height_max}) must be > height_min ({height_min})")

    # Load and process point cloud
    df = pd.read_parquet(parquet_path)
    required_cols = {"x", "y", "z", "label_id"}
    if not required_cols.issubset(df.columns):
        raise RuntimeError(f"Missing required columns {required_cols} in {parquet_path}")

    xyz = df[["x", "y", "z"]].to_numpy(dtype=np.float32)
    labels = df["label_id"].to_numpy(dtype=np.int32)

    # Transform to local coordinates
    uvh = world_to_local_coords(xyz, origin, u, v, n)

    # Apply height filtering
    #print("DEBUG layout:", height_min, height_max)
    height_mask = (uvh[:, 2] >= height_min) & (uvh[:, 2] <= height_max)
    if height_mask.sum() == 0:
        print(f"[warn] no points in height band [{height_min},{height_max}] m in {parquet_path}",flush=True)
        return

    u_vals, v_vals = uvh[height_mask, 0], uvh[height_mask, 1]
    filtered_labels = labels[height_mask]

    # Convert to image coordinates
    x_img, y_img = points_to_image_coords(u_vals, v_vals, uv_bounds, resolution, margin)

    # Render to canvas
    canvas = np.full((resolution, resolution, 3), 240, dtype=np.uint8)
    for lbl, x, y in zip(filtered_labels, x_img, y_img):
        if lbl is None:
            raise ValueError(f"Unexpected None label in {parquet_path}")
        color = TAXONOMY.get_color(lbl, mode=color_mode)
        draw_point(canvas, x, y, np.array(color, dtype=np.uint8), size=point_size)

    # Save image
    safe_mkdir(output_path.parent)
    Image.fromarray(canvas).save(output_path)

# ---------- Scene Layout Generation ----------

# def create_scene_layout(
#     scene_dir: Path,
#     output_path: Path,
#     color_mode: str = "category",
#     resolution: int = 512,
#     margin: int = 10,
#     height_min: float = None,
#     height_max: float = None,
#     point_size: int = 1,
# ):
#     """Generate combined layout image for entire scene using taxonomy palette."""
#     scene_id = scene_dir.name
#     room_parquets = sorted(scene_dir.rglob("rooms/*/*.parquet"))
#     if not room_parquets:
#         print(f"[warn] no room parquets found in {scene_dir}",flush=True)
#         return

#     # Get reference frame from first room
#     first_meta = load_room_meta(room_parquets[0].parent)
#     if first_meta is None:
#         print(f"[warn] missing metadata for {room_parquets[0]}",flush=True)
#         return
#     origin, u, v, n, _, _, _ = extract_frame_from_meta(first_meta)

#     # Collect global bounds
#     all_u_bounds, all_v_bounds = [], []
#     for parquet_path in room_parquets:
#         try:
#             df = pd.read_parquet(parquet_path, columns=["x", "y", "z"])
#             xyz = df.to_numpy(dtype=np.float32)
#             uvh = world_to_local_coords(xyz, origin, u, v, n)
#             all_u_bounds.extend([uvh[:, 0].min(), uvh[:, 0].max()])
#             all_v_bounds.extend([uvh[:, 1].min(), uvh[:, 1].max()])
#         except Exception as e:
#             print(f"[warn] failed bounds for {parquet_path}: {e}",flush=True)

#     if not all_u_bounds:
#         print(f"[warn] no usable points in {scene_dir}",flush=True)
#         return
#     global_uv_bounds = (min(all_u_bounds), max(all_u_bounds), min(all_v_bounds), max(all_v_bounds))

#     # Render all rooms to single canvas
#     canvas = np.full((resolution, resolution, 3), 240, dtype=np.uint8)
#     for parquet_path in room_parquets:
#         try:
#             df = pd.read_parquet(parquet_path)
#             required_cols = {"x", "y", "z", "label_id"}
#             if not required_cols.issubset(df.columns):
#                 continue

#             xyz = df[["x", "y", "z"]].to_numpy(dtype=np.float32)
#             labels = df["label_id"].to_numpy(dtype=np.int32)

#             uvh = world_to_local_coords(xyz, origin, u, v, n)
#             mask = np.ones(len(xyz), dtype=bool)
#             if height_min is not None:
#                 mask &= uvh[:, 2] >= height_min
#             if height_max is not None:
#                 mask &= uvh[:, 2] <= height_max

#             u_vals, v_vals = uvh[mask, 0], uvh[mask, 1]
#             f_labels = labels[mask]

#             x_img, y_img = points_to_image_coords(u_vals, v_vals, global_uv_bounds, resolution, margin)
#             for lbl, x, y in zip(f_labels, x_img, y_img):
#                 color = TAXONOMY.get_color(lbl, mode=color_mode)
#                 draw_point(canvas, x, y, np.array(color, dtype=np.uint8), size=point_size)
#         except Exception as e:
#             print(f"[warn] skipping {parquet_path}: {e}",flush=True)

#     safe_mkdir(output_path.parent)
#     Image.fromarray(canvas).save(output_path)



def create_scene_layout(
    scene_dir: Path,
    output_path: Path,
    color_mode: str = "category",
    resolution: int = 512,
    margin: int = 10,
    height_min: float = None,
    height_max: float = None,
    point_size: int = 1,
):
    """Generate combined layout image for entire scene using taxonomy palette."""
    scene_id = scene_dir.name
    room_parquets = sorted(scene_dir.rglob("rooms/*/*.parquet"))
    if not room_parquets:
        print(f"[warn] no room parquets found in {scene_dir}",flush=True)
        return

    # Get reference frame from first room
    first_meta = load_room_meta(room_parquets[0].parent)
    if first_meta is None:
        print(f"[warn] missing metadata for {room_parquets[0]}",flush=True)
        return
    origin, u, v, n, _, _, _ = extract_frame_from_meta(first_meta)

    # SAVE THE SCENE COORDINATE FRAME TO scene_info.json
    scene_info_path = scene_dir / f"{scene_id}_scene_info.json"
    if scene_info_path.exists():
        # Load existing scene_info
        scene_info = json.loads(scene_info_path.read_text(encoding="utf-8"))
    else:
        scene_info = {}
    
    # Add coordinate frame (same frame used to generate layout)
    scene_info["origin_world"] = origin.tolist()
    scene_info["u_world"] = u.tolist()
    scene_info["v_world"] = v.tolist()
    scene_info["n_world"] = n.tolist()
    
    # Save updated scene_info
    scene_info_path.write_text(json.dumps(scene_info, indent=2), encoding="utf-8")

    # Collect global bounds
    all_u_bounds, all_v_bounds = [], []
    for parquet_path in room_parquets:
        try:
            df = pd.read_parquet(parquet_path, columns=["x", "y", "z"])
            xyz = df.to_numpy(dtype=np.float32)
            uvh = world_to_local_coords(xyz, origin, u, v, n)
            all_u_bounds.extend([uvh[:, 0].min(), uvh[:, 0].max()])
            all_v_bounds.extend([uvh[:, 1].min(), uvh[:, 1].max()])
        except Exception as e:
            print(f"[warn] failed bounds for {parquet_path}: {e}",flush=True)

    if not all_u_bounds:
        print(f"[warn] no usable points in {scene_dir}",flush=True)
        return
    global_uv_bounds = (min(all_u_bounds), max(all_u_bounds), min(all_v_bounds), max(all_v_bounds))

    # Render all rooms to single canvas
    canvas = np.full((resolution, resolution, 3), 240, dtype=np.uint8)
    for parquet_path in room_parquets:
        try:
            df = pd.read_parquet(parquet_path)
            required_cols = {"x", "y", "z", "label_id"}
            if not required_cols.issubset(df.columns):
                continue

            xyz = df[["x", "y", "z"]].to_numpy(dtype=np.float32)
            labels = df["label_id"].to_numpy(dtype=np.int32)

            uvh = world_to_local_coords(xyz, origin, u, v, n)
            mask = np.ones(len(xyz), dtype=bool)
            if height_min is not None:
                mask &= uvh[:, 2] >= height_min
            if height_max is not None:
                mask &= uvh[:, 2] <= height_max

            u_vals, v_vals = uvh[mask, 0], uvh[mask, 1]
            f_labels = labels[mask]

            x_img, y_img = points_to_image_coords(u_vals, v_vals, global_uv_bounds, resolution, margin)
            for lbl, x, y in zip(f_labels, x_img, y_img):
                color = TAXONOMY.get_color(lbl, mode=color_mode)
                draw_point(canvas, x, y, np.array(color, dtype=np.uint8), size=point_size)
        except Exception as e:
            print(f"[warn] skipping {parquet_path}: {e}",flush=True)

    safe_mkdir(output_path.parent)
    Image.fromarray(canvas).save(output_path)
# ---------- Discovery Helpers ----------

def discover_rooms(root: Path, pattern: str = None, manifest: Path = None) -> List[Path]:
    """Discover room parquet files."""
    if manifest:
        # Match your manifest column header
        return discover_files(root, pattern, manifest, "room_parquet_file_path")
    
    # Default discovery
    files = list(root.rglob("part-*.parquet"))        # old format
    files.extend(root.rglob("*_*[0-9].parquet"))      # new format
    if not files:
        files = list(root.rglob("rooms/*/*.parquet"))
    
    return sorted(files)


def discover_scenes_from_rooms(room_files: List[Path]) -> List[str]:
    """Extract unique scene IDs from room files."""
    scene_ids = set()
    for room_file in room_files:
        parts = room_file.stem.split("_")
        if len(parts) >= 2:
            scene_ids.add(parts[0])  # <scene_id>_<room_id>
        else:
            for parent in room_file.parents:
                if parent.name.startswith("scene_id="):
                    scene_ids.add(parent.name.replace("scene_id=", ""))
                    break
    return sorted(scene_ids)


def discover_scenes_from_manifest(manifest: Path) -> List[str]:
    """Extract scene IDs from manifest CSV."""
    scene_ids = set()
    with open(manifest, newline='', encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            if "scene_id" in row and row["scene_id"]:
                scene_ids.add(row["scene_id"])
    return sorted(scene_ids)

# ---------- Main Processing ----------


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_root", required=True, help="Root folder with scenes or room dataset")
    ap.add_argument("--taxonomy", required=True, help="Path to taxonomy JSON file")
    ap.add_argument("--pattern", help="Glob pattern for parquet files")
    ap.add_argument("--res", type=int, default=512, help="Output image resolution")
    ap.add_argument("--hmin", type=float, default=0.1, help="Minimum height filter")
    ap.add_argument("--hmax", type=float, default=1.8, help="Maximum height filter")
    ap.add_argument("--point-size", type=int, default=5, help="Point rendering size")
    ap.add_argument("--manifest", help="Optional manifest CSV")
    ap.add_argument("--mode", choices=["room", "scene", "both"], default="both")
    ap.add_argument("--color-mode", choices=["category", "super"], default="category",
                    help="Color mode for rendering")
    args = ap.parse_args()

    in_root = Path(args.in_root)
    taxonomy_path = Path(args.taxonomy)
    manifest_path = Path(args.manifest) if args.manifest else None
    
    # Load taxonomy once
    global TAXONOMY
    TAXONOMY = Taxonomy(taxonomy_path)


    # Rooms
    if args.mode in ("room", "both"):
        room_files = discover_rooms(in_root, args.pattern, manifest_path)
        progress = create_progress_tracker(len(room_files), "room layouts")
        for i, parquet_path in enumerate(room_files, 1):
            scene_id, room_id = parquet_path.stem.split("_")[:2]
            output_path = parquet_path.parent / "layouts" / f"{scene_id}_{room_id}_room_seg_layout.png"
            try:
                create_room_layout(parquet_path, output_path,
                                   args.color_mode,
                                   resolution=args.res, height_min=args.hmin,
                                   height_max=args.hmax, point_size=args.point_size)
                progress(i, f"{parquet_path.name} -> {output_path}", True)
            except Exception as e:
                progress(i, f"failed {parquet_path.name}: {e}", False)

    # Scenes
    if args.mode in ("scene", "both"):
        if manifest_path:
            # Use scene IDs from manifest
            scene_ids = discover_scenes_from_manifest(manifest_path)
            scene_info_files = [in_root / sid / f"{sid}_scene_info.json" for sid in scene_ids]
        else:
            # Default discovery
            scene_info_files = list(in_root.rglob("*_scene_info.json"))
            scene_ids = [p.stem.replace("_scene_info", "") for p in scene_info_files]

        progress = create_progress_tracker(len(scene_ids), "scene layouts")
        for i, scene_id in enumerate(scene_ids, 1):
            scene_dir = in_root / scene_id
            output_path = scene_dir / "layouts" / f"{scene_id}_scene_layout.png"
            try:
                create_scene_layout(scene_dir, output_path,
                                    args.color_mode,
                                    resolution=args.res, height_min=args.hmin,
                                    height_max=args.hmax, point_size=args.point_size)
                progress(i, f"{scene_id} -> {output_path}", True)
            except Exception as e:
                progress(i, f"failed {scene_id}: {e}", False)




if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\stage4_create_room_povs.py
================================================================================
#!/usr/bin/env python3

import argparse, json, math, re, sys, hashlib
from pathlib import Path
from typing import Optional, Tuple, List
import numpy as np
import open3d as o3d
import pandas as pd
import csv
import time
from pathlib import Path
from typing import Optional, List
from sklearn.cluster import KMeans
from utils.utils import create_progress_tracker
from shapely.geometry import MultiPoint
import alphashape
from utils.semantic_utils import Taxonomy  # Import our taxonomy class

# only for HPC
# from xvfbwrapper import Xvfb
# from xvfbwrapper import Xvfb

# ----------- constants -----------
TILT_DEG = 10.0  # look slightly downward for better floor visibility
SEED = 1

# ----------- IO helpers -----------
def infer_scene_id(p: Path) -> str:
    # new filename format: <scene_id>_<room_id>.parquet
    m = re.match(r"([0-9a-fA-F-]+)_\d+\.parquet$", p.name)
    if m: return m.group(1)
    # old path format: .../scene_id=<ID>/room_id=...
    m = re.search(r"scene_id=([^/\\]+)", str(p))
    if m: return m.group(1)
    # fallback from data if present later
    return p.stem

def infer_room_id(p: Path) -> int:
    # new filename format: ..._<room_id>.parquet
    m = re.match(r".+_(\d+)\.parquet$", p.name)
    if m: return int(m.group(1))
    # old path format
    m = re.search(r"room_id=(\d+)", str(p))
    if m: return int(m.group(1))
    return -1

def find_room_files(root: Path, manifest: Optional[Path] = None) -> List[Path]:
    """
    Discover room parquet files either from a manifest CSV or by scanning the dataset root.
    """
    if manifest is not None:
        rows = []
        with open(manifest, newline='', encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                if "room_parquet" in row:
                    rows.append(Path(row["room_parquet"]))
                elif "room_parquet_file_path" in row:
                    rows.append(Path(row["room_parquet_file_path"]))

        return rows

    files = sorted(root.rglob("scene_id=*/room_id=*/*.parquet"))
    return files or sorted(root.rglob("*.parquet"))

def load_meta(parquet_path: Path):
    """Load meta from room folder. Tries new '<scene>_<rid>_meta.json', then legacy 'meta.json'."""
    room_dir = parquet_path.parent
    cand = list(room_dir.glob("*_meta.json"))
    if cand:
        mpath = cand[0]
    else:
        mpath = room_dir / "meta.json"
        if not mpath.exists():
            return None
    j = json.loads(mpath.read_text(encoding="utf-8"))
    to_arr = lambda k: np.array(j[k], dtype=np.float32)
    origin = to_arr("origin_world")
    u = to_arr("u_world")
    v = to_arr("v_world")
    n = to_arr("n_world")
    uv_bounds = tuple(j["uv_bounds"])         # (umin, umax, vmin, vmax)
    yaw_auto = float(j.get("yaw_auto", 0.0))
    map_band = tuple(j.get("map_band_m", [0.05, 0.50]))
    return origin, u, v, n, uv_bounds, yaw_auto, map_band

def get_pov_locations(
    u: np.ndarray,
    v: np.ndarray,
    n: np.ndarray,
    origin: np.ndarray,
    uvh: np.ndarray,
    is_floor: np.ndarray,
    num_views: int,
    yaw_auto: float,
    center_uv: Tuple[float, float],
    seed: int
):
    """
    Extract POVs from floor corners instead of clustering.
    Works for convex and concave rooms (e.g. L-shaped).
    """

    # --- Step 1: collect floor points ---
    floor_pts = uvh[is_floor, :2] if is_floor.any() else uvh[:, :2]
    if floor_pts.shape[0] < 4:
        return []  # not enough data for polygon

    # --- Step 2: concave hull polygon (Î±-shape) ---
    try:
        alpha = 0.05  # controls detail, may tune per dataset
        poly = alphashape.alphashape(floor_pts, alpha)
    except Exception:
        return []

    if poly.is_empty or not poly.is_valid:
        return []

    # --- Step 3: polygon vertices ---
    coords = list(poly.exterior.coords)
    corners = []

    # --- Step 4: corner detection via angle test ---
    def angle(p_prev, p, p_next):
        a = np.array(p_prev) - np.array(p)
        b = np.array(p_next) - np.array(p)
        cosang = np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-9)
        return np.degrees(np.arccos(np.clip(cosang, -1.0, 1.0)))

    angle_thresh = 150.0  # anything sharper = real corner
    for i in range(1, len(coords) - 1):
        ang = angle(coords[i - 1], coords[i], coords[i + 1])
        if ang < angle_thresh:
            corners.append(coords[i])

    # --- Step 5: select up to num_views corners ---
    if len(corners) > num_views:
        rng = np.random.RandomState(seed if seed is not None else 0)
        if len(corners) > num_views:
            rng = np.random.RandomState(seed if seed is not None else 0)
            idxs = rng.choice(len(corners), num_views, replace=False)
            corners = [corners[i] for i in idxs]

    povs = []
    for idx, (cu, cv) in enumerate(corners, start=1):
        # direction: look at polygon centroid
        center = np.array(poly.centroid.coords[0])
        d = center - np.array([cu, cv])
        if np.linalg.norm(d) < 1e-9:
            f_world = math.cos(math.radians(yaw_auto)) * v + math.sin(math.radians(yaw_auto)) * u
        else:
            d = d / (np.linalg.norm(d) + 1e-12)
            f_world = float(d[0]) * u + float(d[1]) * v

        povs.append({
            "name": f"v{idx:02d}",
            "uv": (float(cu), float(cv)),
            "forward": f_world,
            "center_uv": (float(center[0]), float(center[1]))
        })

    return povs

def render_offscreen(pcd, width, height, eye, center, up, fov_deg, bg_rgb, point_size, out_path) -> bool:
    import open3d as o3d
    import time as _t, math as _m
    vis = o3d.visualization.Visualizer()
    vis.create_window(visible=False, width=width, height=height)
    vis.add_geometry(pcd)
    opt = vis.get_render_option()
    opt.point_size = float(point_size)
    opt.background_color = np.array(bg_rgb, dtype=np.float32) / 255.0

    fx = (0.5 * width) / _m.tan(_m.radians(fov_deg) / 2.0)
    fy = (0.5 * height) / _m.tan(_m.radians(fov_deg) / 2.0)
    cx, cy = width / 2.0, height / 2.0
    pin = o3d.camera.PinholeCameraParameters()
    pin.intrinsic = o3d.camera.PinholeCameraIntrinsic(width, height, fx, fy, cx, cy)

    def look_at(eye_, center_, up_):
        f = center_ - eye_
        f = f / (np.linalg.norm(f) + 1e-12)
        upn = up_ / (np.linalg.norm(up_) + 1e-12)
        l = np.cross(upn, f)
        l = l / (np.linalg.norm(l) + 1e-12)
        u2 = np.cross(f, l)
        M = np.eye(4, dtype=np.float64)
        M[0, :3] = l
        M[1, :3] = u2
        M[2, :3] = f
        T = np.eye(4, dtype=np.float64)
        T[:3, 3] = -eye_
        return M @ T

    pin.extrinsic = look_at(
        eye.astype(np.float64), center.astype(np.float64), up.astype(np.float64)
    )

    ctr = vis.get_view_control()
    ctr.convert_from_pinhole_camera_parameters(pin, allow_arbitrary=True)

    vis.poll_events()
    vis.update_renderer()
    _t.sleep(0.12)  # give OpenGL time
    vis.capture_screen_image(str(out_path), do_render=True)
    vis.destroy_window()
    return True

def render_legacy_capture(pcd, width, height, eye, center, up, fov_deg, bg_rgb, point_size, out_path) -> bool:
    import open3d as o3d, time as _t, math as _m
    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)
    vis = o3d.visualization.Visualizer()
    vis.create_window(visible=False, width=width, height=height)
    vis.add_geometry(pcd)
    opt = vis.get_render_option()
    opt.point_size = float(point_size)
    opt.background_color = np.array(bg_rgb, dtype=np.float32)/255.0
    fx = (0.5*width) / _m.tan(_m.radians(fov_deg)/2.0)
    fy = (0.5*height) / _m.tan(_m.radians(fov_deg)/2.0)
    cx, cy = width/2.0, height/2.0
    pin = o3d.camera.PinholeCameraParameters()
    pin.intrinsic = o3d.camera.PinholeCameraIntrinsic(width, height, fx, fy, cx, cy)
    def look_at(eye_, center_, up_):
        f = center_ - eye_
        f = f / (np.linalg.norm(f) + 1e-12)
        upn = up_ / (np.linalg.norm(up_) + 1e-12)
        l = np.cross(upn, f); l = l / (np.linalg.norm(l) + 1e-12)
        u2 = np.cross(f, l)
        M = np.eye(4, dtype=np.float64)
        M[0, :3] = l; M[1, :3] = u2; M[2, :3] = f
        T = np.eye(4, dtype=np.float64); T[:3, 3] = -eye_
        return M @ T
    pin.extrinsic = look_at(eye.astype(np.float64), center.astype(np.float64), up.astype(np.float64))
    ctr = vis.get_view_control()
    ctr.convert_from_pinhole_camera_parameters(pin, allow_arbitrary=True)
    vis.poll_events(); vis.update_renderer(); _t.sleep(0.12)
    vis.capture_screen_image(str(out_path), do_render=True)
    vis.destroy_window()
    return True

# ----------- minimap -----------
def minimap_floor_black(uv: np.ndarray, is_floor: np.ndarray, res=768, margin=10,
                        floor_rgb=(255,0,0), other_rgb=(0,0,0), bg=(240,240,240)):
    from PIL import Image, ImageDraw
    if uv.shape[0]==0:
        return Image.new("RGB",(res,res),bg), (0,1,0,1)
    umin,vmin = uv.min(axis=0); umax,vmax = uv.max(axis=0)
    L = max(umax-umin, vmax-vmin, 1e-9); scale = (res-2*margin)/L
    upix = (uv[:,0]-umin)*scale + margin
    vpix = (uv[:,1]-vmin)*scale + margin
    xi = np.clip(np.round(upix).astype(np.int32), 0, res-1)
    yi = np.clip(np.round((res-1)-vpix).astype(np.int32), 0, res-1)
    floor_count = np.zeros((res,res), dtype=np.int32)
    other_count = np.zeros((res,res), dtype=np.int32)
    np.add.at(floor_count, (yi[is_floor], xi[is_floor]), 1)
    np.add.at(other_count, (yi[~is_floor], xi[~is_floor]), 1)
    canvas = np.full((res,res,3), np.array(bg,dtype=np.uint8), dtype=np.uint8)
    canvas[other_count>0] = np.array(other_rgb, dtype=np.uint8)
    canvas[floor_count>0] = np.array(floor_rgb, dtype=np.uint8)
    img = Image.fromarray(canvas)
    # axes legend uâ†’, vâ†‘
    draw = ImageDraw.Draw(img); ax=max(24,res//7); ox=margin+6; oy=res-margin-6
    draw.line([ox,oy,ox+ax,oy], fill=(0,0,0), width=2)
    draw.polygon([(ox+ax,oy),(ox+ax-8,oy-4),(ox+ax-8,oy+4)], fill=(0,0,0))
    draw.line([ox,oy,ox,oy-ax], fill=(0,0,0), width=2)
    draw.polygon([(ox,oy-ax),(ox-4,oy-ax+8),(ox+4,oy-ax+8)], fill=(0,0,0))
    return img, (umin,umax,vmin,vmax)

def draw_cam_arrows_on_minimap_uv(img, uv_bounds, cams_uv: np.ndarray, angles_deg: List[float], res: int):
    from PIL import ImageDraw
    umin,umax,vmin,vmax = uv_bounds; margin=10
    L = max(umax-umin, vmax-vmin, 1e-6); scale = (res-2*margin)/L
    draw = ImageDraw.Draw(img)
    Lp = max(16, res//10); head = max(6, res//40); r = max(3, res//90)
    for (cu,cv), ang in zip(cams_uv, angles_deg):
        cx = (cu-umin)*scale + margin
        cy = (cv-vmin)*scale + margin; cy = (res-1)-cy
        draw.ellipse([cx-r,cy-r,cx+r,cy+r], fill=(220,30,30), outline=(20,20,20), width=1)
        th = math.radians(float(ang))
        ex,ey = cx + Lp*math.sin(th), cy - Lp*math.cos(th)  # 0Â° = +v
        draw.line([cx,cy,ex,ey], fill=(220,30,30), width=2)
        left = th+math.radians(150); right = th-math.radians(150)
        p2=(ex+head*math.sin(left),  ey-head*math.cos(left))
        p3=(ex+head*math.sin(right), ey-head*math.cos(right))
        draw.polygon([(ex,ey),p2,p3], fill=(220,30,30))

def process_room(parquet_path: Path, root_out_unused: Path, taxonomy: Taxonomy,
                 width=1280, height=800, fov_deg=70.0, eye_height=1.6,
                 point_size=2.0, bg_rgb=(0,0,0),
                 num_views: int = 6, seed: int = SEED, verbose=False) -> bool:
    """
    Process a single room and generate POV renders.
    Now uses the Taxonomy class instead of hardcoded semantic_maps.json lookups.
    """
    meta = load_meta(parquet_path)
    if meta is None:
        print(f"[skip] no meta.json â†’ {parquet_path.parent}")
        return 0
    origin, u, v, n, uv_bounds_all, yaw_auto, _band = meta

    # Get floor IDs from taxonomy instead of semantic_maps.json
    floor_ids = taxonomy.get_floor_ids()
    if not floor_ids:
        print(f"[warning] No floor IDs found in taxonomy for {parquet_path.parent}")
        floor_ids = []

    df = pd.read_parquet(parquet_path)
    xyz = df[["x","y","z"]].to_numpy(np.float32)
    raw = df[["r","g","b"]].to_numpy()
    rgb = (raw.astype(np.float32)/255.0) if not np.issubdtype(raw.dtype, np.floating) else raw.astype(np.float32)
    labels = df["label_id"].to_numpy() if "label_id" in df.columns else None
    scene_id = df["scene_id"].iloc[0] if "scene_id" in df.columns else infer_scene_id(parquet_path)
    room_id  = int(df["room_id"].iloc[0]) if "room_id" in df.columns else infer_room_id(parquet_path)

    if labels is None:
        raise RuntimeError(f"'label_id' column missing in {parquet_path}")

    # --- output dirs inside room folder ---
    out_dir = parquet_path.parent / "povs"
    tex_dir = out_dir / "tex"
    seg_dir = out_dir / "seg"
    tex_dir.mkdir(parents=True, exist_ok=True)
    seg_dir.mkdir(parents=True, exist_ok=True)

    # build Open3D clouds
    pcd = o3d.geometry.PointCloud()
    pcd.points = o3d.utility.Vector3dVector(xyz.astype(np.float64, copy=False))
    pcd.colors = o3d.utility.Vector3dVector(rgb.astype(np.float64, copy=False))

    # Use taxonomy for colors instead of loading global palette
    seg_cols = np.zeros((labels.shape[0], 3), dtype=np.float32)
    for uid in np.unique(labels):
        color = taxonomy.get_color(int(uid))
        seg_cols[labels == uid] = np.array(color, dtype=np.float32) / 255.0
    
    seg = o3d.geometry.PointCloud()
    seg.points = pcd.points
    seg.colors = o3d.utility.Vector3dVector(seg_cols.astype(np.float64, copy=False))

    # local coords strictly from meta
    R = np.stack([u, v, n], axis=1)           # world <- local
    uvh = (xyz - origin) @ R
    is_floor = np.isin(labels, np.array(floor_ids, dtype=labels.dtype))
    center_uv = uvh[:, :2].mean(axis=0)
    
    # --- determine valid POV locations ---
    pov_locs = get_pov_locations(
            u=u,
            v=v,
            n=n,
            origin=origin,
            uvh=uvh,
            is_floor=is_floor,
            num_views=num_views,
            yaw_auto=yaw_auto,
            center_uv=center_uv,
            seed=seed
        )

    pov_meta = {"views": []}
    aims_used = []

    tilt = math.tan(math.radians(TILT_DEG))

    for idx, pov in enumerate(pov_locs, 1):
        cu, cv = pov["uv"]
        f_world = pov["forward"]

        aim = f_world - tilt * n
        eye = origin + cu*u + cv*v + eye_height*n
        center = eye + aim
        up = -n

        base_name = f"{scene_id}_{room_id}_v{idx:02d}"
        tex_name = f"{base_name}_pov_tex.png"
        seg_name = f"{base_name}_pov_seg.png"

        tex_path = tex_dir / tex_name
        seg_path = seg_dir / seg_name

        try:
            render_offscreen(pcd, width, height, eye, center, up,
                             fov_deg, bg_rgb, point_size, tex_path)
            render_offscreen(seg, width, height, eye, center, up,
                             fov_deg, bg_rgb, point_size, seg_path)
            if verbose:
                print(f"  âœ” {tex_path}", flush=True)
            aims_used.append(aim)

            pov_meta["views"].append({
                "name": base_name,
                "tex": str(tex_path),
                "seg": str(seg_path),
                "eye": eye.tolist(),
                "center": center.tolist(),
                "up": up.tolist(),
                "uv": [float(cu), float(cv)],
                "forward": f_world.tolist()
            })
        except Exception as e:
            print(f"  âœ— Failed POV {base_name}: {e}", flush=True)

    # --- minimap ---
    uv = uvh[:, :2]
    mm_img, uvb = minimap_floor_black(uv, is_floor, res=768, margin=10)

    angles = []
    for aim in aims_used:
        horiz = aim - np.dot(aim, n) * n
        au, av = float(np.dot(horiz, u)), float(np.dot(horiz, v))
        angles.append(math.degrees(math.atan2(au, av)))

    mm_uv = np.array([p["uv"] for p in pov_locs], dtype=np.float32)
    draw_cam_arrows_on_minimap_uv(mm_img, uvb, mm_uv, angles, 768)
    mm_name = f"{scene_id}_{room_id}_minimap.png"
    mm_path = out_dir / mm_name
    mm_img.save(str(mm_path))
    print(f"  âœ” {mm_path}", flush=True)

    pov_meta["minimap"] = str(mm_path)

    # --- save pov meta ---
    pov_meta_path = out_dir / f"{scene_id}_{room_id}_pov_meta.json"
    with open(pov_meta_path, "w", encoding="utf-8") as f:
        json.dump(pov_meta, f, indent=2)
    print(f"  âœ” {pov_meta_path}", flush=True)

    return len(pov_locs)

def render_pair(
    pcd, seg, eye, center, up,
    width, height, fov_deg, bg_rgb, point_size,
    tex_path: Path, seg_path: Path
):
    render_offscreen(pcd, width, height, eye, center, up,
                     fov_deg, bg_rgb, point_size, tex_path)
    render_offscreen(seg, width, height, eye, center, up,
                     fov_deg, bg_rgb, point_size, seg_path)
    return center - eye

def main_entry():
    ap = argparse.ArgumentParser()
    g = ap.add_mutually_exclusive_group(required=True)
    g.add_argument("--path", type=str, help="Single room parquet (new or old layout)")
    g.add_argument("--dataset-root", type=str, help="Root with scenes or room_dataset")
    ap.add_argument("--out-dir", type=str, default="./pov_out")
    ap.add_argument("--width", type=int, default=512)
    ap.add_argument("--height", type=int, default=512)
    ap.add_argument("--fov-deg", type=float, default=70.0)
    ap.add_argument("--eye-height", type=float, default=1.6)
    ap.add_argument("--point-size", type=float, default=3.0)
    ap.add_argument("--bg", type=int, nargs=3, default=[0, 0, 0])
    ap.add_argument("--num-views", type=int, default=6)
    ap.add_argument("--seed", type=int, default=SEED)
    ap.add_argument("--manifest", type=str)
    ap.add_argument("--taxonomy", type=str, required=True, 
                    help="Path to taxonomy.json file")
    ap.add_argument("--hpc", action="store_true", default=False,
                    help="Run inside Xvfb for headless HPC rendering")

    args = ap.parse_args()

    if args.hpc:
        try:
            from xvfbwrapper import Xvfb
            with Xvfb(width=args.width, height=args.height, colordepth=24):
                return run_main(args)
        except ImportError:
            print("[error] --hpc flag set but xvfbwrapper not installed", flush=True)
            sys.exit(1)
    else:
        return run_main(args)

def run_main(args):
    bg_rgb = tuple(args.bg)

    # Load taxonomy once at the beginning
    print(f"Loading taxonomy from {args.taxonomy}...")
    taxonomy = Taxonomy(args.taxonomy)
    print(f"Taxonomy loaded successfully.")

    # --- Single room mode ---
    if args.path:
        t0 = time.time()
        ok = process_room(
            Path(args.path), Path(args.out_dir), taxonomy,
            args.width, args.height, args.fov_deg,
            args.eye_height, args.point_size, bg_rgb,
            num_views=args.num_views, seed=args.seed
        )
        dt = time.time() - t0
        print(f"Time for room {args.path}: {dt:.2f}s", flush=True)
        sys.exit(0 if ok else 1)

    # --- Multi-room mode ---
    root = Path(args.dataset_root)
    files = find_room_files(root, Path(args.manifest) if args.manifest else None)
    if not files:
        print(f"No room parquets found (manifest or scan).", flush=True)
        sys.exit(2)

    total_images = (args.num_views * 2 + 1) * len(files)  # 2 images per view (tex+seg) + minimap
    print(f"Found {len(files)} room files.")
    print(f"Will generate {total_images} images total ({args.num_views * 2 + 1} per room)", flush=True)
    
    ok_count = 0
    fail_count = 0
    skip_count = 0
    total_views = 0
    skipped_rooms = []
    failed_rooms = []

    for i, f in enumerate(files, 1):
        try:
            print(f"[{i}/{len(files)}] Processing {f}", flush=True)
            t0 = time.time()
            views = process_room(
                f, Path(args.out_dir), taxonomy,
                args.width, args.height, args.fov_deg,
                args.eye_height, args.point_size, bg_rgb,
                num_views=args.num_views, seed=args.seed
            )
            dt = time.time() - t0

            if views > 0:
                ok_count += 1
                total_views += views
                print(f"  âœ“ Completed in {dt:.2f}s with {views} POVs", flush=True)
            else:
                skip_count += 1
                skipped_rooms.append(str(f))
                print(f"  [skip] No valid POVs â†’ {f}", flush=True)

        except Exception as e:
            fail_count += 1
            failed_rooms.append(str(f))
            print(f"  [error] {e}", flush=True)

    # --- final summary ---
    print("\nSummary:")
    print(f"  Completed: {ok_count}/{len(files)} rooms")
    print(f"  Skipped:   {skip_count} {skipped_rooms}")
    print(f"  Failed:    {fail_count} {failed_rooms}")
    print(f"  Generated: {total_views * 2 + ok_count} images total "
          f"({total_views} POVs x 2 + {ok_count} minimaps)")

if __name__ == "__main__":
    main_entry()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\stage5_1_build_room_graphs.py
================================================================================
#!/usr/bin/env python3
"""
build_room_graphs_from_layouts.py

Creates room-level graphs from color-segmented layouts.
Each color blob (clustered by minimal linkage) becomes a node.
"""

import argparse, json, csv, gc
from pathlib import Path
import cv2
import numpy as np
from scipy.cluster.hierarchy import fclusterdata
from collections import defaultdict

# ------------------------------------------------------------
def load_taxonomy(tax_path: Path):
    tax = json.loads(tax_path.read_text(encoding="utf-8"))
    color_to_label = {}
    
    # Load super-category colors (1000-1999) and wall category (2053)
    for sid, rgb in tax.get("id2color", {}).items():
        sid_int = int(sid)
        
        if 1000 <= sid_int <= 1999:
            # Super categories
            super_label = tax["id2super"].get(sid, f"Unknown_{sid}")
            color_to_label[tuple(rgb)] = {
                "label_id": sid_int,
                "label": super_label
            }
        elif sid_int == 2053:
            # Wall - get category name first, then map to super
            category_name = tax["id2category"].get(sid, "wall")
            super_label = tax.get("category2super", {}).get(category_name, "Structure")
            color_to_label[tuple(rgb)] = {
                "label_id": sid_int,
                "label": super_label
            }
    
    print(f"Loaded {len(color_to_label)} taxonomy colors (super categories + wall)", flush=True)
    return color_to_label

def find_layouts(root: Path | None, manifest: Path | None):
    layouts = []
    if manifest and manifest.exists():
        with open(manifest, newline='', encoding='utf-8') as f:
            for row in csv.DictReader(f):
                # Skip scene-level layouts - only process individual rooms
                if row["room_id"] == "scene":
                    continue
                    
                # Use the layout_path from CSV directly
                layout_path = Path(row["layout_path"])
                if layout_path.exists():
                    sid = row["scene_id"]
                    rid = row["room_id"]
                    layouts.append((sid, rid, layout_path))
    elif root:
        for p in root.rglob("*_room_seg_layout.png"):
            parts = p.stem.split("_")
            # Skip scene layouts based on filename pattern
            if len(parts) >= 3 and parts[1] == "scene":
                continue
            if len(parts) >= 3:
                sid, rid = parts[0], parts[1]
                layouts.append((sid, rid, p))
    return layouts

# ------------------------------------------------------------
def compute_room_center(img):
    mask_room = ~(img == 255).all(axis=2)
    ys, xs = np.nonzero(mask_room)
    if len(xs) == 0:
        return np.array([img.shape[1] / 2, img.shape[0] / 2])
    return np.array([xs.mean(), ys.mean()])

def angle_from_center(center, point):
    v = point - center
    return np.arctan2(v[1], v[0])

# ------------------------------------------------------------
# No longer needed - replaced by vectorized approach

def extract_color_regions(img, color_to_label):
    """
    Extract regions by exact color match to taxonomy colors.
    Returns dict mapping taxonomy_color -> list of pixel coordinates
    """
    h, w = img.shape[:2]
    color_masks = defaultdict(list)
    
    # Get unique colors in the image
    unique_colors = np.unique(img.reshape(-1, 3), axis=0)
    
    print(f"  Found {len(unique_colors)} unique colors in image", flush=True)
    
    # For each unique color, check if it matches a taxonomy color
    for color in unique_colors:
        color_t = tuple(int(c) for c in color)
        
        # Skip white background
        if color_t == (255, 255, 255):
            continue
        
        # Check if this exact color is in taxonomy
        if color_t in color_to_label:
            # Find all pixels with this color
            mask = np.all(img == color, axis=-1)
            ys, xs = np.nonzero(mask)
            points = list(zip(xs, ys))
            color_masks[color_t] = points
            print(f"  Matched color {color_t} ({color_to_label[color_t]['label']}): {len(points)} pixels", flush=True)
        else:
            print(f"  Skipping unknown color {color_t}", flush=True)
    
    return color_masks

def find_color_clusters(points, linkage_thresh=15):
    """Cluster points using DBSCAN (handles sparse clouds well)."""
    if len(points) == 0:
        return []
    pts = np.array(points)
    if len(pts) == 1:
        return [pts]
    
    from sklearn.cluster import DBSCAN
    
    # DBSCAN parameters:
    # eps = maximum distance between points in same cluster
    # min_samples = minimum points to form a cluster
    clustering = DBSCAN(eps=linkage_thresh, min_samples=5).fit(pts)
    
    labels = clustering.labels_
    
    # Group points by cluster (-1 is noise, which we'll ignore or treat separately)
    clusters = []
    for label in set(labels):
        if label == -1:
            # Noise points - you can choose to ignore or keep as individual clusters
            continue
        cluster_mask = labels == label
        clusters.append(pts[cluster_mask])
    
    return clusters

# ------------------------------------------------------------
def visualize(img, room_center, nodes, edges, out_path):
    """Create a clear visualization with colored nodes and black edges."""
    h, w = img.shape[:2]
    
    # Create visualization on white background
    vis = np.ones((h, w, 3), dtype=np.uint8) * 255
    
    # Draw the original image with transparency
    alpha = 0.3
    vis = cv2.addWeighted(img, alpha, vis, 1 - alpha, 0)
    
    # Define color scheme
    EDGE_COLOR = (0, 0, 0)           # Black for edges
    NODE_OUTLINE = (0, 0, 0)         # Black outline for nodes
    ROOM_CENTER_COLOR = (0, 200, 0)  # Green for room center
    TEXT_COLOR = (0, 0, 0)           # Black text
    TEXT_BG_COLOR = (255, 255, 255)  # White background for text
    
    # Draw edges first (behind nodes)
    # Only visualize edges that have a distance relation (by/near)
    for e in edges:
        # Skip if no distance relation
        if e["distance_relation"] is None:
            continue
            
        a = next((n for n in nodes if n["id"] == e["obj_a"]), None)
        b = next((n for n in nodes if n["id"] == e["obj_b"]), None)
        if not a or not b:
            continue
        
        ca, cb = np.array(a["center"], int), np.array(b["center"], int)
        cv2.line(vis, tuple(ca), tuple(cb), EDGE_COLOR, 1, cv2.LINE_AA)
        
        # Label format: (distance_relation, direction_relation)
        mid = ((ca + cb) / 2).astype(int)
        text = f"({e['distance_relation']}, {e['direction_relation']})"
        (tw, th), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, 0.3, 1)
        cv2.rectangle(vis, (mid[0] - 2, mid[1] - th - 2), 
                     (mid[0] + tw + 2, mid[1] + 2), TEXT_BG_COLOR, -1)
        cv2.putText(vis, text, tuple(mid), cv2.FONT_HERSHEY_SIMPLEX, 
                   0.3, TEXT_COLOR, 1, cv2.LINE_AA)
    
    # Draw room center as a larger black circle
    rcx, rcy = map(int, room_center)
    # Draw filled black circle (larger than nodes)
    cv2.circle(vis, (rcx, rcy), 12, (0, 0, 0), -1, cv2.LINE_AA)
    # Optional: draw white outline for visibility
    cv2.circle(vis, (rcx, rcy), 12, (255, 255, 255), 1, cv2.LINE_AA)
    
    # Draw nodes with cluster colors
    for n in nodes:
        cx, cy = map(int, n["center"])
        
        # Use the stored taxonomy color instead of sampling from image
        node_color = n.get("color", (128, 128, 128))  # Default gray if color missing
        
        # Draw filled circle with cluster color
        cv2.circle(vis, (cx, cy), 8, node_color, -1, cv2.LINE_AA)
        # Draw black outline
        cv2.circle(vis, (cx, cy), 8, NODE_OUTLINE, 2, cv2.LINE_AA)
        
        # Add label BELOW the node
        label = n["label"]
        (tw, th), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.45, 1)
        
        # Position text below node (centered)
        text_x = cx - tw // 2
        text_y = cy + 18  # Below the node
        
        # Draw semi-transparent background for text
        overlay = vis.copy()
        cv2.rectangle(overlay, (text_x - 2, text_y - th - 2), 
                     (text_x + tw + 2, text_y + baseline), TEXT_BG_COLOR, -1)
        cv2.addWeighted(overlay, 0.8, vis, 0.2, 0, vis)
        
        # Draw text
        cv2.putText(vis, label, (text_x, text_y), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, TEXT_COLOR, 1, cv2.LINE_AA)
    
    # Convert back to BGR for saving with OpenCV
    vis_bgr = cv2.cvtColor(vis, cv2.COLOR_RGB2BGR)
    cv2.imwrite(str(out_path), vis_bgr)
    print(f"  â†³ saved visualization {out_path}", flush=True)

# ------------------------------------------------------------
def build_room_graph(scene_id, room_id, layout_path, color_to_label):
    img = cv2.imread(str(layout_path))
    if img is None:
        print(f"[warn] cannot read {layout_path}", flush=True)
        return None
    
    # Convert BGR to RGB (OpenCV loads as BGR, taxonomy is RGB)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w = img.shape[:2]

    room_center = compute_room_center(img)
    near_thresh = 0.05 * w
    by_thresh = 0.02 * w

    # Extract color regions with exact matching
    color_regions = extract_color_regions(img, color_to_label)
    print(f"  Found {len(color_regions)} distinct taxonomy colors", flush=True)

    nodes = []
    node_id = 0

    # Store cluster points with nodes for distance calculations
    node_clusters = {}

    for tax_color, points in color_regions.items():
        label_info = color_to_label[tax_color]
        clusters = find_color_clusters(points, linkage_thresh=15)
        print(f"  Color {tax_color} ({label_info['label']}): {len(clusters)} clusters from {len(points)} pixels", flush=True)
        
        for ci, cluster in enumerate(clusters):
            centroid = cluster.mean(axis=0)
            node_key = f"{label_info['label']}_{node_id}"
            nodes.append({
                "id": node_key,
                "label": label_info["label"],
                "label_id": label_info["label_id"],
                "center": centroid.tolist(),
                "color": tax_color
            })
            # Store cluster points for distance calculation
            node_clusters[node_key] = cluster
            node_id += 1

    print(f"  Total nodes created: {len(nodes)}", flush=True)

    from scipy.spatial.distance import cdist
    
    edges = []
    
    for i, a in enumerate(nodes):
        ca = np.array(a["center"])
        la = a["label"].lower()
        cluster_a = node_clusters[a["id"]]
        
        for j, b in enumerate(nodes):
            if j <= i:
                continue
            cb = np.array(b["center"])
            lb = b["label"].lower()
            cluster_b = node_clusters[b["id"]]
            
            # Compute true minimum distance between clusters using cdist
            distances = cdist(cluster_a, cluster_b, metric='euclidean')
            min_cluster_distance = distances.min()
            
            # Delete distance matrix immediately
            del distances

            # Determine proximity relation based on minimum cluster distance
            prox_a_to_b = None
            prox_b_to_a = None
            if "structure" in (la, lb):
                if min_cluster_distance < by_thresh:
                    prox_a_to_b = prox_b_to_a = "by"
            else:
                if min_cluster_distance < near_thresh:
                    prox_a_to_b = prox_b_to_a = "near"

            # Directional relations based on centroids
            ang_a = angle_from_center(room_center, ca)
            ang_b = angle_from_center(room_center, cb)
            d_ang = np.rad2deg((ang_b - ang_a + np.pi*2) % (np.pi*2))
            
            # Direction from A to B
            if d_ang < 45 or d_ang > 315:
                dir_a_to_b = "front_of"
                dir_b_to_a = "behind"
            elif 45 <= d_ang < 135:
                dir_a_to_b = "right_of"
                dir_b_to_a = "left_of"
            elif 135 <= d_ang < 225:
                dir_a_to_b = "behind"
                dir_b_to_a = "front_of"
            else:
                dir_a_to_b = "left_of"
                dir_b_to_a = "right_of"

            # Create edges
            edges.append({
                "obj_a": a["id"], 
                "obj_b": b["id"], 
                "distance_relation": prox_a_to_b,
                "direction_relation": dir_a_to_b
            })
            edges.append({
                "obj_a": b["id"], 
                "obj_b": a["id"], 
                "distance_relation": prox_b_to_a,
                "direction_relation": dir_b_to_a
            })

    graph = {
        "scene_id": scene_id,
        "room_id": room_id,
        "room_center": room_center.tolist(),
        "nodes": nodes,
        "edges": edges
    }

    out_json = layout_path.with_name(f"{scene_id}_{room_id}_graph.json")
    out_vis = layout_path.with_name(f"{scene_id}_{room_id}_graph_vis.png")
    out_json.write_text(json.dumps(graph, indent=2), encoding="utf-8")
    visualize(img, room_center, nodes, edges, out_vis)
    print(f"âœ” wrote {out_json}", flush=True)
    
    # Clean up memory before returning
    del img, color_regions, node_clusters, nodes, edges, graph
    gc.collect()
    
    return None  # Changed from returning graph since we delete it

# ------------------------------------------------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_dir", help="Dataset root containing scenes/<scene_id>/rooms/<room_id>")
    ap.add_argument("--taxonomy", required=True, help="Path to taxonomy.json")
    ap.add_argument("--manifest", help="Optional manifest CSV listing scene_id,room_id,layout_path")
    ap.add_argument("--layout", help="Optional single layout path for testing")
    args = ap.parse_args()

    color_to_label = load_taxonomy(Path(args.taxonomy))

    if args.layout:
        layout_path = Path(args.layout)
        parts = layout_path.stem.split("_")
        if len(parts) >= 3:
            sid, rid = parts[0], parts[1]
        else:
            sid, rid = "unknown_scene", "unknown_room"
        build_room_graph(sid, rid, layout_path, color_to_label)
        return

    # Find layouts from manifest or directory
    layouts = find_layouts(Path(args.in_dir) if args.in_dir else None, 
                          Path(args.manifest) if args.manifest else None)
    if not layouts:
        print("No layouts found.", flush=True)
        return

    for sid, rid, layout_path in layouts:
        build_room_graph(sid, rid, layout_path, color_to_label)
        gc.collect()  # Force garbage collection after each graph

if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\stage5_2_build_scene_graphs.py
================================================================================
#!/usr/bin/env python3
"""
stage5_build_and_visualize_scene_graphs.py

Build scene graphs from point clouds and visualize them on scene layouts.
"""

import argparse
import json
import csv
from pathlib import Path
import cv2
import numpy as np
import pandas as pd
from scipy.spatial.distance import cdist


def load_taxonomy(tax_path: Path):
    """Load room type mappings from taxonomy."""
    tax = json.loads(tax_path.read_text(encoding="utf-8"))
    return tax.get("id2room", {}), tax.get("room2id", {})


def find_scene_pointclouds(root: Path, manifest: Path = None):
    """Find scene-level point cloud files."""
    scenes = []
    if manifest and manifest.exists():
        with open(manifest, newline='', encoding='utf-8') as f:
            for row in csv.DictReader(f):
                # Accept your existing scene_list.csv format
                scene_id = row["scene_id"]
                pc_path = Path(row["parquet_file_path"])
                if pc_path.exists():
                    scenes.append((scene_id, pc_path))

    else:
        for pc_path in root.rglob("*_sem_pointcloud.parquet"):
            scene_id = pc_path.stem.replace("_sem_pointcloud", "")
            scenes.append((scene_id, pc_path))
    return scenes


def extract_room_data(pc_path: Path, id2room: dict):
    """Extract room information from scene point cloud."""
    df = pd.read_parquet(pc_path)
    
    if "room_id" not in df.columns:
        print(f"[warn] No room_id column in {pc_path}", flush=True)
        return []
    
    rooms = []
    xyz = df[["x", "y", "z"]].to_numpy()
    floor_label_ids = [4002, 2052]
    has_labels = "label_id" in df.columns
    
    for room_id, group in df.groupby("room_id"):
        room_id_int = int(room_id)
        room_type = id2room.get(str(room_id_int), f"Room_{room_id_int}")
        
        indices = group.index.to_numpy()
        room_xyz = xyz[indices]
        
        if len(room_xyz) < 10:
            continue
        
        centroid_xyz = room_xyz.mean(axis=0)
        centroid_xy = centroid_xyz[:2].tolist()
        floor_centroid_xy = centroid_xy
        
        if has_labels:
            room_labels = df.iloc[indices]["label_id"].to_numpy()
            floor_mask = np.isin(room_labels, floor_label_ids)
            if floor_mask.sum() > 0:
                floor_xyz = room_xyz[floor_mask]
                floor_centroid_xy = floor_xyz.mean(axis=0)[:2].tolist()
        
        rooms.append({
            "room_id": room_id_int,
            "room_type": room_type,
            "centroid_xy": centroid_xy,
            "floor_centroid_xy": floor_centroid_xy,
            "centroid_xyz": centroid_xyz.tolist(),
            "points": room_xyz,
            "bbox": {
                "min": room_xyz.min(axis=0).tolist(),
                "max": room_xyz.max(axis=0).tolist()
            }
        })
    
    return rooms


def check_adjacency_3d(points_a, points_b, threshold=0.3):
    """Check if two rooms are adjacent based on 3D proximity."""
    max_sample = 1000
    if len(points_a) > max_sample:
        points_a = points_a[np.random.choice(len(points_a), max_sample, replace=False)]
    if len(points_b) > max_sample:
        points_b = points_b[np.random.choice(len(points_b), max_sample, replace=False)]
    
    distances = cdist(points_a, points_b, metric='euclidean')
    return distances.min() < threshold


def compute_scene_center(room_centers):
    """Compute center of all room centers."""
    if len(room_centers) == 0:
        return np.array([0.0, 0.0])
    return np.array(room_centers).mean(axis=0)


def angle_from_center(center, point):
    """Calculate angle from center to point."""
    v = point - center
    return np.arctan2(v[1], v[0])


def build_scene_graph(scene_id, pc_path, id2room, dataset_root):
    """Build scene graph from point cloud."""
    print(f"\nProcessing: {scene_id}", flush=True)
    
    rooms = extract_room_data(pc_path, id2room)
    if len(rooms) == 0:
        print(f"[warn] No rooms found", flush=True)
        return None
    
    print(f"  Found {len(rooms)} rooms", flush=True)
    
    scene_center = compute_scene_center([r["floor_centroid_xy"] for r in rooms])
    
    # Build nodes
    nodes = []
    for r in rooms:
        nodes.append({
            "id": f"room_{r['room_id']}",
            "room_id": r["room_id"],
            "room_type": r["room_type"],
            "centroid_xy": r["centroid_xy"],
            "floor_centroid_xy": r["floor_centroid_xy"],
            "centroid_xyz": r["centroid_xyz"],
            "bbox": r["bbox"]
        })
    
    # Build edges
    edges = []
    for i, a in enumerate(rooms):
        for j, b in enumerate(rooms):
            if j <= i:
                continue
            
            is_adjacent = check_adjacency_3d(a["points"], b["points"])
            dist_rel = "adjacent" if is_adjacent else None
            
            ang_a = angle_from_center(scene_center, a["centroid_xy"])
            ang_b = angle_from_center(scene_center, b["centroid_xy"])
            d_ang = np.rad2deg((ang_b - ang_a + np.pi*2) % (np.pi*2))
            
            if d_ang < 45 or d_ang > 315:
                dir_a_to_b, dir_b_to_a = "front_of", "behind"
            elif 45 <= d_ang < 135:
                dir_a_to_b, dir_b_to_a = "right_of", "left_of"
            elif 135 <= d_ang < 225:
                dir_a_to_b, dir_b_to_a = "behind", "front_of"
            else:
                dir_a_to_b, dir_b_to_a = "left_of", "right_of"
            
            edges.append({
                "room_a": f"room_{a['room_id']}",
                "room_b": f"room_{b['room_id']}",
                "distance_relation": dist_rel,
                "direction_relation": dir_a_to_b
            })
            edges.append({
                "room_a": f"room_{b['room_id']}",
                "room_b": f"room_{a['room_id']}",
                "distance_relation": dist_rel,
                "direction_relation": dir_b_to_a
            })
    
    scene_graph = {
        "scene_id": scene_id,
        "scene_center": scene_center.tolist(),
        "nodes": nodes,
        "edges": edges
    }
    
    # Save
    output_json = pc_path.parent / f"{scene_id}_scene_graph.json"
    output_json.write_text(json.dumps(scene_graph, indent=2), encoding="utf-8")
    print(f"  âœ” Saved scene graph", flush=True)
    
    return scene_graph


def visualize_scene_graph(scene_id, dataset_root):
    """Create visualization of scene graph on scene layout."""
    scene_dir = dataset_root / scene_id
    
    # Load files
    scene_graph_path = scene_dir / f"{scene_id}_scene_graph.json"
    scene_info_path = scene_dir / f"{scene_id}_scene_info.json"
    layout_path = scene_dir / "layouts" / f"{scene_id}_scene_layout.png"
    
    if not all([scene_graph_path.exists(), scene_info_path.exists(), layout_path.exists()]):
        print(f"  [skip] Missing files for visualization", flush=True)
        return
    
    scene_graph = json.loads(scene_graph_path.read_text(encoding="utf-8"))
    scene_info = json.loads(scene_info_path.read_text(encoding="utf-8"))
    
    if "origin_world" not in scene_info:
        print(f"  [skip] No coordinate frame in scene_info", flush=True)
        return
    
    # Load coordinate frame
    origin = np.array(scene_info["origin_world"], dtype=np.float64)
    u = np.array(scene_info["u_world"], dtype=np.float64)
    v = np.array(scene_info["v_world"], dtype=np.float64)
    n = np.array(scene_info["n_world"], dtype=np.float64)
    
    # Transform function
    def world_to_uv(xyz):
        R = np.column_stack([u, v, n])
        local = (xyz - origin) @ R
        return local[:, :2]
    
    # Load image
    img = cv2.imread(str(layout_path))
    if img is None:
        print(f"  [skip] Cannot read layout", flush=True)
        return
    
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    h, w = img.shape[:2]
    vis = cv2.addWeighted(img, 0.3, np.full_like(img, 255), 0.7, 0)
    
    nodes = scene_graph["nodes"]
    edges = scene_graph["edges"]
    scene_center = np.array(scene_graph["scene_center"], dtype=np.float64)
    
    if len(nodes) == 0:
        return
    
    # Transform node positions
    node_xyz = np.array([[n["floor_centroid_xy"][0], n["floor_centroid_xy"][1], n["centroid_xyz"][2]] 
                         for n in nodes], dtype=np.float64)
    node_uv = world_to_uv(node_xyz)
    
    u_min, u_max = node_uv[:, 0].min(), node_uv[:, 0].max()
    v_min, v_max = node_uv[:, 1].min(), node_uv[:, 1].max()
    
    span = max(u_max - u_min, v_max - v_min, 1e-6)
    margin = 10
    scale = (min(w, h) - 2 * margin) / span
    
    def uv_to_img(uv):
        u_pix = (uv[0] - u_min) * scale + margin
        v_pix = (uv[1] - v_min) * scale + margin
        return (int(np.clip(u_pix, 0, w - 1)), int(np.clip((h - 1) - v_pix, 0, h - 1)))
    
    node_positions = {nodes[i]["id"]: uv_to_img(node_uv[i]) for i in range(len(nodes))}
    
    scene_center_xyz = np.array([scene_center[0], scene_center[1], node_xyz[:, 2].mean()])
    scene_center_img = uv_to_img(world_to_uv(scene_center_xyz.reshape(1, 3))[0])
    
    # Draw edges
    drawn = set()
    for e in edges:
        if e.get("distance_relation") != "adjacent":
            continue
        key = tuple(sorted([e["room_a"], e["room_b"]]))
        if key in drawn:
            continue
        drawn.add(key)
        
        pa = node_positions.get(e["room_a"])
        pb = node_positions.get(e["room_b"])
        if pa and pb:
            cv2.line(vis, pa, pb, (0, 0, 0), 2, cv2.LINE_AA)
    
    # Draw center
    cv2.circle(vis, scene_center_img, 8, (0, 0, 0), -1, cv2.LINE_AA)
    cv2.circle(vis, scene_center_img, 8, (255, 255, 255), 1, cv2.LINE_AA)
    
    # Draw nodes
    for node in nodes:
        pos = node_positions[node["id"]]
        
        if 0 <= pos[0] < w and 0 <= pos[1] < h:
            color = tuple(int(c) for c in img[pos[1], pos[0]])
            if color == (255, 255, 255):
                np.random.seed(hash(node["room_type"]) % 2**32)
                color = tuple(np.random.randint(50, 255, 3).tolist())
        else:
            np.random.seed(hash(node["room_type"]) % 2**32)
            color = tuple(np.random.randint(50, 255, 3).tolist())
        
        cv2.circle(vis, pos, 12, color, -1, cv2.LINE_AA)
        cv2.circle(vis, pos, 12, (0, 0, 0), 2, cv2.LINE_AA)
        
        label = node["room_type"]
        (tw, th), baseline = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        tx, ty = pos[0] - tw // 2, pos[1] + 25
        
        overlay = vis.copy()
        cv2.rectangle(overlay, (tx - 3, ty - th - 3), (tx + tw + 3, ty + baseline + 1), (255, 255, 255), -1)
        cv2.addWeighted(overlay, 0.85, vis, 0.15, 0, vis)
        cv2.putText(vis, label, (tx, ty), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)
    
    # Save
    output_path = scene_dir / "layouts" / f"{scene_id}_scene_graph_vis.png"
    output_path.parent.mkdir(exist_ok=True)
    cv2.imwrite(str(output_path), cv2.cvtColor(vis, cv2.COLOR_RGB2BGR))
    print(f"  âœ” Saved visualization", flush=True)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--in_dir", required=True)
    parser.add_argument("--taxonomy", required=True)
    parser.add_argument("--manifest", default=None)
    parser.add_argument("--adjacency_thresh", type=float, default=0.3)
    args = parser.parse_args()
    
    id2room, _ = load_taxonomy(Path(args.taxonomy))
    dataset_root = Path(args.in_dir)
    manifest_path = Path(args.manifest) if args.manifest else None
    
    scenes = find_scene_pointclouds(dataset_root, manifest_path)
    print(f"Found {len(scenes)} scenes\n", flush=True)
    
    for scene_id, pc_path in scenes:
        try:
            build_scene_graph(scene_id, pc_path, id2room, dataset_root)
            visualize_scene_graph(scene_id, dataset_root)
        except Exception as e:
            print(f"[error] {scene_id}: {e}", flush=True)


if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\stage6_create_layout_embeddings.py
================================================================================
#!/usr/bin/env python3
import argparse
import os, sys
from pathlib import Path
import pandas as pd
from tqdm import tqdm


import torch
from torch.utils.data import DataLoader
import torchvision.transforms as T

sys.path.append(str(Path(__file__).parent.parent / "modules"))
from datasets import LayoutDataset, collate_skip_none
from autoencoder import AutoEncoder


def parse_args():
    ap = argparse.ArgumentParser()
    ap.add_argument("--config", required=True, help="Path to autoencoder YAML config")
    ap.add_argument("--ckpt", required=True, help="Path to autoencoder checkpoint .pt/.pth")
    ap.add_argument("--manifest", required=True, help="Input CSV manifest of layouts")
    ap.add_argument("--out_manifest", required=True, help="Output CSV manifest with embeddings column")
    ap.add_argument("--batch_size", type=int, default=128)
    ap.add_argument("--num_workers", type=int, default=4)
    ap.add_argument("--device", default="cuda")
    ap.add_argument("--format", choices=["pt", "npy"], default="pt")
    return ap.parse_args()


def main():
    args = parse_args()
    device = torch.device(args.device if torch.cuda.is_available() else "cpu")

    # --- Load autoencoder ---
    ae = AutoEncoder.from_config(args.config)
    ckpt = torch.load(args.ckpt, map_location=device)
    ae.load_state_dict(ckpt["model"] if "model" in ckpt else ckpt)
    ae.to(device)
    ae.eval()

    # --- Transform (match training setup) ---
    transform = T.Compose([
        T.Resize((512, 512)),  # adjust if config differs
        T.ToTensor()
    ])

    # --- Dataset + Loader ---
    ds = LayoutDataset(args.manifest, transform=transform, mode="all", skip_empty=False, return_embeddings=False)
    dl = DataLoader(
                    ds,
                    batch_size=args.batch_size,
                    shuffle=False,
                    num_workers=args.num_workers,
                    collate_fn=collate_skip_none
                    )

    # --- Prepare manifest update ---
    df = pd.read_csv(args.manifest)
    df = pd.read_csv(args.manifest)
    if "embedding_path" not in df.columns:
        df["embedding_path"] = None
    if "embedding_dim" not in df.columns:
        df["embedding_dim"] = None


    # --- Encode and save with progress bar ---
    total = len(ds)
    with torch.no_grad():
        pbar = tqdm(total=total, desc="Embedding layouts", unit="layout")
        for batch in dl:
            imgs = batch["layout"].to(device)
            z = ae.encoder(imgs)

            for i in range(len(imgs)):
                scene_id = batch["scene_id"][i]
                room_id = batch["room_id"][i]
                typ = batch["type"][i]
                is_empty = batch["is_empty"][i]
                layout_path = Path(batch["path"][i])

                if is_empty:
                    pbar.update(1)
                    continue

                if typ == "room":
                    fname = f"{scene_id}_{room_id}_layout_emb.{args.format}"
                else:  # scene
                    fname = f"{scene_id}_layout_emb.{args.format}"

                out_path = layout_path.parent / fname
                emb = z[i].cpu()

                if args.format == "pt":
                    torch.save(emb, out_path)
                    emb_dim = emb.numel()
                else:
                    import numpy as np
                    np.save(out_path.with_suffix(".npy"), emb.numpy())

                mask = (df["scene_id"] == scene_id) & (df["room_id"] == room_id) & (df["type"] == typ)
                df.loc[mask, "embedding_path"] = str(out_path.resolve())
                df.loc[mask, "embedding_dim"] = emb_dim
                pbar.update(1)

        pbar.close()

    df.to_csv(args.out_manifest, index=False)
    print(f"[INFO] Wrote updated manifest to {args.out_manifest}")


if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\stage7_create_pov_embeddings.py
================================================================================
#!/usr/bin/env python3
"""
stage8_create_pov_embeddings.py

Reads povs.csv manifest, generates ResNet embeddings for POV images,
and creates povs_with_embeddings.csv manifest.
"""

import argparse
import csv
import torch
from pathlib import Path
from torchvision import models, transforms
from PIL import Image
from tqdm import tqdm

from itertools import islice

def batched(iterable, n):
    it = iter(iterable)
    while True:
        batch = list(islice(it, n))
        if not batch:
            break
        yield batch

def load_resnet_model(device="cuda"):
    """Load ResNet18 with removed classifier head for feature extraction."""
    resnet = models.resnet18(weights="IMAGENET1K_V1").to(device)
    resnet.fc = torch.nn.Identity()  # remove classifier head
    resnet.eval()
    return resnet


def get_transform():
    """Get image preprocessing transform for ResNet."""
    return transforms.Compose([
        transforms.Resize((512, 512)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
    ])


def extract_embedding(image_path, model, transform, device):
    """Extract ResNet embedding from a single image."""
    img = Image.open(image_path).convert("RGB")
    x = transform(img).unsqueeze(0).to(device)
    with torch.no_grad(), torch.cuda.amp.autocast():
        embedding = model(x).squeeze(0).cpu()  # [512]
    return embedding



def process_povs(manifest_path: str, output_manifest: str,
                 save_format: str = "pt", batch_size: int = 1):
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")

    print("Loading ResNet18 model...")
    model = load_resnet_model(device)
    transform = get_transform()

    manifest_path = Path(manifest_path)
    with open(manifest_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)

    print(f"Found {len(rows)} POV images to process")

    output_rows = []
    skipped = 0
    processed = 0

    for batch_rows in tqdm(batched(rows, batch_size),
                           total=len(rows)//batch_size + 1,
                           desc="Processing POV images"):
        batch_imgs = []
        valid_rows = []

        for row in batch_rows:
            pov_path = row['pov_path']
            if int(row['is_empty']) or not Path(pov_path).exists():
                out = row.copy()
                out['embedding_path'] = ''
                output_rows.append(out)
                skipped += 1
                continue
            try:
                img = Image.open(pov_path).convert("RGB")
                x = transform(img)
                batch_imgs.append(x)
                valid_rows.append(row)
            except Exception as e:
                print(f"Error reading {pov_path}: {e}")
                out = row.copy()
                out['embedding_path'] = ''
                output_rows.append(out)
                skipped += 1

        if not valid_rows:
            continue

        x = torch.stack(batch_imgs).to(device)
        with torch.no_grad(), torch.cuda.amp.autocast():
            emb = model(x).cpu()  # [B,512]

        for r, e in zip(valid_rows, emb):
            pov_path_obj = Path(r['pov_path'])
            embedding_path = pov_path_obj.with_suffix('.pt')
            torch.save(e, embedding_path)
            out = r.copy()
            out['embedding_path'] = str(embedding_path)
            output_rows.append(out)
            processed += 1

    # Write output manifest
    output_path = Path(output_manifest)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    fieldnames = list(rows[0].keys()) + ['embedding_path']
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(output_rows)

    print(f"\nâœ“ Processed {processed}/{len(rows)} POV images successfully")
    print(f"âœ“ Skipped {skipped} images (empty or errors)")
    print(f"âœ“ Output manifest: {output_path}")

def main():
    parser = argparse.ArgumentParser(
        description="Generate ResNet embeddings for POV images and create updated manifest"
    )
    parser.add_argument(
        "--manifest",
        required=True,
        help="Path to povs.csv manifest"
    )
    parser.add_argument(
    "--batch_size",
    type=int,
    default=32,
    help="Number of images processed per GPU batch"
    )
    parser.add_argument(
        "--output",
        required=True,
        help="Path for output povs_with_embeddings.csv"
    )
    parser.add_argument(
        "--format",
        choices=["pt", "npy"],
        default="pt",
        help="Embedding save format: pt (PyTorch) or npy (NumPy)"
    )
    
    args = parser.parse_args()
    
    process_povs(
        manifest_path=args.manifest,
        output_manifest=args.output,
        save_format=args.format,
        batch_size=args.batch_size
    )


if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\stage8_create_graph_embeddings.py
================================================================================
#!/usr/bin/env python3
"""
stage8_create_graph_embeddings.py

Reads graphs.csv manifest, converts graphs to text, generates embeddings,
and creates graphs_with_embeddings.csv manifest.
"""

import argparse
import csv
import json
import torch
import numpy as np
from pathlib import Path
from sentence_transformers import SentenceTransformer
from tqdm import tqdm


def load_taxonomy(taxonomy_path: str):
    """Load taxonomy mapping from taxonomy.json."""
    t = json.loads(Path(taxonomy_path).read_text(encoding="utf-8"))
    return {int(k): v for k, v in t.get("id2room", {}).items()}


def articleize(label: str) -> str:
    """Add 'the', 'a', or 'an' before a label depending on plurality."""
    clean = label.strip().replace("_", " ")
    lower = clean.lower()

    # heuristic plural detection
    if lower.endswith(("s", "x", "z", "ch", "sh")) and not lower.endswith(("ss", "us")):
        article = "a"
    else:
        # singular
        vowels = "aeiou"
        article = "an" if lower[0] in vowels else "the"
    return f"{article} {clean}"


def graph2text(graph_path: str, taxonomy: dict, max_edges: int = 10_000):
    """
    Converts either a 3D-FRONT room graph or scene graph JSON to text.
    Uses taxonomy to decode room_id when available.
    Removes underscores and adds articles ('the', 'a', 'an').
    """
    path = Path(graph_path)
    
    if not path.exists():
        return ""
    
    g = json.loads(path.read_text(encoding="utf-8"))

    nodes = g.get("nodes", [])
    edges = g.get("edges", [])
    if not edges:
        return ""

    is_scene_graph = "room_a" in edges[0] or "room_b" in edges[0]

    # build node label map
    id_to_label = {}
    for n in nodes:
        if is_scene_graph:
            rid = n.get("room_id")
            raw_label = taxonomy.get(rid, n.get("room_type", str(rid)))
        else:
            raw_label = n.get("label", n.get("id"))
        id_to_label[n["id"]] = articleize(raw_label)

    sentences = []
    seen = set()

    for e in edges[:max_edges]:
        a = e.get("room_a") if is_scene_graph else e.get("obj_a")
        b = e.get("room_b") if is_scene_graph else e.get("obj_b")
        if not a or not b:
            continue

        label_a = id_to_label.get(a)
        label_b = id_to_label.get(b)
        if not label_a or not label_b:
            continue

        key = tuple(sorted([label_a, label_b]))
        if key in seen:
            continue
        seen.add(key)

        dist = e.get("distance_relation")
        direc = e.get("direction_relation")

        if dist and direc:
            sentence = f"{label_a} is {dist} and {direc} {label_b}."
        elif dist:
            sentence = f"{label_a} is {dist} {label_b}."
        elif direc:
            sentence = f"{label_a} is {direc} {label_b}."
        else:
            sentence = f"{label_a} relates to {label_b}."

        sentences.append(sentence)

    text = " ".join(sentences)
    return text.replace("_", " ")


def process_graphs(manifest_path: str, taxonomy_path: str, output_manifest: str, 
                   model_name: str = "all-MiniLM-L6-v2", save_format: str = "pt"):
    """
    Process all graphs in manifest: convert to text, generate embeddings, save.
    
    Args:
        manifest_path: Path to graphs.csv
        taxonomy_path: Path to taxonomy.json
        output_manifest: Path for graphs_with_embeddings.csv
        model_name: SentenceTransformer model to use
        save_format: 'pt' for PyTorch or 'npy' for NumPy
    """
    # Load taxonomy
    print(f"Loading taxonomy from {taxonomy_path}")
    taxonomy = load_taxonomy(taxonomy_path)
    
    # Load embedding model
    print(f"Loading SentenceTransformer model: {model_name}")
    embedder = SentenceTransformer(model_name)
    
    # Read manifest
    print(f"Reading manifest: {manifest_path}")
    manifest_path = Path(manifest_path)
    
    with open(manifest_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    print(f"Found {len(rows)} graphs to process")
    
    # Process each graph
    output_rows = []
    skipped = 0
    
    for row in tqdm(rows, desc="Processing graphs"):
        graph_path = row['graph_path']
        
        # Convert graph to text
        try:
            text = graph2text(graph_path, taxonomy)
            
            if not text:
                print(f"Warning: Empty text for {graph_path}")
                skipped += 1
                # Add row without embedding
                output_row = row.copy()
                output_row['embedding_path'] = ''
                output_rows.append(output_row)
                continue
            
            # Generate embedding
            embedding = embedder.encode(text, normalize_embeddings=True)
            
            # Determine save path
            graph_path_obj = Path(graph_path)
            if save_format == "pt":
                embedding_path = graph_path_obj.with_suffix('.pt')
                torch.save(torch.from_numpy(embedding), embedding_path)
            else:  # npy
                embedding_path = graph_path_obj.with_suffix('.npy')
                np.save(embedding_path, embedding)
            
            # Add to output manifest
            output_row = row.copy()
            output_row['embedding_path'] = str(embedding_path)
            output_rows.append(output_row)
            
        except Exception as e:
            print(f"Error processing {graph_path}: {e}")
            skipped += 1
            output_row = row.copy()
            output_row['embedding_path'] = ''
            output_rows.append(output_row)
    
    # Write output manifest
    output_path = Path(output_manifest)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    fieldnames = list(rows[0].keys()) + ['embedding_path']
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(output_rows)
    
    print(f"\nâœ“ Processed {len(rows) - skipped}/{len(rows)} graphs successfully")
    print(f"âœ“ Skipped {skipped} graphs")
    print(f"âœ“ Output manifest: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Generate embeddings for graph files and create updated manifest"
    )
    parser.add_argument(
        "--manifest",
        required=True,
        help="Path to graphs.csv manifest"
    )
    parser.add_argument(
        "--taxonomy",
        required=True,
        help="Path to taxonomy.json"
    )
    parser.add_argument(
        "--output",
        required=True,
        help="Path for output graphs_with_embeddings.csv"
    )
    parser.add_argument(
        "--model",
        default="all-MiniLM-L6-v2",
        help="SentenceTransformer model name (default: all-MiniLM-L6-v2)"
    )
    parser.add_argument(
        "--format",
        choices=["pt", "npy"],
        default="pt",
        help="Embedding save format: pt (PyTorch) or npy (NumPy)"
    )
    
    args = parser.parse_args()
    
    process_graphs(
        manifest_path=args.manifest,
        taxonomy_path=args.taxonomy,
        output_manifest=args.output,
        model_name=args.model,
        save_format=args.format
    )


if __name__ == "__main__":
    main()



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\utils\geometry_utils.py
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
geometry_utils.py - Pure geometric utility functions

Basic coordinate transformations and geometric calculations used across scripts.
"""

import math
from typing import Tuple
import numpy as np


def world_to_local_coords(points: np.ndarray, origin: np.ndarray, 
                         u: np.ndarray, v: np.ndarray, n: np.ndarray) -> np.ndarray:
    """Transform world coordinates to local UVH frame."""
    R = np.stack([u, v, n], axis=1)
    return (points - origin) @ R


def points_to_image_coords(u_vals: np.ndarray, v_vals: np.ndarray, 
                          uv_bounds: Tuple[float, float, float, float],
                          resolution: int, margin: int = 10) -> Tuple[np.ndarray, np.ndarray]:
    """Convert UV coordinates to image pixel coordinates."""
    umin, umax, vmin, vmax = uv_bounds
    span = max(umax - umin, vmax - vmin, 1e-6)
    scale = (resolution - 2 * margin) / span
    
    u_pix = (u_vals - umin) * scale + margin
    v_pix = (v_vals - vmin) * scale + margin
    
    x_img = np.clip(np.round(u_pix).astype(np.int32), 0, resolution - 1)
    y_img = np.clip(np.round((resolution - 1) - v_pix).astype(np.int32), 0, resolution - 1)
    
    return x_img, y_img


def pca_plane_fit(points: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Return (origin, normal) of best-fit plane through points using PCA."""
    if points.shape[0] < 3:
        return points.mean(axis=0), np.array([0, 0, 1.0], dtype=np.float64)
    
    origin = points.mean(axis=0)
    X = points - origin
    C = np.cov(X.T)
    w, V = np.linalg.eigh(C)
    normal = V[:, 0]  # smallest eigenvalue
    return origin.astype(np.float64), normal / (np.linalg.norm(normal) + 1e-12)


def build_orthonormal_frame(normal: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """Build orthonormal U,V vectors perpendicular to normal."""
    Y = np.array([0, 1, 0], dtype=np.float64)
    X = np.array([1, 0, 0], dtype=np.float64)
    
    v = Y - (Y @ normal) * normal
    if np.linalg.norm(v) < 1e-9:
        v = X - (X @ normal) * normal
    v = v / (np.linalg.norm(v) + 1e-12)
    
    u = np.cross(normal, v)
    u = u / (np.linalg.norm(u) + 1e-12)
    v = np.cross(u, normal)
    v = v / (np.linalg.norm(v) + 1e-12)
    
    return u, v


def get_2d_bounds(points_2d: np.ndarray) -> Tuple[float, float, float, float]:
    """Get 2D bounds (xmin, xmax, ymin, ymax) from 2D points."""
    if points_2d.shape[0] == 0:
        return (0.0, 1.0, 0.0, 1.0)
    
    mins = points_2d.min(axis=0)
    maxs = points_2d.max(axis=0)
    return float(mins[0]), float(maxs[0]), float(mins[1]), float(maxs[1])


def angle_between_vectors(v1: np.ndarray, v2: np.ndarray) -> float:
    """Angle in degrees between two vectors."""
    cos_angle = np.clip(np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)), -1.0, 1.0)
    return math.degrees(math.acos(cos_angle))



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\utils\semantic_utils.py
================================================================================
#!/usr/bin/env python3
"""
taxonomy_builder.py

Build a unified taxonomy for 3D-FRONT scenes and 3D-FUTURE models.

- Scans scene JSON files for furniture, titles, and rooms
- Resolves labels/categories/super-categories using model_info.json (Stage 1 logic)
- Adds structural classes (floor/wall/ceiling)
- Assigns IDs to supers, categories, rooms, labels, and titles in fixed ranges
- Generates categoryâ†’super mapping and a color palette
"""

from __future__ import annotations
import argparse
import json
from pathlib import Path
from tqdm import tqdm
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import numpy as np
import colorsys


# ------------------------------
# Class
# ------------------------------
class Taxonomy:

    def __init__(self, taxonomy_path: str | Path):
        with open(Path(taxonomy_path), "r", encoding="utf-8") as f:
            self.data = json.load(f)
        self.ranges = self.data.get("ranges", {})
        self.structural_categories = {'wall', 'floor', 'ceiling'}



    # ----------------------------
    # Core name â†” id
    # ----------------------------
    def id_to_name(self, val: int) -> str:
        val_str = str(val)
        if self.ranges["super"][0] <= val <= self.ranges["super"][1]:
            return self.data["id2super"].get(val_str, "UnknownSuper")
        if self.ranges["category"][0] <= val <= self.ranges["category"][1]:
            return self.data["id2category"].get(val_str, "UnknownCategory")
        if self.ranges["room"][0] <= val <= self.ranges["room"][1]:
            return self.data["id2room"].get(val_str, "UnknownRoom")
        if self.ranges["label"][0] <= val <= self.ranges["label"][1]:
            return self.data["id2label"].get(val_str, "UnknownLabel")
        if self.ranges["title"][0] <= val <= self.ranges["title"][1]:
            return self.data["id2title"].get(val_str, "UnknownTitle")
        return "Unknown"

    def name_to_id(self, val: str) -> int:
        return (
            self.data.get("room2id", {}).get(val)
            or self.data.get("title2id", {}).get(val)
            or self.data.get("label2id", {}).get(val)
            or self.data.get("category2id", {}).get(val)
            or self.data.get("super2id", {}).get(val, 0)
            or 0
        )


    def translate(self, val, output: str = "id"):
        """
        Generic translator.
        Args:
            val: int or str
            output: 'id' or 'name'
        """
        if isinstance(val, int):
            return self.id_to_name(val) if output == "name" else val
        elif isinstance(val, str):
            return self.name_to_id(val) if output == "id" else val
        return None

    def get_floor_ids(self) -> list[int]:
        """
        Return taxonomy IDs that correspond to 'floor' surfaces.
        Includes both category and label definitions.
        """
        floor_ids = []
        for name, idx in self.data.get("category2id", {}).items():
            if name.lower() == "floor":
                floor_ids.append(int(idx))
        for name, idx in self.data.get("label2id", {}).items():
            if name.lower() == "floor":
                floor_ids.append(int(idx))
        return floor_ids

    # ----------------------------
    # Super-category
    # ----------------------------
    def get_sup(self, val, output: str = "name"):
        """
        Get super-category for a given category/title/etc.
        Args:
            val: str name or int id
            output: 'name' or 'id'
        """
        if isinstance(val, int):
            name = self.id_to_name(val)
        else:
            name = val

        super_name = (
            self.data.get("title2super", {}).get(name)
            or self.data.get("category2super", {}).get(name)
            or (name if name in self.data["super2id"] else "UnknownSuper")
        )

        if output == "name":
            return super_name
        return self.data["super2id"].get(super_name, 0)
    
    def get_category(self, title: str) -> str:
        return self.data.get("title2category", {}).get(title, "UnknownCategory")

    def get_room_id(self, room_type: str) -> int:
        return self.data.get("room2id", {}).get(room_type, 0)

    def get_color(self, val, mode: str = "none"):
        """
        Get color for any taxonomy ID or name.
        For structural elements (floor/wall/ceiling), always returns category color (2000 range).
        For other elements, returns super-category color.
        Returns RGB tuple.
        """
        default_color = (127, 127, 127)  # gray fallback

        if val is None:
            return default_color

        # Convert to int ID if it's a string
        if isinstance(val, str):
            if val.isdigit():
                val = int(val)
            else:
                # Convert name to ID
                val = self.name_to_id(val)
                if val == 0:  # name_to_id returns 0 for unknown
                    return default_color

        # Get the name to check if it's structural
        name = self.id_to_name(val)
        
        # Special handling for structural elements - ALWAYS use category color (2000 range)
        if name.lower() in self.structural_categories:
            # Find the corresponding category ID (2000 range) for this structural element
            category_id = self.data.get("category2id", {}).get(name.lower())
            if category_id is not None:
                color = self.data.get("id2color", {}).get(str(category_id), default_color)
                return tuple(color) if isinstance(color, list) else default_color
            else:
                print(f"DEBUG: No category found for structural element '{name}'")
                return default_color

        # For non-structural items, resolve to super-category and get color
        super_id = self.resolve_super(val)
        if super_id is None:
            print(f"DEBUG get_color: could not resolve super for {val} ({name})")
            return default_color

        # Look up color by super-category ID in id2color
        color = self.data.get("id2color", {}).get(str(super_id), default_color)
        return tuple(color) if isinstance(color, list) else default_color


# Additional helper method for debugging specific failures
    def debug_title_resolution(self, title_id):
        """
        Debug why a specific title ID can't resolve to super-category.
        """
        print(f"\n--- Debug title resolution for ID: {title_id} ---")
        
        title_name = self.id_to_name(title_id)
        print(f"Title ID {title_id} â†’ Name: '{title_name}'")
        
        # Check title2super mapping
        title2super_direct = self.data.get("title2super", {}).get(str(title_id))
        print(f"Direct title2super lookup: {title2super_direct}")
        
        # Check title2category mapping
        title2category = self.data.get("title2category", {}).get(str(title_id))
        print(f"title2category lookup: {title2category}")
        
        if title2category:
            # Check if category has super mapping
            category2super = self.data.get("category2super", {}).get(title2category)
            print(f"category2super for '{title2category}': {category2super}")
            
            if category2super:
                super_id = self.data.get("super2id", {}).get(category2super)
                print(f"super2id for '{category2super}': {super_id}")
        
        # Check available mappings for debugging
        print(f"\nAvailable title2super keys (first 10): {list(self.data.get('title2super', {}).keys())[:10]}")
        print(f"Available title2category keys (first 10): {list(self.data.get('title2category', {}).keys())[:10]}")
        
        print("--- End debug ---\n")


    # Fixed resolve_super method with better case handling and fallbacks
    def resolve_super(self, val: int) -> int | None:
        """
        Resolve any ID (title, label, category, super) to its super-category ID.
        Returns the super-category ID or None if not found.
        """
        if val is None or val == 0:
            return None
        
        val_str = str(val)
        
        # Already a super-category ID
        if self.ranges["super"][0] <= val <= self.ranges["super"][1]:
            return val

        # Title ID â†’ super-category ID
        if self.ranges["title"][0] <= val <= self.ranges["title"][1]:
            # Try direct title â†’ super mapping (using title ID as key)
            super_name = self.data.get("title2super", {}).get(val_str)
            if super_name:
                # Try exact match first
                super_id = self.data.get("super2id", {}).get(super_name)
                if super_id:
                    return super_id
                
                # Try case variations
                for key in self.data.get("super2id", {}):
                    if key.lower() == super_name.lower():
                        return self.data["super2id"][key]
                
                # If "others", try common variations
                if super_name.lower() in ["others", "other"]:
                    for variant in ["Others", "Other", "others", "other", "UnknownSuper"]:
                        super_id = self.data.get("super2id", {}).get(variant)
                        if super_id:
                            return super_id
            
            # Fallback: title â†’ category â†’ super (using title ID as key)
            category_name = self.data.get("title2category", {}).get(val_str)
            if category_name:
                super_name = self.data.get("category2super", {}).get(category_name)
                if super_name:
                    super_id = self.data.get("super2id", {}).get(super_name)
                    if super_id:
                        return super_id
                    
                    # Try case variations for categoryâ†’super lookup too
                    for key in self.data.get("super2id", {}):
                        if key.lower() == super_name.lower():
                            return self.data["super2id"][key]
            
            # Additional fallback: try using title name as key (legacy support)
            title_name = self.id_to_name(val)
            super_name = self.data.get("title2super", {}).get(title_name)
            if super_name:
                super_id = self.data.get("super2id", {}).get(super_name)
                if super_id:
                    return super_id
                
                # Try case variations
                for key in self.data.get("super2id", {}):
                    if key.lower() == super_name.lower():
                        return self.data["super2id"][key]
            
            category_name = self.data.get("title2category", {}).get(title_name)
            if category_name:
                super_name = self.data.get("category2super", {}).get(category_name)
                if super_name:
                    super_id = self.data.get("super2id", {}).get(super_name)
                    if super_id:
                        return super_id

        # Label ID â†’ category â†’ super-category
        if self.ranges["label"][0] <= val <= self.ranges["label"][1]:
            # Get label name, then find its category
            label_name = self.id_to_name(val)
            if label_name.lower() in self.structural_categories:
                # For structural labels, find the corresponding category
                category_name = label_name.lower()  # floor label -> floor category
                super_name = self.data.get("category2super", {}).get(category_name)
                if super_name:
                    super_id = self.data.get("super2id", {}).get(super_name)
                    if super_id:
                        return super_id
            else:
                # For non-structural labels, use label2category mapping if it exists
                category_id = self.data.get("label2category", {}).get(val_str)
                if category_id:
                    category_name = self.id_to_name(int(category_id))
                    super_name = self.data.get("category2super", {}).get(category_name)
                    if super_name:
                        super_id = self.data.get("super2id", {}).get(super_name)
                        if super_id:
                            return super_id

        # Category ID â†’ super-category
        if self.ranges["category"][0] <= val <= self.ranges["category"][1]:
            category_name = self.id_to_name(val)
            super_name = self.data.get("category2super", {}).get(category_name)
            if super_name:
                super_id = self.data.get("super2id", {}).get(super_name)
                if super_id:
                    return super_id

        # Room IDs don't have super-categories
        if self.ranges["room"][0] <= val <= self.ranges["room"][1]:
            return None

        return None


    # Additional helper method for debugging
    def debug_color_resolution(self, val):
        """
        Debug method to trace color resolution process.
        """
        print(f"\n--- Debug color resolution for: {val} ---")
        
        # Convert to ID if needed
        original_val = val
        if isinstance(val, str) and not val.isdigit():
            val = self.name_to_id(val)
            print(f"Converted '{original_val}' to ID: {val}")
        elif isinstance(val, str):
            val = int(val)
        
        if val == 0:
            print("Result: Unknown item, using default color")
            return
        
        name = self.id_to_name(val)
        print(f"ID {val} â†’ Name: '{name}'")
        
        # Check range
        for range_name, (start, end) in self.ranges.items():
            if start <= val <= end:
                print(f"Range: {range_name} ({start}-{end})")
                break
        
        # Check if structural
        if name.lower() in self.structural_categories:
            print(f"Structural element detected: {name}")
            category_id = self.data.get("category2id", {}).get(name.lower())
            print(f"Corresponding category ID (2000 range): {category_id}")
            if category_id:
                color = self.data.get("id2color", {}).get(str(category_id))
                print(f"Category color: {color}")
            return
        
        # Resolve super for non-structural
        super_id = self.resolve_super(val)
        print(f"Resolved super ID: {super_id}")
        
        if super_id:
            super_name = self.id_to_name(super_id)
            print(f"Super name: {super_name}")
            color = self.data.get("id2color", {}).get(str(super_id))
            print(f"Super color: {color}")
        
        final_color = self.get_color(original_val)
        print(f"Final color: {final_color}")
        print("--- End debug ---\n")
# ------------------------------
# Utilities
# ------------------------------

def _make_flat_mapping(items, base=0, unknown_name="Unknown"):
    """Return mapping {name: id} with explicit 0 reserved for Unknown."""
    items = sorted(list(items))
    mapping = {unknown_name: 0}
    mapping.update({name: base + i + 1 for i, name in enumerate(items)})
    return mapping, list(mapping.values())


def _invert_mapping(mapping: dict) -> dict:
    """Build reverse dict {id: name}."""
    return {v: k for k, v in mapping.items()}

# ------------------------------
# Taxonomy Builder
# ------------------------------

def build_taxonomy_full(model_info_path: Path, scenes_dir: Path):
    """
    Build taxonomy by scanning scene JSONs with Stage 1 resolution logic.
    - Supers come only from model_info.json (+Structure).
    - Collects labels, categories, and titles from furniture + room children.
    - Returns one dict with id mappings and categoryâ†’super mapping.
    """

    # --- load model_info.json ---
    with open(model_info_path, "r", encoding="utf-8") as f:
        model_info = json.load(f)

    # jid -> (category, super)
    jid_to_cat_super_map = {}
    category_set, super_set = set(), set()
    category2super = {}

    for model in model_info:
        # The key in model_info.json is 'model_id', which corresponds to 'jid' in scene files
        jid = model.get("model_id")
        cat = model.get("category") or "UnknownCategory"
        sup = model.get("super-category") or "UnknownSuper"
        if jid:
            jid_to_cat_super_map[jid] = (cat, sup)
        category_set.add(cat)
        super_set.add(sup)
        if cat != "UnknownCategory":
            category2super[cat] = sup

    # --- inject structural classes ---
    STRUCTURAL = {
        "floor": ("floor", "Structure"),
        "wall": ("wall", "Structure"),
        "ceiling": ("ceiling", "Structure"),
    }
    label_set = set() # only structural labels
    for lbl, (cat, sup) in STRUCTURAL.items():
        label_set.add(lbl)
        category_set.add(cat)
        super_set.add(sup)
        category2super[cat] = sup

    # --- scan scenes to get titles and build title->cat/super mappings ---
    title_set = set()
    title2super = {}
    title2category = {}

    scene_files = list(scenes_dir.glob("*.json"))
    for scene_file in tqdm(scene_files, desc="[Taxonomy] Scanning scenes"):
        with open(scene_file, "r", encoding="utf-8") as f:
            scene = json.load(f)

        def process_furniture(furniture_list):
            for furn in furniture_list:
                jid = furn.get("jid")
                title = furn.get("title")

                if title:
                    title_set.add(title)

                if jid and jid in jid_to_cat_super_map:
                    cat, sup = jid_to_cat_super_map[jid]
                    if title:
                        title2super[title] = sup
                        title2category[title] = cat

        process_furniture(scene.get("furniture", []))
        for room in scene.get("scene", {}).get("room", []):
            process_furniture(room.get("children", []))


    # --- build final mappings ---
    super2id, super_ids = _make_flat_mapping(super_set, base=1000)
    category2id, category_ids = _make_flat_mapping(category_set, base=2000)
    label2id, label_ids = _make_flat_mapping(label_set, base=4000)
    title2id, title_ids = _make_flat_mapping(title_set, base=5000)


    return {
        "super2id": super2id,
        "category2id": category2id,
        "label2id": label2id,
        "title2id": title2id,
        "category2super": category2super,
        "title2super": title2super,
        "title2category": title2category
    }

def build_room_taxonomy(scenes_dir: Path):
    """Collect unique room types and count empty rooms."""
    room_set = set()
    empty_count = 0
    scene_files = list(scenes_dir.glob("*.json"))
    for scene_file in tqdm(scene_files, desc="[Rooms] Scanning scenes"):
        with open(scene_file, "r", encoding="utf-8") as f:
            scene = json.load(f)
        for room in scene.get("scene", {}).get("room", []):
            if room.get("empty", 0) == 1:
                empty_count += 1
            rtype = room.get("type")
            if isinstance(rtype, str) and rtype.strip():
                room_set.add(rtype.strip())
            else:
                room_set.add("OtherRoom")

    room2id, room_ids = _make_flat_mapping(room_set, base=3000, unknown_name="UnknownRoom")
    return room2id, room_ids, empty_count


# ------------------------------
# Color Palette
# ------------------------------
def assign_colors(super2id: dict, category2id: dict, category2super: dict):
    """
    Assign colors:
      - Structural (floor, wall, ceiling) -> fixed distinct colors
      - Unknown -> gray
      - Supers -> anchor hues
      - Categories -> variations of super hue
    """

    id2color = {}

    # 1. Fixed structural colors
    STRUCTURAL_COLORS = {
        "floor": [50, 50, 50],      # dark gray
        "wall": [200, 200, 200],    # light gray
        "ceiling": [255, 255, 255], # white
    }

    for cat, supercat in category2super.items():
        if cat in STRUCTURAL_COLORS:
            cid = category2id[cat]
            id2color[str(cid)] = STRUCTURAL_COLORS[cat]

    # 2. Anchors for non-structural supers
    super_anchors = [
        (228, 26, 28),    # red
        (55, 126, 184),   # blue
        (77, 175, 74),    # green
        (152, 78, 163),   # purple
        (255, 127, 0),    # orange
        (255, 255, 51),   # yellow
        (166, 86, 40),    # brown
        (0, 191, 196),    # cyan
    ]

    supers = sorted(super2id.keys())
    anchor_index = 0

    for supercat in supers:
        sid = super2id[supercat]
        cats = [c for c, s in category2super.items() if s == supercat]

        # Unknown super
        if supercat.lower() in ("unknown", "others", "other"):
            id2color[str(sid)] = [127, 127, 127]
            for cat in cats:
                cid = category2id[cat]
                id2color[str(cid)] = [127, 127, 127]
            continue

        # Structural handled above
        if supercat == "Structure":
            id2color[str(sid)] = [0, 0, 0]  # black for structure super
            continue

        # Assign anchor color to this super
        base_rgb = super_anchors[anchor_index % len(super_anchors)]
        id2color[str(sid)] = list(base_rgb)
        anchor_index += 1

        # Categories under this super
        n = len(cats)
        if n == 0:
            continue
        if n == 1:
            cid = category2id[cats[0]]
            id2color[str(cid)] = list(base_rgb)
            continue

        import colorsys, numpy as np
        r, g, b = [x / 255 for x in base_rgb]
        h, s, v = colorsys.rgb_to_hsv(r, g, b)

        vals = np.linspace(0.5, 0.95, n)
        sats = np.linspace(0.6, 1.0, n)

        for i, cat in enumerate(sorted(cats)):
            if cat in STRUCTURAL_COLORS:
                continue  # already assigned
            cid = category2id[cat]
            new_s = sats[i % len(sats)]
            new_v = vals[i % len(vals)]
            rr, gg, bb = colorsys.hsv_to_rgb(h, new_s, new_v)
            col = [int(rr * 255), int(gg * 255), int(bb * 255)]
            id2color[str(cid)] = col

    return id2color

# ------------------------------
# Wrapper
# ------------------------------

def build_taxonomy(model_info_path: str, scenes_dir: str, out_path: str) -> None:
    model_info_path = Path(model_info_path)
    scenes_dir = Path(scenes_dir)

    if not model_info_path.exists():
        raise FileNotFoundError(f"model_info.json not found: {model_info_path}")

    taxonomy_dict = build_taxonomy_full(model_info_path, scenes_dir)
    room2id, _, empty_count = build_room_taxonomy(scenes_dir)

    print(f"  Found {len(room2id)} unique room types, {empty_count} empty rooms flagged")

    id2color = assign_colors(
        taxonomy_dict["super2id"],
        taxonomy_dict["category2id"],
        taxonomy_dict["category2super"],
    )

    taxonomy = {
        **taxonomy_dict,
        "room2id": room2id,
        "id2color": id2color,
        "id2label": _invert_mapping(taxonomy_dict["label2id"]),
        "id2category": _invert_mapping(taxonomy_dict["category2id"]),
        "id2super": _invert_mapping(taxonomy_dict["super2id"]),
        "id2room": _invert_mapping(room2id),
        "id2title": _invert_mapping(taxonomy_dict["title2id"]),
        "ranges": {
            "super": [1000, 1999],
            "category": [2000, 2999],
            "room": [3000, 3999],
            "label": [4000, 4999],
            "title": [5000, 5999],
        },
    }

    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(taxonomy, f, indent=2)

    print(f"[INFO] Saved taxonomy to {out_path}")
    print(f"  {len(taxonomy_dict['label2id'])} labels, "
          f"{len(taxonomy_dict['category2id'])} categories, "
          f"{len(taxonomy_dict['super2id'])} super-categories, "
          f"{len(room2id)} room types, "
          f"{len(taxonomy_dict['title2id'])} titles")


# ------------------------------
# CLI
# ------------------------------

def main():
    parser = argparse.ArgumentParser(description="Build taxonomy from 3D-FUTURE and 3D-FRONT.")
    parser.add_argument("--model-info", required=True, help="Path to 3D-FUTURE model_info.json")
    parser.add_argument("--scenes-dir", required=True, help="Path to 3D-FRONT scenes directory")
    parser.add_argument("--out", required=True, help="Output taxonomy.json path")
    args = parser.parse_args()

    print("[INFO] Building taxonomy")
    build_taxonomy(args.model_info, args.scenes_dir, args.out)


if __name__ == "__main__":
    main()




================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\utils\utils.py
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
utils.py - Shared utilities for 3D scene processing pipeline (refactored)

Consolidated common functionality with reduced duplication.
"""

import json
import csv
import re
import glob
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Union
import numpy as np

# ----------------------------
# File Discovery (consolidated)
# ----------------------------

def discover_files(root: Path, pattern: str = None, manifest: Path = None, 
                   column_name: str = "room_parquet") -> List[Path]:
    """
    Unified file discovery with all methods in one function.
    Priority: manifest > pattern > default patterns
    """
    # Method 1: From manifest
    if manifest and manifest.exists():
        files = []
        try:
            with open(manifest, newline='', encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row_num, row in enumerate(reader, start=2):  # start=2 because row 1 is header
                    if column_name in row and row[column_name]:
                        try:
                            path = Path(row[column_name]).expanduser().resolve()
                            if path.exists():
                                files.append(path)
                            else:
                                print(f"Warning: File in manifest row {row_num} doesn't exist: {path}")
                        except Exception as e:
                            print(f"Warning: Invalid path in manifest row {row_num}: {row[column_name]} - {e}")
            
            if not files:
                print(f"Warning: No valid files found in manifest {manifest} using column '{column_name}'")
                print(f"Available columns: {list(csv.DictReader(open(manifest)))}")
            
            return files
        except Exception as e:
            print(f"Error reading manifest {manifest}: {e}")
            # Fall through to other methods
    
    # Method 2: Pattern-based
    if pattern:
        files = sorted(root.rglob(pattern))
        if files:
            return files
        else:
            print(f"Warning: No files found with pattern '{pattern}' in {root}")
    
    # Method 3: Default patterns
    for default_pattern in [
        "part-*.parquet", "*_*[0-9].parquet", "rooms/*/*.parquet", "*_sem_pointcloud.parquet"
    ]:
        files = sorted(root.rglob(default_pattern))
        if files:
            print(f"Found {len(files)} files using default pattern '{default_pattern}'")
            return files
    
    print(f"Error: No files found in {root} using any method")
    return []

def gather_paths_from_sources(file_path: str = None, patterns: List[str] = None, 
                             list_file: str = None) -> List[Path]:
    """Gather paths from multiple sources, deduplicated."""
    all_paths = []
    
    # Single file
    if file_path:
        all_paths.append(Path(file_path))
    
    # Pattern list
    if patterns:
        for pat in patterns:
            expanded = [Path(p) for p in glob.glob(pat)]
            all_paths.extend(expanded if expanded else [Path(pat)])
    
    # List file (JSON array or line-separated)
    if list_file:
        path = Path(list_file)
        txt = path.read_text(encoding="utf-8").strip()
        
        # Try JSON first
        try:
            arr = json.loads(txt)
            if isinstance(arr, list):
                all_paths.extend(Path(p) for p in arr)
            else:
                raise ValueError("Not a list")
        except Exception:
            # Line-separated fallback
            for line in txt.splitlines():
                line = line.strip()
                if line and not line.startswith("#"):
                    all_paths.append(Path(line))
    
    # Deduplicate
    seen = set()
    unique = []
    for p in all_paths:
        resolved = p.resolve()
        if resolved not in seen:
            seen.add(resolved)
            unique.append(resolved)
    
    return unique

# ----------------------------
# ID Inference (consolidated)
# ----------------------------

def infer_ids_from_path(path: Path) -> Tuple[str, int]:
    """Infer scene_id from the file name by taking the part before the first underscore.
    Room_id is parsed if a numeric part follows, otherwise -1.
    """
    stem = path.stem  # filename without extension
    parts = stem.split("_", 1)  # split into [before_first_underscore, rest]
    scene_id = parts[0]

    room_id = -1
    if len(parts) > 1 and parts[1].isdigit():
        room_id = int(parts[1])

    return scene_id, room_id


# ----------------------------
# Aliases / Legacy compatibility
# ----------------------------

def infer_scene_id(path: Path) -> str:
    """Return scene_id only, derived from infer_ids_from_path."""
    scene_id, _ = infer_ids_from_path(path)
    return scene_id

def find_semantic_maps_json(start_path: Path) -> Optional[Path]:
    """Walk up from start_path to find semantic_maps.json."""
    for p in [start_path, *start_path.parents]:
        cand = p / "semantic_maps.json"
        if cand.exists():
            return cand
    return None

def get_floor_label_ids(maps_path: Path) -> Tuple[int, ...]:
    """Load semantic_maps.json and return IDs mapped to 'floor'."""
    maps = json.loads(maps_path.read_text(encoding="utf-8"))
    ids = set()

    for mapping_name, is_key_label in [("label2id", True), ("id2label", False)]:
        if mapping_name in maps:
            for key, value in maps[mapping_name].items():
                label = str(key if is_key_label else value).strip().lower()
                if label == "floor":
                    try:
                        ids.add(int(value if is_key_label else key))
                    except (ValueError, TypeError):
                        pass

    if not ids:
        raise RuntimeError(f"'floor' not found in {maps_path}")
    return tuple(sorted(ids))

# ----------------------------
# Semantic Maps (consolidated)
# ----------------------------

class SemanticMaps:
    """Consolidated semantic maps handling."""
    
    def __init__(self, start_path: Path):
        self.maps_path = self._find_maps_file(start_path)
        self._maps_data = None
    
    def _find_maps_file(self, start: Path) -> Optional[Path]:
        """Walk up from start to locate semantic_maps.json."""
        for p in [start, *start.parents]:
            cand = p / "semantic_maps.json"
            if cand.exists():
                return cand
        return None
    
    @property
    def data(self) -> Dict:
        """Lazy-load maps data."""
        if self._maps_data is None:
            if not self.maps_path:
                raise RuntimeError("semantic_maps.json not found")
            self._maps_data = json.loads(self.maps_path.read_text(encoding="utf-8"))
        return self._maps_data
    
    def get_floor_label_ids(self) -> Tuple[int, ...]:
        """Extract floor label IDs."""
        ids = set()
        
        # Check both label2id and id2label mappings
        for mapping_name, is_key_label in [("label2id", True), ("id2label", False)]:
            if mapping_name in self.data:
                for key, value in self.data[mapping_name].items():
                    label = str(key if is_key_label else value).strip().lower()
                    if label == "floor":
                        try:
                            ids.add(int(value if is_key_label else key))
                        except (ValueError, TypeError):
                            pass
        
        if not ids:
            raise RuntimeError(f"'floor' not found in {self.maps_path}")
        
        return tuple(sorted(ids))
    
    def get_color_palette(self) -> Dict[int, Tuple[int, int, int]]:
        """Load color palette."""
        if "id2color" not in self.data:
            raise RuntimeError("id2color missing in semantic_maps.json")
        
        return {int(k): tuple(v) for k, v in self.data["id2color"].items()}
    
    def update_with_values(self, values_by_category: Dict[str, List[str]], freeze: bool = False):
        """Update maps with new values."""
        if not self.maps_path:
            raise RuntimeError("Cannot update: semantic_maps.json not found")
        
        maps = self.data.copy()
        changed = False
        
        for category, values in values_by_category.items():
            key = f"{category}2id"
            current = {str(k): int(v) for k, v in maps.get(key, {}).items()}
            unique_values = sorted({v for v in values if v}, key=lambda s: s.lower())
            
            if freeze:
                unknown = [v for v in unique_values if v not in current]
                if unknown:
                    raise RuntimeError(f"freeze_maps ON; unseen in '{category}': {unknown[:20]}")
            else:
                next_id = (max(current.values()) + 1) if current else 1
                for value in unique_values:
                    if value not in current:
                        current[value] = next_id
                        next_id += 1
                        changed = True
            
            maps[key] = current
        
        if changed:
            self.maps_path.parent.mkdir(parents=True, exist_ok=True)
            self.maps_path.write_text(json.dumps(maps, indent=2), encoding="utf-8")
        
        self._maps_data = maps  # Update cache

# ----------------------------
# Configuration (simplified)
# ----------------------------

def load_config_with_profile(config_path: str = None, profile: str = None) -> Dict:
    """Load and resolve config in one step."""
    if not config_path:
        return {}
    
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"Config file not found: {path}")
    
    # Load based on extension
    if path.suffix.lower() in (".yml", ".yaml"):
        try:
            import yaml
            data = yaml.safe_load(path.read_text(encoding="utf-8")) or {}
        except ImportError as e:
            raise RuntimeError("YAML config requested but 'pyyaml' is not installed") from e
    else:
        data = json.loads(path.read_text(encoding="utf-8"))
    
    if not isinstance(data, dict):
        raise ValueError("Config must be a dictionary")
    
    # Apply profile if specified
    profile_name = profile or data.get("profile")
    if profile_name and "profiles" in data:
        if profile_name not in data["profiles"]:
            raise ValueError(f"Profile '{profile_name}' not found")
        
        base_config = {k: v for k, v in data.items() if k not in ("profiles", "profile")}
        base_config.update(data["profiles"][profile_name])
        return base_config
    
    return data

# ----------------------------
# Taxonomy (simplified)
# ----------------------------

def load_taxonomy_resolver(taxonomy_path: Path):
    """Load taxonomy and return resolver function (category â†’ super)."""
    if not taxonomy_path or not taxonomy_path.exists():
        return None

    data = json.loads(taxonomy_path.read_text(encoding="utf-8"))

    category2super = data.get("category2super", {})
    aliases = data.get("aliases", {})

    def resolve(raw_label: str, model_info: Dict = None) -> Dict[str, str]:
        candidate = (model_info or {}).get("category") or raw_label or "unknown"

        # Exact category
        if candidate in category2super:
            return {"category": candidate, "super": category2super[candidate]}

        # Alias fallback
        if candidate in aliases:
            alias = aliases[candidate]
            return {"category": alias, "super": category2super.get(alias, "Other")}

        return {"category": "", "super": "Other"}

    return resolve


def load_global_palette(taxonomy_path: Path) -> Dict[int, Tuple[int, int, int]]:
    """
    Load global color palette (id -> RGB tuple) from a taxonomy JSON file.

    Args:
        taxonomy_path: Path to the taxonomy/semantic_maps.json file.

    Returns:
        Dict mapping int label IDs to (R, G, B) tuples.
    """
    if not taxonomy_path.exists():
        raise FileNotFoundError(f"taxonomy file not found: {taxonomy_path}")

    data = json.loads(taxonomy_path.read_text(encoding="utf-8"))

    # Handle list-of-dicts format
    if isinstance(data, list):
        if not data:
            raise RuntimeError(f"taxonomy file {taxonomy_path} is empty")
        data = data[0]

    if "id2color" not in data:
        raise RuntimeError(f"id2color not found in {taxonomy_path}")

    return {int(k): tuple(v) for k, v in data["id2color"].items()}

# ----------------------------
# Room Metadata (simplified)
# ----------------------------

def load_room_meta(room_dir: Path):
    """Load the metadata JSON (room-level or scene-level)."""
    candidates = list(room_dir.glob("*_meta.json"))
    if not candidates:
        candidates = list(room_dir.glob("*_scene_info.json"))
    if not candidates:
        return None

    meta_path = candidates[0]
    meta = json.loads(meta_path.read_text(encoding="utf-8"))

    # Unwrap list
    if isinstance(meta, list) and len(meta) > 0:
        meta = meta[0]

    return meta

def extract_frame_from_meta(meta):
    """
    Extract origin, u, v, n, uv_bounds, yaw_auto, map_band
    from either a room-level *_meta.json or a scene-level *_scene_info.json.
    """
    # Handle list wrapper
    if isinstance(meta, list):
        if not meta:
            raise ValueError("Empty metadata list")
        meta = meta[0]

    # Case 1: Room-level meta.json
    if "origin_world" in meta:
        origin = np.array(meta["origin_world"], dtype=np.float32)
        u = np.array(meta["u_world"], dtype=np.float32)
        v = np.array(meta["v_world"], dtype=np.float32)
        n = np.array(meta["n_world"], dtype=np.float32)
        uv_bounds = tuple(meta["uv_bounds"])
        yaw_auto = float(meta.get("yaw_auto", 0.0))
        map_band = tuple(meta.get("map_band_m", [0.05, 0.50]))
        return origin, u, v, n, uv_bounds, yaw_auto, map_band

    # Case 2: Scene-level scene_info.json
    if "bounds" in meta:
        bounds = meta["bounds"]
        if not bounds or len(bounds) != 2:
            raise ValueError("Invalid bounds in scene_info.json")

        # Bounds are [[xmin,ymin,zmin],[xmax,ymax,zmax]]
        (xmin, ymin, zmin), (xmax, ymax, zmax) = bounds
        origin = np.array([(xmin + xmax) / 2, (ymin + ymax) / 2, (zmin + zmax) / 2], dtype=np.float32)

        # Default orthogonal frame
        u = np.array([1.0, 0.0, 0.0], dtype=np.float32)
        v = np.array([0.0, 1.0, 0.0], dtype=np.float32)
        n = np.array([0.0, 0.0, 1.0], dtype=np.float32)

        uv_bounds = (xmin, xmax, ymin, ymax)
        yaw_auto = 0.0
        map_band = (0.0, zmax - zmin)  # crude height range

        return origin, u, v, n, uv_bounds, yaw_auto, map_band

    raise KeyError("Unrecognized metadata format (no origin_world or bounds)")
# ----------------------------
# Common Helpers (kept)
# ----------------------------

def ensure_columns_exist(df, required_columns: List[str], source: str = "dataframe"):
    """Validate required columns exist."""
    missing = [col for col in required_columns if col not in df.columns]
    if missing:
        raise RuntimeError(f"Missing columns {missing} in {source}")

def safe_mkdir(path: Path, parents: bool = True, exist_ok: bool = True):
    """Safe directory creation."""
    try:
        path.mkdir(parents=parents, exist_ok=exist_ok)
    except Exception as e:
        raise RuntimeError(f"Failed to create directory {path}: {e}")

def write_json(data: Dict, path: Path, indent: int = 2):
    """Write JSON with error handling."""
    try:
        safe_mkdir(path.parent)
        path.write_text(json.dumps(data, indent=indent), encoding="utf-8")
    except Exception as e:
        raise RuntimeError(f"Failed to write JSON to {path}: {e}")

def create_progress_tracker(total: int, description: str = "Processing"):
    """Create simple progress function."""
    def update_progress(current: int, item_name: str = "", success: bool = True):
        status = "âœ“" if success else "âœ—"
        percentage = (current / total) * 100 if total > 0 else 0
        print(f"[{current}/{total}] ({percentage:.1f}%) {status} {description} {item_name}", flush=True)
    return update_progress



================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\utils\validate_scenes.py
================================================================================
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
scan_rooms.py

Scan all JSON scene files in --in_dir and identify which contain rooms.
Outputs two JSONs in --out_dir:
  - valid_files.json: { "scene_file.json": ["LivingRoom", "Bedroom", ...], ... }
  - invalid_files.json: ["scene_file.json", ...]
"""

import json
from pathlib import Path
import argparse
from tqdm import tqdm


def check_rooms(scene_path: Path):
    """Return list of room types if scene exposes rooms, else None."""
    try:
        data = json.loads(scene_path.read_text(encoding="utf-8"))
    except Exception as e:
        return None, f"load error: {e}"

    rooms = data.get("scene", {}).get("room") or data.get("scene", {}).get("rooms")
    if not rooms:  # empty or missing
        return None, None

    room_types = [r.get("type", "UnknownRoom") for r in rooms]
    return room_types, None


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--in_dir", required=True,
                    help="Directory with scene JSON files to scan")
    ap.add_argument("--out_dir", required=True,
                    help="Directory to write valid_files.json and invalid_files.json")
    args = ap.parse_args()

    in_dir = Path(args.in_dir)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    files = sorted(in_dir.glob("*.json"))
    valid = {}
    invalid = []

    for sp in tqdm(files, desc="Scanning scenes"):
        room_types, err = check_rooms(sp)
        if err:
            invalid.append(sp.name)
        elif room_types:  # non-empty list of rooms
            valid[sp.name] = room_types
        else:
            invalid.append(sp.name)

    # write outputs
    (out_dir / "valid_files.json").write_text(json.dumps(valid, indent=2), encoding="utf-8")
    (out_dir / "invalid_files.json").write_text(json.dumps(invalid, indent=2), encoding="utf-8")

    print(f"âœ… wrote {len(valid)} valid and {len(invalid)} invalid files to {out_dir}")


if __name__ == "__main__":
    main()




================================================================================
FILE: C:\Users\Hagai.LAPTOP-QAG9263N\Desktop\Thesis\repositories\ImagiNav\data_preperation\utils\visualize_palette.py
================================================================================
#!/usr/bin/env python3
"""
visualize_palette.py

Generates one PNG per super-category showing its categories and assigned colors,
plus an overview PNG showing all super-categories and special colors (wall/floor/ceiling),
plus a "chosen labels/colors" PNG showing the actual used palette.
"""

import json
import argparse
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import seaborn as sns
import numpy as np


# Set seaborn style for better aesthetics
sns.set_style("whitegrid")
sns.set_context("notebook", font_scale=1.1)


def visualize_chosen_palette(taxonomy_path: str, out_dir: str):
    """Generate a plot showing the actual used palette: all super-categories + wall."""
    with open(taxonomy_path, "r", encoding="utf-8") as f:
        taxonomy = json.load(f)

    super2id = taxonomy["super2id"]
    id2color = taxonomy["id2color"]
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # Get all super-categories
    sorted_supers = sorted(super2id.items(), key=lambda x: x[1])
    
    # Start with super-categories
    chosen_items = list(sorted_supers)
    
    # Add wall - try multiple possible keys
    wall_id = None
    if "wall_id" in taxonomy:
        wall_id = taxonomy["wall_id"]
    elif "wall" in taxonomy:
        wall_id = taxonomy["wall"]
    
    # Also check if wall is in category2id
    if wall_id is None and "category2id" in taxonomy:
        category2id = taxonomy["category2id"]
        if "wall" in category2id:
            wall_id = category2id["wall"]
    
    # ALWAYS add wall to the list
    if wall_id is not None:
        chosen_items.append(("wall", wall_id))
        print(f"[DEBUG] Added wall with ID: {wall_id}")
    else:
        print(f"[WARNING] Wall ID not found in taxonomy!")
        print(f"[DEBUG] Available taxonomy keys: {list(taxonomy.keys())}")
    
    n_items = len(chosen_items)
    print(f"[INFO] Chosen palette has {n_items} items (should be {len(sorted_supers)} super-categories + 1 wall)")
    
    # Calculate grid dimensions
    cols = min(4, n_items)
    rows = int(np.ceil(n_items / cols))
    
    # Create figure with seaborn styling
    fig, ax = plt.subplots(figsize=(14, max(8, rows * 2.5)))
    ax.axis("off")
    ax.set_aspect('equal')
    
    # Patch dimensions and spacing
    patch_width = 2.5
    patch_height = 1.8
    h_spacing = 3.5
    v_spacing = 3.0
    
    for idx, (name, item_id) in enumerate(chosen_items):
        row, col = divmod(idx, cols)
        x = col * h_spacing
        y = -(row * v_spacing)
        
        # Get color
        item_color = tuple(c / 255 for c in id2color.get(str(item_id), [127, 127, 127]))
        
        # Check if this is wall
        is_wall = name == "wall"
        
        # Draw patch with shadow effect (no black outline)
        shadow = mpatches.Rectangle(
            (x + 0.05, y - 0.05), patch_width, patch_height,
            facecolor='gray', alpha=0.3, edgecolor='none'
        )
        ax.add_patch(shadow)
        
        rect = mpatches.Rectangle(
            (x, y), patch_width, patch_height,
            facecolor=item_color, edgecolor='none'
        )
        ax.add_patch(rect)
        
        # Add text with better positioning
        display_name = name.upper() if is_wall else name
        fontsize = 10 if is_wall else 11
        
        ax.text(
            x + patch_width / 2, y + patch_height / 2,
            display_name,
            ha="center", va="center",
            fontsize=fontsize, fontweight="bold",
            bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8, edgecolor='none')
        )
        
        # Add ID label
        label_text = f"ID: {item_id}"
        if is_wall:
            label_text = f"(Wall) {label_text}"
        
        ax.text(
            x + patch_width / 2, y - 0.4,
            label_text,
            ha="center", va="top",
            fontsize=8 if is_wall else 9, 
            style='italic', color='#333333'
        )
    
    # Set limits with padding
    ax.set_xlim(-0.5, cols * h_spacing - 0.5)
    ax.set_ylim(-(rows * v_spacing + 0.5), patch_height + 1.0)
    
    plt.title("Chosen Labels/Colors (Used Palette)", fontsize=16, fontweight='bold', pad=20)
    
    out_path = out_dir / "palette_chosen_labels_colors.png"
    plt.savefig(out_path, dpi=300, bbox_inches="tight", facecolor='white')
    plt.close()
    print(f"[INFO] Saved chosen palette: {out_path}")


def visualize_super_overview(taxonomy_path: str, out_dir: str):
    """Generate an overview plot showing all super-categories and special colors."""
    with open(taxonomy_path, "r", encoding="utf-8") as f:
        taxonomy = json.load(f)

    super2id = taxonomy["super2id"]
    id2color = taxonomy["id2color"]
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # Sort super-categories by ID for consistent ordering
    sorted_supers = sorted(super2id.items(), key=lambda x: x[1])
    
    # Add special categories (wall/floor/ceiling) if they exist
    special_categories = []
    special_mapping = {
        "wall": taxonomy.get("wall_id"),
        "floor": taxonomy.get("floor_id"),
        "ceiling": taxonomy.get("ceiling_id")
    }
    
    # Also check alternative keys
    if special_mapping["wall"] is None:
        special_mapping["wall"] = taxonomy.get("wall")
    if special_mapping["floor"] is None:
        special_mapping["floor"] = taxonomy.get("floor")
    if special_mapping["ceiling"] is None:
        special_mapping["ceiling"] = taxonomy.get("ceiling")
    
    # Check category2id as fallback
    if "category2id" in taxonomy:
        category2id = taxonomy["category2id"]
        for name in ["wall", "floor", "ceiling"]:
            if special_mapping[name] is None and name in category2id:
                special_mapping[name] = category2id[name]
    
    for name, cat_id in special_mapping.items():
        if cat_id is not None:
            special_categories.append((name, cat_id))
    
    # Combine all items to display
    all_items = sorted_supers + special_categories
    n_items = len(all_items)
    
    # Calculate grid dimensions
    cols = min(4, n_items)
    rows = int(np.ceil(n_items / cols))
    
    # Create figure with seaborn styling
    fig, ax = plt.subplots(figsize=(14, max(8, rows * 2.5)))
    ax.axis("off")
    ax.set_aspect('equal')
    
    # Patch dimensions and spacing
    patch_width = 2.5
    patch_height = 1.8
    h_spacing = 3.5
    v_spacing = 3.0
    
    for idx, (name, item_id) in enumerate(all_items):
        row, col = divmod(idx, cols)
        x = col * h_spacing
        y = -(row * v_spacing)
        
        # Get color
        item_color = tuple(c / 255 for c in id2color.get(str(item_id), [127, 127, 127]))
        
        # Check if this is a special category
        is_special = name in ["wall", "floor", "ceiling"]
        
        # Draw patch with shadow effect (no black outline)
        shadow = mpatches.Rectangle(
            (x + 0.05, y - 0.05), patch_width, patch_height,
            facecolor='gray', alpha=0.3, edgecolor='none'
        )
        ax.add_patch(shadow)
        
        rect = mpatches.Rectangle(
            (x, y), patch_width, patch_height,
            facecolor=item_color, edgecolor='none'
        )
        ax.add_patch(rect)
        
        # Add text with better positioning
        # Add special indicator for wall/floor/ceiling
        display_name = name.upper() if is_special else name
        fontsize = 11 if not is_special else 10
        
        ax.text(
            x + patch_width / 2, y + patch_height / 2,
            display_name,
            ha="center", va="center",
            fontsize=fontsize, fontweight="bold",
            bbox=dict(boxstyle="round,pad=0.3", facecolor='white', alpha=0.8, edgecolor='none')
        )
        
        # Add label for special categories
        label_text = f"ID: {item_id}"
        if is_special:
            label_text = f"(Special) {label_text}"
        
        ax.text(
            x + patch_width / 2, y - 0.4,
            label_text,
            ha="center", va="top",
            fontsize=8 if is_special else 9, 
            style='italic', color='#333333'
        )
    
    # Set limits with padding - added top padding to prevent cropping
    ax.set_xlim(-0.5, cols * h_spacing - 0.5)
    ax.set_ylim(-(rows * v_spacing + 0.5), patch_height + 1.0)
    
    plt.title("Super-Categories & Special Colors Overview", fontsize=16, fontweight='bold', pad=20)
    
    out_path = out_dir / "palette_super_categories_overview.png"
    plt.savefig(out_path, dpi=300, bbox_inches="tight", facecolor='white')
    plt.close()
    print(f"[INFO] Saved super-categories overview: {out_path}")


def visualize_per_super(taxonomy_path: str, out_dir: str):
    """Generate one visualization per super-category."""
    with open(taxonomy_path, "r", encoding="utf-8") as f:
        taxonomy = json.load(f)

    super2id = taxonomy["super2id"]
    category2id = taxonomy["category2id"]
    category2super = taxonomy["category2super"]
    id2color = taxonomy["id2color"]

    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    for supercat, sid in sorted(super2id.items(), key=lambda x: x[1]):
        # Categories belonging to this super
        categories = sorted([c for c, s in category2super.items() if s == supercat])
        
        if not categories:
            continue
        
        # Calculate dynamic figure size based on number of categories
        n_cats = len(categories)
        cols = 5
        rows = int(np.ceil(n_cats / cols))
        fig_height = max(8, 4 + rows * 2.5)
        
        fig, ax = plt.subplots(figsize=(14, fig_height))
        ax.axis("off")
        ax.set_aspect('equal')

        # Draw super-category block with enhanced styling
        scol = tuple(c / 255 for c in id2color.get(str(sid), [127, 127, 127]))
        
        # Shadow for super block
        super_shadow = mpatches.Rectangle(
            (0.05, 0.05), 3.5, 1.8,
            facecolor='gray', alpha=0.3, edgecolor='none'
        )
        ax.add_patch(super_shadow)
        
        # Main super block (no black outline)
        super_rect = mpatches.Rectangle(
            (0, 0.1), 3.5, 1.8,
            facecolor=scol, edgecolor='none'
        )
        ax.add_patch(super_rect)
        
        ax.text(
            1.75, 1.0,
            f"{supercat}",
            ha="center", va="center",
            fontsize=14, fontweight="bold",
            bbox=dict(boxstyle="round,pad=0.4", facecolor='white', alpha=0.9, edgecolor='none')
        )
        ax.text(
            1.75, -0.3,
            f"Super-Category ID: {sid}",
            ha="center", va="top",
            fontsize=10, style='italic', color='#444444'
        )

        # Draw categories in a grid with better spacing
        patch_width = 2.0
        patch_height = 1.5
        h_spacing = 2.8
        v_spacing = 2.5
        
        for i, cat in enumerate(categories):
            cid = category2id[cat]
            ccol = tuple(c / 255 for c in id2color.get(str(cid), [127, 127, 127]))
            
            row, col = divmod(i, cols)
            x = col * h_spacing
            y = -(row + 2) * v_spacing
            
            # Shadow effect
            shadow = mpatches.Rectangle(
                (x + 0.05, y - 0.05), patch_width, patch_height,
                facecolor='gray', alpha=0.2, edgecolor='none'
            )
            ax.add_patch(shadow)
            
            # Category patch
            rect = mpatches.Rectangle(
                (x, y), patch_width, patch_height,
                facecolor=ccol, edgecolor='black', linewidth=1.5
            )
            ax.add_patch(rect)
            
            # Category name (with wrapping for long names)
            cat_display = cat if len(cat) <= 20 else cat[:17] + "..."
            ax.text(
                x + patch_width / 2, y + patch_height / 2,
                cat_display,
                ha="center", va="center",
                fontsize=9, fontweight="semibold",
                bbox=dict(boxstyle="round,pad=0.2", facecolor='white', alpha=0.7, edgecolor='none')
            )
            
            # Category ID below patch
            ax.text(
                x + patch_width / 2, y - 0.3,
                f"ID: {cid}",
                ha="center", va="top",
                fontsize=8, color='#555555'
            )

        # Set limits with proper padding
        ax.set_xlim(-0.5, cols * h_spacing - 0.5)
        ax.set_ylim(-(rows + 2) * v_spacing - 1, 2.5)
        
        plt.title(
            f"Category Palette for: {supercat}",
            fontsize=15, fontweight='bold', pad=20
        )

        out_path = out_dir / f"palette_{supercat.replace('/', '_').replace(' ', '_')}.png"
        plt.savefig(out_path, dpi=300, bbox_inches="tight", facecolor='white')
        plt.close()
        print(f"[INFO] Saved {out_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Visualize taxonomy color palette (one PNG per super + overview)."
    )
    parser.add_argument("taxonomy", help="Path to taxonomy.json")
    parser.add_argument("--out-dir", required=True, help="Directory to save palette visualizations")
    args = parser.parse_args()

    # Generate chosen labels/colors (actual used palette)
    visualize_chosen_palette(args.taxonomy, args.out_dir)
    
    # Generate super-categories overview
    visualize_super_overview(args.taxonomy, args.out_dir)
    
    # Generate individual super-category visualizations
    visualize_per_super(args.taxonomy, args.out_dir)



